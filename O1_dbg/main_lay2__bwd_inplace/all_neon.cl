proc main(
    # array
    sint16 arr0000, sint16 arr0001, sint16 arr0002, sint16 arr0003, sint16 arr0004, sint16 arr0005, sint16 arr0006, sint16 arr0007,
    sint16 arr0010, sint16 arr0011, sint16 arr0012, sint16 arr0013, sint16 arr0014, sint16 arr0015, sint16 arr0016, sint16 arr0017,
    sint16 arr0020, sint16 arr0021, sint16 arr0022, sint16 arr0023, sint16 arr0024, sint16 arr0025, sint16 arr0026, sint16 arr0027,
    sint16 arr0030, sint16 arr0031, sint16 arr0032, sint16 arr0033, sint16 arr0034, sint16 arr0035, sint16 arr0036, sint16 arr0037,
    sint16 arr0040, sint16 arr0041, sint16 arr0042, sint16 arr0043, sint16 arr0044, sint16 arr0045, sint16 arr0046, sint16 arr0047,
    sint16 arr0050, sint16 arr0051, sint16 arr0052, sint16 arr0053, sint16 arr0054, sint16 arr0055, sint16 arr0056, sint16 arr0057,
    sint16 arr0060, sint16 arr0061, sint16 arr0062, sint16 arr0063, sint16 arr0064, sint16 arr0065, sint16 arr0066, sint16 arr0067,
    sint16 arr0070, sint16 arr0071, sint16 arr0072, sint16 arr0073, sint16 arr0074, sint16 arr0075, sint16 arr0076, sint16 arr0077,
    sint16 arr0080, sint16 arr0081, sint16 arr0082, sint16 arr0083, sint16 arr0084, sint16 arr0085, sint16 arr0086, sint16 arr0087,
    sint16 arr0100, sint16 arr0101, sint16 arr0102, sint16 arr0103, sint16 arr0104, sint16 arr0105, sint16 arr0106, sint16 arr0107,
    sint16 arr0110, sint16 arr0111, sint16 arr0112, sint16 arr0113, sint16 arr0114, sint16 arr0115, sint16 arr0116, sint16 arr0117,
    sint16 arr0120, sint16 arr0121, sint16 arr0122, sint16 arr0123, sint16 arr0124, sint16 arr0125, sint16 arr0126, sint16 arr0127,
    sint16 arr0130, sint16 arr0131, sint16 arr0132, sint16 arr0133, sint16 arr0134, sint16 arr0135, sint16 arr0136, sint16 arr0137,
    sint16 arr0140, sint16 arr0141, sint16 arr0142, sint16 arr0143, sint16 arr0144, sint16 arr0145, sint16 arr0146, sint16 arr0147,
    sint16 arr0150, sint16 arr0151, sint16 arr0152, sint16 arr0153, sint16 arr0154, sint16 arr0155, sint16 arr0156, sint16 arr0157,
    sint16 arr0160, sint16 arr0161, sint16 arr0162, sint16 arr0163, sint16 arr0164, sint16 arr0165, sint16 arr0166, sint16 arr0167,
    sint16 arr0170, sint16 arr0171, sint16 arr0172, sint16 arr0173, sint16 arr0174, sint16 arr0175, sint16 arr0176, sint16 arr0177,
    sint16 arr0180, sint16 arr0181, sint16 arr0182, sint16 arr0183, sint16 arr0184, sint16 arr0185, sint16 arr0186, sint16 arr0187,
    sint16 arr1000, sint16 arr1001, sint16 arr1002, sint16 arr1003, sint16 arr1004, sint16 arr1005, sint16 arr1006, sint16 arr1007,
    sint16 arr1010, sint16 arr1011, sint16 arr1012, sint16 arr1013, sint16 arr1014, sint16 arr1015, sint16 arr1016, sint16 arr1017,
    sint16 arr1020, sint16 arr1021, sint16 arr1022, sint16 arr1023, sint16 arr1024, sint16 arr1025, sint16 arr1026, sint16 arr1027,
    sint16 arr1030, sint16 arr1031, sint16 arr1032, sint16 arr1033, sint16 arr1034, sint16 arr1035, sint16 arr1036, sint16 arr1037,
    sint16 arr1040, sint16 arr1041, sint16 arr1042, sint16 arr1043, sint16 arr1044, sint16 arr1045, sint16 arr1046, sint16 arr1047,
    sint16 arr1050, sint16 arr1051, sint16 arr1052, sint16 arr1053, sint16 arr1054, sint16 arr1055, sint16 arr1056, sint16 arr1057,
    sint16 arr1060, sint16 arr1061, sint16 arr1062, sint16 arr1063, sint16 arr1064, sint16 arr1065, sint16 arr1066, sint16 arr1067,
    sint16 arr1070, sint16 arr1071, sint16 arr1072, sint16 arr1073, sint16 arr1074, sint16 arr1075, sint16 arr1076, sint16 arr1077,
    sint16 arr1080, sint16 arr1081, sint16 arr1082, sint16 arr1083, sint16 arr1084, sint16 arr1085, sint16 arr1086, sint16 arr1087,
    sint16 arr1100, sint16 arr1101, sint16 arr1102, sint16 arr1103, sint16 arr1104, sint16 arr1105, sint16 arr1106, sint16 arr1107,
    sint16 arr1110, sint16 arr1111, sint16 arr1112, sint16 arr1113, sint16 arr1114, sint16 arr1115, sint16 arr1116, sint16 arr1117,
    sint16 arr1120, sint16 arr1121, sint16 arr1122, sint16 arr1123, sint16 arr1124, sint16 arr1125, sint16 arr1126, sint16 arr1127,
    sint16 arr1130, sint16 arr1131, sint16 arr1132, sint16 arr1133, sint16 arr1134, sint16 arr1135, sint16 arr1136, sint16 arr1137,
    sint16 arr1140, sint16 arr1141, sint16 arr1142, sint16 arr1143, sint16 arr1144, sint16 arr1145, sint16 arr1146, sint16 arr1147,
    sint16 arr1150, sint16 arr1151, sint16 arr1152, sint16 arr1153, sint16 arr1154, sint16 arr1155, sint16 arr1156, sint16 arr1157,
    sint16 arr1160, sint16 arr1161, sint16 arr1162, sint16 arr1163, sint16 arr1164, sint16 arr1165, sint16 arr1166, sint16 arr1167,
    sint16 arr1170, sint16 arr1171, sint16 arr1172, sint16 arr1173, sint16 arr1174, sint16 arr1175, sint16 arr1176, sint16 arr1177,
    sint16 arr1180, sint16 arr1181, sint16 arr1182, sint16 arr1183, sint16 arr1184, sint16 arr1185, sint16 arr1186, sint16 arr1187,
    sint16 arr2000, sint16 arr2001, sint16 arr2002, sint16 arr2003, sint16 arr2004, sint16 arr2005, sint16 arr2006, sint16 arr2007,
    sint16 arr2010, sint16 arr2011, sint16 arr2012, sint16 arr2013, sint16 arr2014, sint16 arr2015, sint16 arr2016, sint16 arr2017,
    sint16 arr2020, sint16 arr2021, sint16 arr2022, sint16 arr2023, sint16 arr2024, sint16 arr2025, sint16 arr2026, sint16 arr2027,
    sint16 arr2030, sint16 arr2031, sint16 arr2032, sint16 arr2033, sint16 arr2034, sint16 arr2035, sint16 arr2036, sint16 arr2037,
    sint16 arr2040, sint16 arr2041, sint16 arr2042, sint16 arr2043, sint16 arr2044, sint16 arr2045, sint16 arr2046, sint16 arr2047,
    sint16 arr2050, sint16 arr2051, sint16 arr2052, sint16 arr2053, sint16 arr2054, sint16 arr2055, sint16 arr2056, sint16 arr2057,
    sint16 arr2060, sint16 arr2061, sint16 arr2062, sint16 arr2063, sint16 arr2064, sint16 arr2065, sint16 arr2066, sint16 arr2067,
    sint16 arr2070, sint16 arr2071, sint16 arr2072, sint16 arr2073, sint16 arr2074, sint16 arr2075, sint16 arr2076, sint16 arr2077,
    sint16 arr2080, sint16 arr2081, sint16 arr2082, sint16 arr2083, sint16 arr2084, sint16 arr2085, sint16 arr2086, sint16 arr2087,
    sint16 arr2100, sint16 arr2101, sint16 arr2102, sint16 arr2103, sint16 arr2104, sint16 arr2105, sint16 arr2106, sint16 arr2107,
    sint16 arr2110, sint16 arr2111, sint16 arr2112, sint16 arr2113, sint16 arr2114, sint16 arr2115, sint16 arr2116, sint16 arr2117,
    sint16 arr2120, sint16 arr2121, sint16 arr2122, sint16 arr2123, sint16 arr2124, sint16 arr2125, sint16 arr2126, sint16 arr2127,
    sint16 arr2130, sint16 arr2131, sint16 arr2132, sint16 arr2133, sint16 arr2134, sint16 arr2135, sint16 arr2136, sint16 arr2137,
    sint16 arr2140, sint16 arr2141, sint16 arr2142, sint16 arr2143, sint16 arr2144, sint16 arr2145, sint16 arr2146, sint16 arr2147,
    sint16 arr2150, sint16 arr2151, sint16 arr2152, sint16 arr2153, sint16 arr2154, sint16 arr2155, sint16 arr2156, sint16 arr2157,
    sint16 arr2160, sint16 arr2161, sint16 arr2162, sint16 arr2163, sint16 arr2164, sint16 arr2165, sint16 arr2166, sint16 arr2167,
    sint16 arr2170, sint16 arr2171, sint16 arr2172, sint16 arr2173, sint16 arr2174, sint16 arr2175, sint16 arr2176, sint16 arr2177,
    sint16 arr2180, sint16 arr2181, sint16 arr2182, sint16 arr2183, sint16 arr2184, sint16 arr2185, sint16 arr2186, sint16 arr2187,
    sint16 arr3000, sint16 arr3001, sint16 arr3002, sint16 arr3003, sint16 arr3004, sint16 arr3005, sint16 arr3006, sint16 arr3007,
    sint16 arr3010, sint16 arr3011, sint16 arr3012, sint16 arr3013, sint16 arr3014, sint16 arr3015, sint16 arr3016, sint16 arr3017,
    sint16 arr3020, sint16 arr3021, sint16 arr3022, sint16 arr3023, sint16 arr3024, sint16 arr3025, sint16 arr3026, sint16 arr3027,
    sint16 arr3030, sint16 arr3031, sint16 arr3032, sint16 arr3033, sint16 arr3034, sint16 arr3035, sint16 arr3036, sint16 arr3037,
    sint16 arr3040, sint16 arr3041, sint16 arr3042, sint16 arr3043, sint16 arr3044, sint16 arr3045, sint16 arr3046, sint16 arr3047,
    sint16 arr3050, sint16 arr3051, sint16 arr3052, sint16 arr3053, sint16 arr3054, sint16 arr3055, sint16 arr3056, sint16 arr3057,
    sint16 arr3060, sint16 arr3061, sint16 arr3062, sint16 arr3063, sint16 arr3064, sint16 arr3065, sint16 arr3066, sint16 arr3067,
    sint16 arr3070, sint16 arr3071, sint16 arr3072, sint16 arr3073, sint16 arr3074, sint16 arr3075, sint16 arr3076, sint16 arr3077,
    sint16 arr3080, sint16 arr3081, sint16 arr3082, sint16 arr3083, sint16 arr3084, sint16 arr3085, sint16 arr3086, sint16 arr3087,
    sint16 arr3100, sint16 arr3101, sint16 arr3102, sint16 arr3103, sint16 arr3104, sint16 arr3105, sint16 arr3106, sint16 arr3107,
    sint16 arr3110, sint16 arr3111, sint16 arr3112, sint16 arr3113, sint16 arr3114, sint16 arr3115, sint16 arr3116, sint16 arr3117,
    sint16 arr3120, sint16 arr3121, sint16 arr3122, sint16 arr3123, sint16 arr3124, sint16 arr3125, sint16 arr3126, sint16 arr3127,
    sint16 arr3130, sint16 arr3131, sint16 arr3132, sint16 arr3133, sint16 arr3134, sint16 arr3135, sint16 arr3136, sint16 arr3137,
    sint16 arr3140, sint16 arr3141, sint16 arr3142, sint16 arr3143, sint16 arr3144, sint16 arr3145, sint16 arr3146, sint16 arr3147,
    sint16 arr3150, sint16 arr3151, sint16 arr3152, sint16 arr3153, sint16 arr3154, sint16 arr3155, sint16 arr3156, sint16 arr3157,
    sint16 arr3160, sint16 arr3161, sint16 arr3162, sint16 arr3163, sint16 arr3164, sint16 arr3165, sint16 arr3166, sint16 arr3167,
    sint16 arr3170, sint16 arr3171, sint16 arr3172, sint16 arr3173, sint16 arr3174, sint16 arr3175, sint16 arr3176, sint16 arr3177,
    sint16 arr3180, sint16 arr3181, sint16 arr3182, sint16 arr3183, sint16 arr3184, sint16 arr3185, sint16 arr3186, sint16 arr3187,
    sint16 arr4000, sint16 arr4001, sint16 arr4002, sint16 arr4003, sint16 arr4004, sint16 arr4005, sint16 arr4006, sint16 arr4007,
    sint16 arr4010, sint16 arr4011, sint16 arr4012, sint16 arr4013, sint16 arr4014, sint16 arr4015, sint16 arr4016, sint16 arr4017,
    sint16 arr4020, sint16 arr4021, sint16 arr4022, sint16 arr4023, sint16 arr4024, sint16 arr4025, sint16 arr4026, sint16 arr4027,
    sint16 arr4030, sint16 arr4031, sint16 arr4032, sint16 arr4033, sint16 arr4034, sint16 arr4035, sint16 arr4036, sint16 arr4037,
    sint16 arr4040, sint16 arr4041, sint16 arr4042, sint16 arr4043, sint16 arr4044, sint16 arr4045, sint16 arr4046, sint16 arr4047,
    sint16 arr4050, sint16 arr4051, sint16 arr4052, sint16 arr4053, sint16 arr4054, sint16 arr4055, sint16 arr4056, sint16 arr4057,
    sint16 arr4060, sint16 arr4061, sint16 arr4062, sint16 arr4063, sint16 arr4064, sint16 arr4065, sint16 arr4066, sint16 arr4067,
    sint16 arr4070, sint16 arr4071, sint16 arr4072, sint16 arr4073, sint16 arr4074, sint16 arr4075, sint16 arr4076, sint16 arr4077,
    sint16 arr4080, sint16 arr4081, sint16 arr4082, sint16 arr4083, sint16 arr4084, sint16 arr4085, sint16 arr4086, sint16 arr4087,
    sint16 arr4100, sint16 arr4101, sint16 arr4102, sint16 arr4103, sint16 arr4104, sint16 arr4105, sint16 arr4106, sint16 arr4107,
    sint16 arr4110, sint16 arr4111, sint16 arr4112, sint16 arr4113, sint16 arr4114, sint16 arr4115, sint16 arr4116, sint16 arr4117,
    sint16 arr4120, sint16 arr4121, sint16 arr4122, sint16 arr4123, sint16 arr4124, sint16 arr4125, sint16 arr4126, sint16 arr4127,
    sint16 arr4130, sint16 arr4131, sint16 arr4132, sint16 arr4133, sint16 arr4134, sint16 arr4135, sint16 arr4136, sint16 arr4137,
    sint16 arr4140, sint16 arr4141, sint16 arr4142, sint16 arr4143, sint16 arr4144, sint16 arr4145, sint16 arr4146, sint16 arr4147,
    sint16 arr4150, sint16 arr4151, sint16 arr4152, sint16 arr4153, sint16 arr4154, sint16 arr4155, sint16 arr4156, sint16 arr4157,
    sint16 arr4160, sint16 arr4161, sint16 arr4162, sint16 arr4163, sint16 arr4164, sint16 arr4165, sint16 arr4166, sint16 arr4167,
    sint16 arr4170, sint16 arr4171, sint16 arr4172, sint16 arr4173, sint16 arr4174, sint16 arr4175, sint16 arr4176, sint16 arr4177,
    sint16 arr4180, sint16 arr4181, sint16 arr4182, sint16 arr4183, sint16 arr4184, sint16 arr4185, sint16 arr4186, sint16 arr4187,
    sint16 arr5000, sint16 arr5001, sint16 arr5002, sint16 arr5003, sint16 arr5004, sint16 arr5005, sint16 arr5006, sint16 arr5007,
    sint16 arr5010, sint16 arr5011, sint16 arr5012, sint16 arr5013, sint16 arr5014, sint16 arr5015, sint16 arr5016, sint16 arr5017,
    sint16 arr5020, sint16 arr5021, sint16 arr5022, sint16 arr5023, sint16 arr5024, sint16 arr5025, sint16 arr5026, sint16 arr5027,
    sint16 arr5030, sint16 arr5031, sint16 arr5032, sint16 arr5033, sint16 arr5034, sint16 arr5035, sint16 arr5036, sint16 arr5037,
    sint16 arr5040, sint16 arr5041, sint16 arr5042, sint16 arr5043, sint16 arr5044, sint16 arr5045, sint16 arr5046, sint16 arr5047,
    sint16 arr5050, sint16 arr5051, sint16 arr5052, sint16 arr5053, sint16 arr5054, sint16 arr5055, sint16 arr5056, sint16 arr5057,
    sint16 arr5060, sint16 arr5061, sint16 arr5062, sint16 arr5063, sint16 arr5064, sint16 arr5065, sint16 arr5066, sint16 arr5067,
    sint16 arr5070, sint16 arr5071, sint16 arr5072, sint16 arr5073, sint16 arr5074, sint16 arr5075, sint16 arr5076, sint16 arr5077,
    sint16 arr5080, sint16 arr5081, sint16 arr5082, sint16 arr5083, sint16 arr5084, sint16 arr5085, sint16 arr5086, sint16 arr5087,
    sint16 arr5100, sint16 arr5101, sint16 arr5102, sint16 arr5103, sint16 arr5104, sint16 arr5105, sint16 arr5106, sint16 arr5107,
    sint16 arr5110, sint16 arr5111, sint16 arr5112, sint16 arr5113, sint16 arr5114, sint16 arr5115, sint16 arr5116, sint16 arr5117,
    sint16 arr5120, sint16 arr5121, sint16 arr5122, sint16 arr5123, sint16 arr5124, sint16 arr5125, sint16 arr5126, sint16 arr5127,
    sint16 arr5130, sint16 arr5131, sint16 arr5132, sint16 arr5133, sint16 arr5134, sint16 arr5135, sint16 arr5136, sint16 arr5137,
    sint16 arr5140, sint16 arr5141, sint16 arr5142, sint16 arr5143, sint16 arr5144, sint16 arr5145, sint16 arr5146, sint16 arr5147,
    sint16 arr5150, sint16 arr5151, sint16 arr5152, sint16 arr5153, sint16 arr5154, sint16 arr5155, sint16 arr5156, sint16 arr5157,
    sint16 arr5160, sint16 arr5161, sint16 arr5162, sint16 arr5163, sint16 arr5164, sint16 arr5165, sint16 arr5166, sint16 arr5167,
    sint16 arr5170, sint16 arr5171, sint16 arr5172, sint16 arr5173, sint16 arr5174, sint16 arr5175, sint16 arr5176, sint16 arr5177,
    sint16 arr5180, sint16 arr5181, sint16 arr5182, sint16 arr5183, sint16 arr5184, sint16 arr5185, sint16 arr5186, sint16 arr5187,
    sint16 arr6000, sint16 arr6001, sint16 arr6002, sint16 arr6003, sint16 arr6004, sint16 arr6005, sint16 arr6006, sint16 arr6007,
    sint16 arr6010, sint16 arr6011, sint16 arr6012, sint16 arr6013, sint16 arr6014, sint16 arr6015, sint16 arr6016, sint16 arr6017,
    sint16 arr6020, sint16 arr6021, sint16 arr6022, sint16 arr6023, sint16 arr6024, sint16 arr6025, sint16 arr6026, sint16 arr6027,
    sint16 arr6030, sint16 arr6031, sint16 arr6032, sint16 arr6033, sint16 arr6034, sint16 arr6035, sint16 arr6036, sint16 arr6037,
    sint16 arr6040, sint16 arr6041, sint16 arr6042, sint16 arr6043, sint16 arr6044, sint16 arr6045, sint16 arr6046, sint16 arr6047,
    sint16 arr6050, sint16 arr6051, sint16 arr6052, sint16 arr6053, sint16 arr6054, sint16 arr6055, sint16 arr6056, sint16 arr6057,
    sint16 arr6060, sint16 arr6061, sint16 arr6062, sint16 arr6063, sint16 arr6064, sint16 arr6065, sint16 arr6066, sint16 arr6067,
    sint16 arr6070, sint16 arr6071, sint16 arr6072, sint16 arr6073, sint16 arr6074, sint16 arr6075, sint16 arr6076, sint16 arr6077,
    sint16 arr6080, sint16 arr6081, sint16 arr6082, sint16 arr6083, sint16 arr6084, sint16 arr6085, sint16 arr6086, sint16 arr6087,
    sint16 arr6100, sint16 arr6101, sint16 arr6102, sint16 arr6103, sint16 arr6104, sint16 arr6105, sint16 arr6106, sint16 arr6107,
    sint16 arr6110, sint16 arr6111, sint16 arr6112, sint16 arr6113, sint16 arr6114, sint16 arr6115, sint16 arr6116, sint16 arr6117,
    sint16 arr6120, sint16 arr6121, sint16 arr6122, sint16 arr6123, sint16 arr6124, sint16 arr6125, sint16 arr6126, sint16 arr6127,
    sint16 arr6130, sint16 arr6131, sint16 arr6132, sint16 arr6133, sint16 arr6134, sint16 arr6135, sint16 arr6136, sint16 arr6137,
    sint16 arr6140, sint16 arr6141, sint16 arr6142, sint16 arr6143, sint16 arr6144, sint16 arr6145, sint16 arr6146, sint16 arr6147,
    sint16 arr6150, sint16 arr6151, sint16 arr6152, sint16 arr6153, sint16 arr6154, sint16 arr6155, sint16 arr6156, sint16 arr6157,
    sint16 arr6160, sint16 arr6161, sint16 arr6162, sint16 arr6163, sint16 arr6164, sint16 arr6165, sint16 arr6166, sint16 arr6167,
    sint16 arr6170, sint16 arr6171, sint16 arr6172, sint16 arr6173, sint16 arr6174, sint16 arr6175, sint16 arr6176, sint16 arr6177,
    sint16 arr6180, sint16 arr6181, sint16 arr6182, sint16 arr6183, sint16 arr6184, sint16 arr6185, sint16 arr6186, sint16 arr6187,
    sint16 arr7000, sint16 arr7001, sint16 arr7002, sint16 arr7003, sint16 arr7004, sint16 arr7005, sint16 arr7006, sint16 arr7007,
    sint16 arr7010, sint16 arr7011, sint16 arr7012, sint16 arr7013, sint16 arr7014, sint16 arr7015, sint16 arr7016, sint16 arr7017,
    sint16 arr7020, sint16 arr7021, sint16 arr7022, sint16 arr7023, sint16 arr7024, sint16 arr7025, sint16 arr7026, sint16 arr7027,
    sint16 arr7030, sint16 arr7031, sint16 arr7032, sint16 arr7033, sint16 arr7034, sint16 arr7035, sint16 arr7036, sint16 arr7037,
    sint16 arr7040, sint16 arr7041, sint16 arr7042, sint16 arr7043, sint16 arr7044, sint16 arr7045, sint16 arr7046, sint16 arr7047,
    sint16 arr7050, sint16 arr7051, sint16 arr7052, sint16 arr7053, sint16 arr7054, sint16 arr7055, sint16 arr7056, sint16 arr7057,
    sint16 arr7060, sint16 arr7061, sint16 arr7062, sint16 arr7063, sint16 arr7064, sint16 arr7065, sint16 arr7066, sint16 arr7067,
    sint16 arr7070, sint16 arr7071, sint16 arr7072, sint16 arr7073, sint16 arr7074, sint16 arr7075, sint16 arr7076, sint16 arr7077,
    sint16 arr7080, sint16 arr7081, sint16 arr7082, sint16 arr7083, sint16 arr7084, sint16 arr7085, sint16 arr7086, sint16 arr7087,
    sint16 arr7100, sint16 arr7101, sint16 arr7102, sint16 arr7103, sint16 arr7104, sint16 arr7105, sint16 arr7106, sint16 arr7107,
    sint16 arr7110, sint16 arr7111, sint16 arr7112, sint16 arr7113, sint16 arr7114, sint16 arr7115, sint16 arr7116, sint16 arr7117,
    sint16 arr7120, sint16 arr7121, sint16 arr7122, sint16 arr7123, sint16 arr7124, sint16 arr7125, sint16 arr7126, sint16 arr7127,
    sint16 arr7130, sint16 arr7131, sint16 arr7132, sint16 arr7133, sint16 arr7134, sint16 arr7135, sint16 arr7136, sint16 arr7137,
    sint16 arr7140, sint16 arr7141, sint16 arr7142, sint16 arr7143, sint16 arr7144, sint16 arr7145, sint16 arr7146, sint16 arr7147,
    sint16 arr7150, sint16 arr7151, sint16 arr7152, sint16 arr7153, sint16 arr7154, sint16 arr7155, sint16 arr7156, sint16 arr7157,
    sint16 arr7160, sint16 arr7161, sint16 arr7162, sint16 arr7163, sint16 arr7164, sint16 arr7165, sint16 arr7166, sint16 arr7167,
    sint16 arr7170, sint16 arr7171, sint16 arr7172, sint16 arr7173, sint16 arr7174, sint16 arr7175, sint16 arr7176, sint16 arr7177,
    sint16 arr7180, sint16 arr7181, sint16 arr7182, sint16 arr7183, sint16 arr7184, sint16 arr7185, sint16 arr7186, sint16 arr7187,
    sint16 arr8000, sint16 arr8001, sint16 arr8002, sint16 arr8003, sint16 arr8004, sint16 arr8005, sint16 arr8006, sint16 arr8007,
    sint16 arr8010, sint16 arr8011, sint16 arr8012, sint16 arr8013, sint16 arr8014, sint16 arr8015, sint16 arr8016, sint16 arr8017,
    sint16 arr8020, sint16 arr8021, sint16 arr8022, sint16 arr8023, sint16 arr8024, sint16 arr8025, sint16 arr8026, sint16 arr8027,
    sint16 arr8030, sint16 arr8031, sint16 arr8032, sint16 arr8033, sint16 arr8034, sint16 arr8035, sint16 arr8036, sint16 arr8037,
    sint16 arr8040, sint16 arr8041, sint16 arr8042, sint16 arr8043, sint16 arr8044, sint16 arr8045, sint16 arr8046, sint16 arr8047,
    sint16 arr8050, sint16 arr8051, sint16 arr8052, sint16 arr8053, sint16 arr8054, sint16 arr8055, sint16 arr8056, sint16 arr8057,
    sint16 arr8060, sint16 arr8061, sint16 arr8062, sint16 arr8063, sint16 arr8064, sint16 arr8065, sint16 arr8066, sint16 arr8067,
    sint16 arr8070, sint16 arr8071, sint16 arr8072, sint16 arr8073, sint16 arr8074, sint16 arr8075, sint16 arr8076, sint16 arr8077,
    sint16 arr8080, sint16 arr8081, sint16 arr8082, sint16 arr8083, sint16 arr8084, sint16 arr8085, sint16 arr8086, sint16 arr8087,
    sint16 arr8100, sint16 arr8101, sint16 arr8102, sint16 arr8103, sint16 arr8104, sint16 arr8105, sint16 arr8106, sint16 arr8107,
    sint16 arr8110, sint16 arr8111, sint16 arr8112, sint16 arr8113, sint16 arr8114, sint16 arr8115, sint16 arr8116, sint16 arr8117,
    sint16 arr8120, sint16 arr8121, sint16 arr8122, sint16 arr8123, sint16 arr8124, sint16 arr8125, sint16 arr8126, sint16 arr8127,
    sint16 arr8130, sint16 arr8131, sint16 arr8132, sint16 arr8133, sint16 arr8134, sint16 arr8135, sint16 arr8136, sint16 arr8137,
    sint16 arr8140, sint16 arr8141, sint16 arr8142, sint16 arr8143, sint16 arr8144, sint16 arr8145, sint16 arr8146, sint16 arr8147,
    sint16 arr8150, sint16 arr8151, sint16 arr8152, sint16 arr8153, sint16 arr8154, sint16 arr8155, sint16 arr8156, sint16 arr8157,
    sint16 arr8160, sint16 arr8161, sint16 arr8162, sint16 arr8163, sint16 arr8164, sint16 arr8165, sint16 arr8166, sint16 arr8167,
    sint16 arr8170, sint16 arr8171, sint16 arr8172, sint16 arr8173, sint16 arr8174, sint16 arr8175, sint16 arr8176, sint16 arr8177,
    sint16 arr8180, sint16 arr8181, sint16 arr8182, sint16 arr8183, sint16 arr8184, sint16 arr8185, sint16 arr8186, sint16 arr8187,
    sint16 arr9000, sint16 arr9001, sint16 arr9002, sint16 arr9003, sint16 arr9004, sint16 arr9005, sint16 arr9006, sint16 arr9007,
    sint16 arr9010, sint16 arr9011, sint16 arr9012, sint16 arr9013, sint16 arr9014, sint16 arr9015, sint16 arr9016, sint16 arr9017,
    sint16 arr9020, sint16 arr9021, sint16 arr9022, sint16 arr9023, sint16 arr9024, sint16 arr9025, sint16 arr9026, sint16 arr9027,
    sint16 arr9030, sint16 arr9031, sint16 arr9032, sint16 arr9033, sint16 arr9034, sint16 arr9035, sint16 arr9036, sint16 arr9037,
    sint16 arr9040, sint16 arr9041, sint16 arr9042, sint16 arr9043, sint16 arr9044, sint16 arr9045, sint16 arr9046, sint16 arr9047,
    sint16 arr9050, sint16 arr9051, sint16 arr9052, sint16 arr9053, sint16 arr9054, sint16 arr9055, sint16 arr9056, sint16 arr9057,
    sint16 arr9060, sint16 arr9061, sint16 arr9062, sint16 arr9063, sint16 arr9064, sint16 arr9065, sint16 arr9066, sint16 arr9067,
    sint16 arr9070, sint16 arr9071, sint16 arr9072, sint16 arr9073, sint16 arr9074, sint16 arr9075, sint16 arr9076, sint16 arr9077,
    sint16 arr9080, sint16 arr9081, sint16 arr9082, sint16 arr9083, sint16 arr9084, sint16 arr9085, sint16 arr9086, sint16 arr9087,
    sint16 arr9100, sint16 arr9101, sint16 arr9102, sint16 arr9103, sint16 arr9104, sint16 arr9105, sint16 arr9106, sint16 arr9107,
    sint16 arr9110, sint16 arr9111, sint16 arr9112, sint16 arr9113, sint16 arr9114, sint16 arr9115, sint16 arr9116, sint16 arr9117,
    sint16 arr9120, sint16 arr9121, sint16 arr9122, sint16 arr9123, sint16 arr9124, sint16 arr9125, sint16 arr9126, sint16 arr9127,
    sint16 arr9130, sint16 arr9131, sint16 arr9132, sint16 arr9133, sint16 arr9134, sint16 arr9135, sint16 arr9136, sint16 arr9137,
    sint16 arr9140, sint16 arr9141, sint16 arr9142, sint16 arr9143, sint16 arr9144, sint16 arr9145, sint16 arr9146, sint16 arr9147,
    sint16 arr9150, sint16 arr9151, sint16 arr9152, sint16 arr9153, sint16 arr9154, sint16 arr9155, sint16 arr9156, sint16 arr9157,
    sint16 arr9160, sint16 arr9161, sint16 arr9162, sint16 arr9163, sint16 arr9164, sint16 arr9165, sint16 arr9166, sint16 arr9167,
    sint16 arr9170, sint16 arr9171, sint16 arr9172, sint16 arr9173, sint16 arr9174, sint16 arr9175, sint16 arr9176, sint16 arr9177,
    sint16 arr9180, sint16 arr9181, sint16 arr9182, sint16 arr9183, sint16 arr9184, sint16 arr9185, sint16 arr9186, sint16 arr9187,

    # ghost
    sint16 Q, sint16 Q2, sint16 NQ2, sint16 W9
) =
{
    Q = 4591 /\ Q2 = 2295 /\ NQ2 = -2295 /\
    W9 = -1606 /\

    [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\
    [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] <= [4585, 4585, 4585, 4585, 4585, 4585, 4585, 4585] /\

    [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] >= [-4585, -4585, -4585, -4585, -4585, -4585, -4585, -4585] /\
    true
  &&
    Q = 4591@16 /\ Q2 = 2295@16 /\ NQ2 = (-2295)@16 /\
    W9 = (-1606)@16 /\

    [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\
    [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] <=s [4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16, 4585@16] /\

    [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] >=s [(-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16, (-4585)@16] /\
    true
}

# constants

mov [L0x5555552810, L0x5555552812, L0x5555552814, L0x5555552816, L0x5555552818, L0x555555281a, L0x555555281c, L0x555555281e, L0x5555552820, L0x5555552822, L0x5555552824, L0x5555552826, L0x5555552828, L0x555555282a, L0x555555282c, L0x555555282e] [4591@sint16, 29235@sint16, 0@sint16, 0@sint16, 0@sint16, 0@sint16, 0@sint16, 0@sint16, (-621)@sint16, 1891@sint16, (-803)@sint16, 0@sint16, (-4432)@sint16, 13497@sint16, (-5731)@sint16, (-17729)@sint16];


# setup
mov [L0x7fffffc350, L0x7fffffc352, L0x7fffffc354, L0x7fffffc356, L0x7fffffc358, L0x7fffffc35a, L0x7fffffc35c, L0x7fffffc35e] [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007];
mov [L0x7fffffc360, L0x7fffffc362, L0x7fffffc364, L0x7fffffc366, L0x7fffffc368, L0x7fffffc36a, L0x7fffffc36c, L0x7fffffc36e] [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017];
mov [L0x7fffffc370, L0x7fffffc372, L0x7fffffc374, L0x7fffffc376, L0x7fffffc378, L0x7fffffc37a, L0x7fffffc37c, L0x7fffffc37e] [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027];
mov [L0x7fffffc380, L0x7fffffc382, L0x7fffffc384, L0x7fffffc386, L0x7fffffc388, L0x7fffffc38a, L0x7fffffc38c, L0x7fffffc38e] [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037];
mov [L0x7fffffc390, L0x7fffffc392, L0x7fffffc394, L0x7fffffc396, L0x7fffffc398, L0x7fffffc39a, L0x7fffffc39c, L0x7fffffc39e] [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047];
mov [L0x7fffffc3a0, L0x7fffffc3a2, L0x7fffffc3a4, L0x7fffffc3a6, L0x7fffffc3a8, L0x7fffffc3aa, L0x7fffffc3ac, L0x7fffffc3ae] [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057];
mov [L0x7fffffc3b0, L0x7fffffc3b2, L0x7fffffc3b4, L0x7fffffc3b6, L0x7fffffc3b8, L0x7fffffc3ba, L0x7fffffc3bc, L0x7fffffc3be] [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067];
mov [L0x7fffffc3c0, L0x7fffffc3c2, L0x7fffffc3c4, L0x7fffffc3c6, L0x7fffffc3c8, L0x7fffffc3ca, L0x7fffffc3cc, L0x7fffffc3ce] [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077];
mov [L0x7fffffc3d0, L0x7fffffc3d2, L0x7fffffc3d4, L0x7fffffc3d6, L0x7fffffc3d8, L0x7fffffc3da, L0x7fffffc3dc, L0x7fffffc3de] [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087];
mov [L0x7fffffc3e0, L0x7fffffc3e2, L0x7fffffc3e4, L0x7fffffc3e6, L0x7fffffc3e8, L0x7fffffc3ea, L0x7fffffc3ec, L0x7fffffc3ee] [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107];
mov [L0x7fffffc3f0, L0x7fffffc3f2, L0x7fffffc3f4, L0x7fffffc3f6, L0x7fffffc3f8, L0x7fffffc3fa, L0x7fffffc3fc, L0x7fffffc3fe] [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117];
mov [L0x7fffffc400, L0x7fffffc402, L0x7fffffc404, L0x7fffffc406, L0x7fffffc408, L0x7fffffc40a, L0x7fffffc40c, L0x7fffffc40e] [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127];
mov [L0x7fffffc410, L0x7fffffc412, L0x7fffffc414, L0x7fffffc416, L0x7fffffc418, L0x7fffffc41a, L0x7fffffc41c, L0x7fffffc41e] [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137];
mov [L0x7fffffc420, L0x7fffffc422, L0x7fffffc424, L0x7fffffc426, L0x7fffffc428, L0x7fffffc42a, L0x7fffffc42c, L0x7fffffc42e] [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147];
mov [L0x7fffffc430, L0x7fffffc432, L0x7fffffc434, L0x7fffffc436, L0x7fffffc438, L0x7fffffc43a, L0x7fffffc43c, L0x7fffffc43e] [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157];
mov [L0x7fffffc440, L0x7fffffc442, L0x7fffffc444, L0x7fffffc446, L0x7fffffc448, L0x7fffffc44a, L0x7fffffc44c, L0x7fffffc44e] [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167];
mov [L0x7fffffc450, L0x7fffffc452, L0x7fffffc454, L0x7fffffc456, L0x7fffffc458, L0x7fffffc45a, L0x7fffffc45c, L0x7fffffc45e] [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177];
mov [L0x7fffffc460, L0x7fffffc462, L0x7fffffc464, L0x7fffffc466, L0x7fffffc468, L0x7fffffc46a, L0x7fffffc46c, L0x7fffffc46e] [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187];
mov [L0x7fffffc470, L0x7fffffc472, L0x7fffffc474, L0x7fffffc476, L0x7fffffc478, L0x7fffffc47a, L0x7fffffc47c, L0x7fffffc47e] [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007];
mov [L0x7fffffc480, L0x7fffffc482, L0x7fffffc484, L0x7fffffc486, L0x7fffffc488, L0x7fffffc48a, L0x7fffffc48c, L0x7fffffc48e] [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017];
mov [L0x7fffffc490, L0x7fffffc492, L0x7fffffc494, L0x7fffffc496, L0x7fffffc498, L0x7fffffc49a, L0x7fffffc49c, L0x7fffffc49e] [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027];
mov [L0x7fffffc4a0, L0x7fffffc4a2, L0x7fffffc4a4, L0x7fffffc4a6, L0x7fffffc4a8, L0x7fffffc4aa, L0x7fffffc4ac, L0x7fffffc4ae] [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037];
mov [L0x7fffffc4b0, L0x7fffffc4b2, L0x7fffffc4b4, L0x7fffffc4b6, L0x7fffffc4b8, L0x7fffffc4ba, L0x7fffffc4bc, L0x7fffffc4be] [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047];
mov [L0x7fffffc4c0, L0x7fffffc4c2, L0x7fffffc4c4, L0x7fffffc4c6, L0x7fffffc4c8, L0x7fffffc4ca, L0x7fffffc4cc, L0x7fffffc4ce] [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057];
mov [L0x7fffffc4d0, L0x7fffffc4d2, L0x7fffffc4d4, L0x7fffffc4d6, L0x7fffffc4d8, L0x7fffffc4da, L0x7fffffc4dc, L0x7fffffc4de] [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067];
mov [L0x7fffffc4e0, L0x7fffffc4e2, L0x7fffffc4e4, L0x7fffffc4e6, L0x7fffffc4e8, L0x7fffffc4ea, L0x7fffffc4ec, L0x7fffffc4ee] [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077];
mov [L0x7fffffc4f0, L0x7fffffc4f2, L0x7fffffc4f4, L0x7fffffc4f6, L0x7fffffc4f8, L0x7fffffc4fa, L0x7fffffc4fc, L0x7fffffc4fe] [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087];
mov [L0x7fffffc500, L0x7fffffc502, L0x7fffffc504, L0x7fffffc506, L0x7fffffc508, L0x7fffffc50a, L0x7fffffc50c, L0x7fffffc50e] [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107];
mov [L0x7fffffc510, L0x7fffffc512, L0x7fffffc514, L0x7fffffc516, L0x7fffffc518, L0x7fffffc51a, L0x7fffffc51c, L0x7fffffc51e] [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117];
mov [L0x7fffffc520, L0x7fffffc522, L0x7fffffc524, L0x7fffffc526, L0x7fffffc528, L0x7fffffc52a, L0x7fffffc52c, L0x7fffffc52e] [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127];
mov [L0x7fffffc530, L0x7fffffc532, L0x7fffffc534, L0x7fffffc536, L0x7fffffc538, L0x7fffffc53a, L0x7fffffc53c, L0x7fffffc53e] [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137];
mov [L0x7fffffc540, L0x7fffffc542, L0x7fffffc544, L0x7fffffc546, L0x7fffffc548, L0x7fffffc54a, L0x7fffffc54c, L0x7fffffc54e] [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147];
mov [L0x7fffffc550, L0x7fffffc552, L0x7fffffc554, L0x7fffffc556, L0x7fffffc558, L0x7fffffc55a, L0x7fffffc55c, L0x7fffffc55e] [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157];
mov [L0x7fffffc560, L0x7fffffc562, L0x7fffffc564, L0x7fffffc566, L0x7fffffc568, L0x7fffffc56a, L0x7fffffc56c, L0x7fffffc56e] [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167];
mov [L0x7fffffc570, L0x7fffffc572, L0x7fffffc574, L0x7fffffc576, L0x7fffffc578, L0x7fffffc57a, L0x7fffffc57c, L0x7fffffc57e] [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177];
mov [L0x7fffffc580, L0x7fffffc582, L0x7fffffc584, L0x7fffffc586, L0x7fffffc588, L0x7fffffc58a, L0x7fffffc58c, L0x7fffffc58e] [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187];
mov [L0x7fffffc590, L0x7fffffc592, L0x7fffffc594, L0x7fffffc596, L0x7fffffc598, L0x7fffffc59a, L0x7fffffc59c, L0x7fffffc59e] [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007];
mov [L0x7fffffc5a0, L0x7fffffc5a2, L0x7fffffc5a4, L0x7fffffc5a6, L0x7fffffc5a8, L0x7fffffc5aa, L0x7fffffc5ac, L0x7fffffc5ae] [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017];
mov [L0x7fffffc5b0, L0x7fffffc5b2, L0x7fffffc5b4, L0x7fffffc5b6, L0x7fffffc5b8, L0x7fffffc5ba, L0x7fffffc5bc, L0x7fffffc5be] [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027];
mov [L0x7fffffc5c0, L0x7fffffc5c2, L0x7fffffc5c4, L0x7fffffc5c6, L0x7fffffc5c8, L0x7fffffc5ca, L0x7fffffc5cc, L0x7fffffc5ce] [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037];
mov [L0x7fffffc5d0, L0x7fffffc5d2, L0x7fffffc5d4, L0x7fffffc5d6, L0x7fffffc5d8, L0x7fffffc5da, L0x7fffffc5dc, L0x7fffffc5de] [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047];
mov [L0x7fffffc5e0, L0x7fffffc5e2, L0x7fffffc5e4, L0x7fffffc5e6, L0x7fffffc5e8, L0x7fffffc5ea, L0x7fffffc5ec, L0x7fffffc5ee] [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057];
mov [L0x7fffffc5f0, L0x7fffffc5f2, L0x7fffffc5f4, L0x7fffffc5f6, L0x7fffffc5f8, L0x7fffffc5fa, L0x7fffffc5fc, L0x7fffffc5fe] [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067];
mov [L0x7fffffc600, L0x7fffffc602, L0x7fffffc604, L0x7fffffc606, L0x7fffffc608, L0x7fffffc60a, L0x7fffffc60c, L0x7fffffc60e] [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077];
mov [L0x7fffffc610, L0x7fffffc612, L0x7fffffc614, L0x7fffffc616, L0x7fffffc618, L0x7fffffc61a, L0x7fffffc61c, L0x7fffffc61e] [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087];
mov [L0x7fffffc620, L0x7fffffc622, L0x7fffffc624, L0x7fffffc626, L0x7fffffc628, L0x7fffffc62a, L0x7fffffc62c, L0x7fffffc62e] [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107];
mov [L0x7fffffc630, L0x7fffffc632, L0x7fffffc634, L0x7fffffc636, L0x7fffffc638, L0x7fffffc63a, L0x7fffffc63c, L0x7fffffc63e] [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117];
mov [L0x7fffffc640, L0x7fffffc642, L0x7fffffc644, L0x7fffffc646, L0x7fffffc648, L0x7fffffc64a, L0x7fffffc64c, L0x7fffffc64e] [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127];
mov [L0x7fffffc650, L0x7fffffc652, L0x7fffffc654, L0x7fffffc656, L0x7fffffc658, L0x7fffffc65a, L0x7fffffc65c, L0x7fffffc65e] [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137];
mov [L0x7fffffc660, L0x7fffffc662, L0x7fffffc664, L0x7fffffc666, L0x7fffffc668, L0x7fffffc66a, L0x7fffffc66c, L0x7fffffc66e] [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147];
mov [L0x7fffffc670, L0x7fffffc672, L0x7fffffc674, L0x7fffffc676, L0x7fffffc678, L0x7fffffc67a, L0x7fffffc67c, L0x7fffffc67e] [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157];
mov [L0x7fffffc680, L0x7fffffc682, L0x7fffffc684, L0x7fffffc686, L0x7fffffc688, L0x7fffffc68a, L0x7fffffc68c, L0x7fffffc68e] [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167];
mov [L0x7fffffc690, L0x7fffffc692, L0x7fffffc694, L0x7fffffc696, L0x7fffffc698, L0x7fffffc69a, L0x7fffffc69c, L0x7fffffc69e] [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177];
mov [L0x7fffffc6a0, L0x7fffffc6a2, L0x7fffffc6a4, L0x7fffffc6a6, L0x7fffffc6a8, L0x7fffffc6aa, L0x7fffffc6ac, L0x7fffffc6ae] [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187];
mov [L0x7fffffc6b0, L0x7fffffc6b2, L0x7fffffc6b4, L0x7fffffc6b6, L0x7fffffc6b8, L0x7fffffc6ba, L0x7fffffc6bc, L0x7fffffc6be] [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007];
mov [L0x7fffffc6c0, L0x7fffffc6c2, L0x7fffffc6c4, L0x7fffffc6c6, L0x7fffffc6c8, L0x7fffffc6ca, L0x7fffffc6cc, L0x7fffffc6ce] [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017];
mov [L0x7fffffc6d0, L0x7fffffc6d2, L0x7fffffc6d4, L0x7fffffc6d6, L0x7fffffc6d8, L0x7fffffc6da, L0x7fffffc6dc, L0x7fffffc6de] [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027];
mov [L0x7fffffc6e0, L0x7fffffc6e2, L0x7fffffc6e4, L0x7fffffc6e6, L0x7fffffc6e8, L0x7fffffc6ea, L0x7fffffc6ec, L0x7fffffc6ee] [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037];
mov [L0x7fffffc6f0, L0x7fffffc6f2, L0x7fffffc6f4, L0x7fffffc6f6, L0x7fffffc6f8, L0x7fffffc6fa, L0x7fffffc6fc, L0x7fffffc6fe] [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047];
mov [L0x7fffffc700, L0x7fffffc702, L0x7fffffc704, L0x7fffffc706, L0x7fffffc708, L0x7fffffc70a, L0x7fffffc70c, L0x7fffffc70e] [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057];
mov [L0x7fffffc710, L0x7fffffc712, L0x7fffffc714, L0x7fffffc716, L0x7fffffc718, L0x7fffffc71a, L0x7fffffc71c, L0x7fffffc71e] [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067];
mov [L0x7fffffc720, L0x7fffffc722, L0x7fffffc724, L0x7fffffc726, L0x7fffffc728, L0x7fffffc72a, L0x7fffffc72c, L0x7fffffc72e] [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077];
mov [L0x7fffffc730, L0x7fffffc732, L0x7fffffc734, L0x7fffffc736, L0x7fffffc738, L0x7fffffc73a, L0x7fffffc73c, L0x7fffffc73e] [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087];
mov [L0x7fffffc740, L0x7fffffc742, L0x7fffffc744, L0x7fffffc746, L0x7fffffc748, L0x7fffffc74a, L0x7fffffc74c, L0x7fffffc74e] [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107];
mov [L0x7fffffc750, L0x7fffffc752, L0x7fffffc754, L0x7fffffc756, L0x7fffffc758, L0x7fffffc75a, L0x7fffffc75c, L0x7fffffc75e] [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117];
mov [L0x7fffffc760, L0x7fffffc762, L0x7fffffc764, L0x7fffffc766, L0x7fffffc768, L0x7fffffc76a, L0x7fffffc76c, L0x7fffffc76e] [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127];
mov [L0x7fffffc770, L0x7fffffc772, L0x7fffffc774, L0x7fffffc776, L0x7fffffc778, L0x7fffffc77a, L0x7fffffc77c, L0x7fffffc77e] [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137];
mov [L0x7fffffc780, L0x7fffffc782, L0x7fffffc784, L0x7fffffc786, L0x7fffffc788, L0x7fffffc78a, L0x7fffffc78c, L0x7fffffc78e] [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147];
mov [L0x7fffffc790, L0x7fffffc792, L0x7fffffc794, L0x7fffffc796, L0x7fffffc798, L0x7fffffc79a, L0x7fffffc79c, L0x7fffffc79e] [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157];
mov [L0x7fffffc7a0, L0x7fffffc7a2, L0x7fffffc7a4, L0x7fffffc7a6, L0x7fffffc7a8, L0x7fffffc7aa, L0x7fffffc7ac, L0x7fffffc7ae] [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167];
mov [L0x7fffffc7b0, L0x7fffffc7b2, L0x7fffffc7b4, L0x7fffffc7b6, L0x7fffffc7b8, L0x7fffffc7ba, L0x7fffffc7bc, L0x7fffffc7be] [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177];
mov [L0x7fffffc7c0, L0x7fffffc7c2, L0x7fffffc7c4, L0x7fffffc7c6, L0x7fffffc7c8, L0x7fffffc7ca, L0x7fffffc7cc, L0x7fffffc7ce] [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187];
mov [L0x7fffffc7d0, L0x7fffffc7d2, L0x7fffffc7d4, L0x7fffffc7d6, L0x7fffffc7d8, L0x7fffffc7da, L0x7fffffc7dc, L0x7fffffc7de] [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007];
mov [L0x7fffffc7e0, L0x7fffffc7e2, L0x7fffffc7e4, L0x7fffffc7e6, L0x7fffffc7e8, L0x7fffffc7ea, L0x7fffffc7ec, L0x7fffffc7ee] [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017];
mov [L0x7fffffc7f0, L0x7fffffc7f2, L0x7fffffc7f4, L0x7fffffc7f6, L0x7fffffc7f8, L0x7fffffc7fa, L0x7fffffc7fc, L0x7fffffc7fe] [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027];
mov [L0x7fffffc800, L0x7fffffc802, L0x7fffffc804, L0x7fffffc806, L0x7fffffc808, L0x7fffffc80a, L0x7fffffc80c, L0x7fffffc80e] [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037];
mov [L0x7fffffc810, L0x7fffffc812, L0x7fffffc814, L0x7fffffc816, L0x7fffffc818, L0x7fffffc81a, L0x7fffffc81c, L0x7fffffc81e] [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047];
mov [L0x7fffffc820, L0x7fffffc822, L0x7fffffc824, L0x7fffffc826, L0x7fffffc828, L0x7fffffc82a, L0x7fffffc82c, L0x7fffffc82e] [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057];
mov [L0x7fffffc830, L0x7fffffc832, L0x7fffffc834, L0x7fffffc836, L0x7fffffc838, L0x7fffffc83a, L0x7fffffc83c, L0x7fffffc83e] [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067];
mov [L0x7fffffc840, L0x7fffffc842, L0x7fffffc844, L0x7fffffc846, L0x7fffffc848, L0x7fffffc84a, L0x7fffffc84c, L0x7fffffc84e] [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077];
mov [L0x7fffffc850, L0x7fffffc852, L0x7fffffc854, L0x7fffffc856, L0x7fffffc858, L0x7fffffc85a, L0x7fffffc85c, L0x7fffffc85e] [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087];
mov [L0x7fffffc860, L0x7fffffc862, L0x7fffffc864, L0x7fffffc866, L0x7fffffc868, L0x7fffffc86a, L0x7fffffc86c, L0x7fffffc86e] [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107];
mov [L0x7fffffc870, L0x7fffffc872, L0x7fffffc874, L0x7fffffc876, L0x7fffffc878, L0x7fffffc87a, L0x7fffffc87c, L0x7fffffc87e] [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117];
mov [L0x7fffffc880, L0x7fffffc882, L0x7fffffc884, L0x7fffffc886, L0x7fffffc888, L0x7fffffc88a, L0x7fffffc88c, L0x7fffffc88e] [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127];
mov [L0x7fffffc890, L0x7fffffc892, L0x7fffffc894, L0x7fffffc896, L0x7fffffc898, L0x7fffffc89a, L0x7fffffc89c, L0x7fffffc89e] [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137];
mov [L0x7fffffc8a0, L0x7fffffc8a2, L0x7fffffc8a4, L0x7fffffc8a6, L0x7fffffc8a8, L0x7fffffc8aa, L0x7fffffc8ac, L0x7fffffc8ae] [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147];
mov [L0x7fffffc8b0, L0x7fffffc8b2, L0x7fffffc8b4, L0x7fffffc8b6, L0x7fffffc8b8, L0x7fffffc8ba, L0x7fffffc8bc, L0x7fffffc8be] [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157];
mov [L0x7fffffc8c0, L0x7fffffc8c2, L0x7fffffc8c4, L0x7fffffc8c6, L0x7fffffc8c8, L0x7fffffc8ca, L0x7fffffc8cc, L0x7fffffc8ce] [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167];
mov [L0x7fffffc8d0, L0x7fffffc8d2, L0x7fffffc8d4, L0x7fffffc8d6, L0x7fffffc8d8, L0x7fffffc8da, L0x7fffffc8dc, L0x7fffffc8de] [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177];
mov [L0x7fffffc8e0, L0x7fffffc8e2, L0x7fffffc8e4, L0x7fffffc8e6, L0x7fffffc8e8, L0x7fffffc8ea, L0x7fffffc8ec, L0x7fffffc8ee] [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187];
mov [L0x7fffffc8f0, L0x7fffffc8f2, L0x7fffffc8f4, L0x7fffffc8f6, L0x7fffffc8f8, L0x7fffffc8fa, L0x7fffffc8fc, L0x7fffffc8fe] [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007];
mov [L0x7fffffc900, L0x7fffffc902, L0x7fffffc904, L0x7fffffc906, L0x7fffffc908, L0x7fffffc90a, L0x7fffffc90c, L0x7fffffc90e] [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017];
mov [L0x7fffffc910, L0x7fffffc912, L0x7fffffc914, L0x7fffffc916, L0x7fffffc918, L0x7fffffc91a, L0x7fffffc91c, L0x7fffffc91e] [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027];
mov [L0x7fffffc920, L0x7fffffc922, L0x7fffffc924, L0x7fffffc926, L0x7fffffc928, L0x7fffffc92a, L0x7fffffc92c, L0x7fffffc92e] [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037];
mov [L0x7fffffc930, L0x7fffffc932, L0x7fffffc934, L0x7fffffc936, L0x7fffffc938, L0x7fffffc93a, L0x7fffffc93c, L0x7fffffc93e] [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047];
mov [L0x7fffffc940, L0x7fffffc942, L0x7fffffc944, L0x7fffffc946, L0x7fffffc948, L0x7fffffc94a, L0x7fffffc94c, L0x7fffffc94e] [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057];
mov [L0x7fffffc950, L0x7fffffc952, L0x7fffffc954, L0x7fffffc956, L0x7fffffc958, L0x7fffffc95a, L0x7fffffc95c, L0x7fffffc95e] [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067];
mov [L0x7fffffc960, L0x7fffffc962, L0x7fffffc964, L0x7fffffc966, L0x7fffffc968, L0x7fffffc96a, L0x7fffffc96c, L0x7fffffc96e] [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077];
mov [L0x7fffffc970, L0x7fffffc972, L0x7fffffc974, L0x7fffffc976, L0x7fffffc978, L0x7fffffc97a, L0x7fffffc97c, L0x7fffffc97e] [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087];
mov [L0x7fffffc980, L0x7fffffc982, L0x7fffffc984, L0x7fffffc986, L0x7fffffc988, L0x7fffffc98a, L0x7fffffc98c, L0x7fffffc98e] [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107];
mov [L0x7fffffc990, L0x7fffffc992, L0x7fffffc994, L0x7fffffc996, L0x7fffffc998, L0x7fffffc99a, L0x7fffffc99c, L0x7fffffc99e] [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117];
mov [L0x7fffffc9a0, L0x7fffffc9a2, L0x7fffffc9a4, L0x7fffffc9a6, L0x7fffffc9a8, L0x7fffffc9aa, L0x7fffffc9ac, L0x7fffffc9ae] [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127];
mov [L0x7fffffc9b0, L0x7fffffc9b2, L0x7fffffc9b4, L0x7fffffc9b6, L0x7fffffc9b8, L0x7fffffc9ba, L0x7fffffc9bc, L0x7fffffc9be] [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137];
mov [L0x7fffffc9c0, L0x7fffffc9c2, L0x7fffffc9c4, L0x7fffffc9c6, L0x7fffffc9c8, L0x7fffffc9ca, L0x7fffffc9cc, L0x7fffffc9ce] [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147];
mov [L0x7fffffc9d0, L0x7fffffc9d2, L0x7fffffc9d4, L0x7fffffc9d6, L0x7fffffc9d8, L0x7fffffc9da, L0x7fffffc9dc, L0x7fffffc9de] [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157];
mov [L0x7fffffc9e0, L0x7fffffc9e2, L0x7fffffc9e4, L0x7fffffc9e6, L0x7fffffc9e8, L0x7fffffc9ea, L0x7fffffc9ec, L0x7fffffc9ee] [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167];
mov [L0x7fffffc9f0, L0x7fffffc9f2, L0x7fffffc9f4, L0x7fffffc9f6, L0x7fffffc9f8, L0x7fffffc9fa, L0x7fffffc9fc, L0x7fffffc9fe] [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177];
mov [L0x7fffffca00, L0x7fffffca02, L0x7fffffca04, L0x7fffffca06, L0x7fffffca08, L0x7fffffca0a, L0x7fffffca0c, L0x7fffffca0e] [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187];
mov [L0x7fffffca10, L0x7fffffca12, L0x7fffffca14, L0x7fffffca16, L0x7fffffca18, L0x7fffffca1a, L0x7fffffca1c, L0x7fffffca1e] [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007];
mov [L0x7fffffca20, L0x7fffffca22, L0x7fffffca24, L0x7fffffca26, L0x7fffffca28, L0x7fffffca2a, L0x7fffffca2c, L0x7fffffca2e] [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017];
mov [L0x7fffffca30, L0x7fffffca32, L0x7fffffca34, L0x7fffffca36, L0x7fffffca38, L0x7fffffca3a, L0x7fffffca3c, L0x7fffffca3e] [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027];
mov [L0x7fffffca40, L0x7fffffca42, L0x7fffffca44, L0x7fffffca46, L0x7fffffca48, L0x7fffffca4a, L0x7fffffca4c, L0x7fffffca4e] [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037];
mov [L0x7fffffca50, L0x7fffffca52, L0x7fffffca54, L0x7fffffca56, L0x7fffffca58, L0x7fffffca5a, L0x7fffffca5c, L0x7fffffca5e] [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047];
mov [L0x7fffffca60, L0x7fffffca62, L0x7fffffca64, L0x7fffffca66, L0x7fffffca68, L0x7fffffca6a, L0x7fffffca6c, L0x7fffffca6e] [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057];
mov [L0x7fffffca70, L0x7fffffca72, L0x7fffffca74, L0x7fffffca76, L0x7fffffca78, L0x7fffffca7a, L0x7fffffca7c, L0x7fffffca7e] [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067];
mov [L0x7fffffca80, L0x7fffffca82, L0x7fffffca84, L0x7fffffca86, L0x7fffffca88, L0x7fffffca8a, L0x7fffffca8c, L0x7fffffca8e] [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077];
mov [L0x7fffffca90, L0x7fffffca92, L0x7fffffca94, L0x7fffffca96, L0x7fffffca98, L0x7fffffca9a, L0x7fffffca9c, L0x7fffffca9e] [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087];
mov [L0x7fffffcaa0, L0x7fffffcaa2, L0x7fffffcaa4, L0x7fffffcaa6, L0x7fffffcaa8, L0x7fffffcaaa, L0x7fffffcaac, L0x7fffffcaae] [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107];
mov [L0x7fffffcab0, L0x7fffffcab2, L0x7fffffcab4, L0x7fffffcab6, L0x7fffffcab8, L0x7fffffcaba, L0x7fffffcabc, L0x7fffffcabe] [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117];
mov [L0x7fffffcac0, L0x7fffffcac2, L0x7fffffcac4, L0x7fffffcac6, L0x7fffffcac8, L0x7fffffcaca, L0x7fffffcacc, L0x7fffffcace] [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127];
mov [L0x7fffffcad0, L0x7fffffcad2, L0x7fffffcad4, L0x7fffffcad6, L0x7fffffcad8, L0x7fffffcada, L0x7fffffcadc, L0x7fffffcade] [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137];
mov [L0x7fffffcae0, L0x7fffffcae2, L0x7fffffcae4, L0x7fffffcae6, L0x7fffffcae8, L0x7fffffcaea, L0x7fffffcaec, L0x7fffffcaee] [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147];
mov [L0x7fffffcaf0, L0x7fffffcaf2, L0x7fffffcaf4, L0x7fffffcaf6, L0x7fffffcaf8, L0x7fffffcafa, L0x7fffffcafc, L0x7fffffcafe] [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157];
mov [L0x7fffffcb00, L0x7fffffcb02, L0x7fffffcb04, L0x7fffffcb06, L0x7fffffcb08, L0x7fffffcb0a, L0x7fffffcb0c, L0x7fffffcb0e] [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167];
mov [L0x7fffffcb10, L0x7fffffcb12, L0x7fffffcb14, L0x7fffffcb16, L0x7fffffcb18, L0x7fffffcb1a, L0x7fffffcb1c, L0x7fffffcb1e] [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177];
mov [L0x7fffffcb20, L0x7fffffcb22, L0x7fffffcb24, L0x7fffffcb26, L0x7fffffcb28, L0x7fffffcb2a, L0x7fffffcb2c, L0x7fffffcb2e] [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187];
mov [L0x7fffffcb30, L0x7fffffcb32, L0x7fffffcb34, L0x7fffffcb36, L0x7fffffcb38, L0x7fffffcb3a, L0x7fffffcb3c, L0x7fffffcb3e] [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007];
mov [L0x7fffffcb40, L0x7fffffcb42, L0x7fffffcb44, L0x7fffffcb46, L0x7fffffcb48, L0x7fffffcb4a, L0x7fffffcb4c, L0x7fffffcb4e] [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017];
mov [L0x7fffffcb50, L0x7fffffcb52, L0x7fffffcb54, L0x7fffffcb56, L0x7fffffcb58, L0x7fffffcb5a, L0x7fffffcb5c, L0x7fffffcb5e] [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027];
mov [L0x7fffffcb60, L0x7fffffcb62, L0x7fffffcb64, L0x7fffffcb66, L0x7fffffcb68, L0x7fffffcb6a, L0x7fffffcb6c, L0x7fffffcb6e] [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037];
mov [L0x7fffffcb70, L0x7fffffcb72, L0x7fffffcb74, L0x7fffffcb76, L0x7fffffcb78, L0x7fffffcb7a, L0x7fffffcb7c, L0x7fffffcb7e] [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047];
mov [L0x7fffffcb80, L0x7fffffcb82, L0x7fffffcb84, L0x7fffffcb86, L0x7fffffcb88, L0x7fffffcb8a, L0x7fffffcb8c, L0x7fffffcb8e] [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057];
mov [L0x7fffffcb90, L0x7fffffcb92, L0x7fffffcb94, L0x7fffffcb96, L0x7fffffcb98, L0x7fffffcb9a, L0x7fffffcb9c, L0x7fffffcb9e] [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067];
mov [L0x7fffffcba0, L0x7fffffcba2, L0x7fffffcba4, L0x7fffffcba6, L0x7fffffcba8, L0x7fffffcbaa, L0x7fffffcbac, L0x7fffffcbae] [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077];
mov [L0x7fffffcbb0, L0x7fffffcbb2, L0x7fffffcbb4, L0x7fffffcbb6, L0x7fffffcbb8, L0x7fffffcbba, L0x7fffffcbbc, L0x7fffffcbbe] [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087];
mov [L0x7fffffcbc0, L0x7fffffcbc2, L0x7fffffcbc4, L0x7fffffcbc6, L0x7fffffcbc8, L0x7fffffcbca, L0x7fffffcbcc, L0x7fffffcbce] [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107];
mov [L0x7fffffcbd0, L0x7fffffcbd2, L0x7fffffcbd4, L0x7fffffcbd6, L0x7fffffcbd8, L0x7fffffcbda, L0x7fffffcbdc, L0x7fffffcbde] [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117];
mov [L0x7fffffcbe0, L0x7fffffcbe2, L0x7fffffcbe4, L0x7fffffcbe6, L0x7fffffcbe8, L0x7fffffcbea, L0x7fffffcbec, L0x7fffffcbee] [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127];
mov [L0x7fffffcbf0, L0x7fffffcbf2, L0x7fffffcbf4, L0x7fffffcbf6, L0x7fffffcbf8, L0x7fffffcbfa, L0x7fffffcbfc, L0x7fffffcbfe] [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137];
mov [L0x7fffffcc00, L0x7fffffcc02, L0x7fffffcc04, L0x7fffffcc06, L0x7fffffcc08, L0x7fffffcc0a, L0x7fffffcc0c, L0x7fffffcc0e] [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147];
mov [L0x7fffffcc10, L0x7fffffcc12, L0x7fffffcc14, L0x7fffffcc16, L0x7fffffcc18, L0x7fffffcc1a, L0x7fffffcc1c, L0x7fffffcc1e] [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157];
mov [L0x7fffffcc20, L0x7fffffcc22, L0x7fffffcc24, L0x7fffffcc26, L0x7fffffcc28, L0x7fffffcc2a, L0x7fffffcc2c, L0x7fffffcc2e] [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167];
mov [L0x7fffffcc30, L0x7fffffcc32, L0x7fffffcc34, L0x7fffffcc36, L0x7fffffcc38, L0x7fffffcc3a, L0x7fffffcc3c, L0x7fffffcc3e] [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177];
mov [L0x7fffffcc40, L0x7fffffcc42, L0x7fffffcc44, L0x7fffffcc46, L0x7fffffcc48, L0x7fffffcc4a, L0x7fffffcc4c, L0x7fffffcc4e] [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187];
mov [L0x7fffffcc50, L0x7fffffcc52, L0x7fffffcc54, L0x7fffffcc56, L0x7fffffcc58, L0x7fffffcc5a, L0x7fffffcc5c, L0x7fffffcc5e] [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007];
mov [L0x7fffffcc60, L0x7fffffcc62, L0x7fffffcc64, L0x7fffffcc66, L0x7fffffcc68, L0x7fffffcc6a, L0x7fffffcc6c, L0x7fffffcc6e] [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017];
mov [L0x7fffffcc70, L0x7fffffcc72, L0x7fffffcc74, L0x7fffffcc76, L0x7fffffcc78, L0x7fffffcc7a, L0x7fffffcc7c, L0x7fffffcc7e] [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027];
mov [L0x7fffffcc80, L0x7fffffcc82, L0x7fffffcc84, L0x7fffffcc86, L0x7fffffcc88, L0x7fffffcc8a, L0x7fffffcc8c, L0x7fffffcc8e] [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037];
mov [L0x7fffffcc90, L0x7fffffcc92, L0x7fffffcc94, L0x7fffffcc96, L0x7fffffcc98, L0x7fffffcc9a, L0x7fffffcc9c, L0x7fffffcc9e] [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047];
mov [L0x7fffffcca0, L0x7fffffcca2, L0x7fffffcca4, L0x7fffffcca6, L0x7fffffcca8, L0x7fffffccaa, L0x7fffffccac, L0x7fffffccae] [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057];
mov [L0x7fffffccb0, L0x7fffffccb2, L0x7fffffccb4, L0x7fffffccb6, L0x7fffffccb8, L0x7fffffccba, L0x7fffffccbc, L0x7fffffccbe] [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067];
mov [L0x7fffffccc0, L0x7fffffccc2, L0x7fffffccc4, L0x7fffffccc6, L0x7fffffccc8, L0x7fffffccca, L0x7fffffcccc, L0x7fffffccce] [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077];
mov [L0x7fffffccd0, L0x7fffffccd2, L0x7fffffccd4, L0x7fffffccd6, L0x7fffffccd8, L0x7fffffccda, L0x7fffffccdc, L0x7fffffccde] [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087];
mov [L0x7fffffcce0, L0x7fffffcce2, L0x7fffffcce4, L0x7fffffcce6, L0x7fffffcce8, L0x7fffffccea, L0x7fffffccec, L0x7fffffccee] [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107];
mov [L0x7fffffccf0, L0x7fffffccf2, L0x7fffffccf4, L0x7fffffccf6, L0x7fffffccf8, L0x7fffffccfa, L0x7fffffccfc, L0x7fffffccfe] [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117];
mov [L0x7fffffcd00, L0x7fffffcd02, L0x7fffffcd04, L0x7fffffcd06, L0x7fffffcd08, L0x7fffffcd0a, L0x7fffffcd0c, L0x7fffffcd0e] [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127];
mov [L0x7fffffcd10, L0x7fffffcd12, L0x7fffffcd14, L0x7fffffcd16, L0x7fffffcd18, L0x7fffffcd1a, L0x7fffffcd1c, L0x7fffffcd1e] [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137];
mov [L0x7fffffcd20, L0x7fffffcd22, L0x7fffffcd24, L0x7fffffcd26, L0x7fffffcd28, L0x7fffffcd2a, L0x7fffffcd2c, L0x7fffffcd2e] [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147];
mov [L0x7fffffcd30, L0x7fffffcd32, L0x7fffffcd34, L0x7fffffcd36, L0x7fffffcd38, L0x7fffffcd3a, L0x7fffffcd3c, L0x7fffffcd3e] [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157];
mov [L0x7fffffcd40, L0x7fffffcd42, L0x7fffffcd44, L0x7fffffcd46, L0x7fffffcd48, L0x7fffffcd4a, L0x7fffffcd4c, L0x7fffffcd4e] [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167];
mov [L0x7fffffcd50, L0x7fffffcd52, L0x7fffffcd54, L0x7fffffcd56, L0x7fffffcd58, L0x7fffffcd5a, L0x7fffffcd5c, L0x7fffffcd5e] [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177];
mov [L0x7fffffcd60, L0x7fffffcd62, L0x7fffffcd64, L0x7fffffcd66, L0x7fffffcd68, L0x7fffffcd6a, L0x7fffffcd6c, L0x7fffffcd6e] [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187];
mov [L0x7fffffcd70, L0x7fffffcd72, L0x7fffffcd74, L0x7fffffcd76, L0x7fffffcd78, L0x7fffffcd7a, L0x7fffffcd7c, L0x7fffffcd7e] [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007];
mov [L0x7fffffcd80, L0x7fffffcd82, L0x7fffffcd84, L0x7fffffcd86, L0x7fffffcd88, L0x7fffffcd8a, L0x7fffffcd8c, L0x7fffffcd8e] [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017];
mov [L0x7fffffcd90, L0x7fffffcd92, L0x7fffffcd94, L0x7fffffcd96, L0x7fffffcd98, L0x7fffffcd9a, L0x7fffffcd9c, L0x7fffffcd9e] [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027];
mov [L0x7fffffcda0, L0x7fffffcda2, L0x7fffffcda4, L0x7fffffcda6, L0x7fffffcda8, L0x7fffffcdaa, L0x7fffffcdac, L0x7fffffcdae] [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037];
mov [L0x7fffffcdb0, L0x7fffffcdb2, L0x7fffffcdb4, L0x7fffffcdb6, L0x7fffffcdb8, L0x7fffffcdba, L0x7fffffcdbc, L0x7fffffcdbe] [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047];
mov [L0x7fffffcdc0, L0x7fffffcdc2, L0x7fffffcdc4, L0x7fffffcdc6, L0x7fffffcdc8, L0x7fffffcdca, L0x7fffffcdcc, L0x7fffffcdce] [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057];
mov [L0x7fffffcdd0, L0x7fffffcdd2, L0x7fffffcdd4, L0x7fffffcdd6, L0x7fffffcdd8, L0x7fffffcdda, L0x7fffffcddc, L0x7fffffcdde] [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067];
mov [L0x7fffffcde0, L0x7fffffcde2, L0x7fffffcde4, L0x7fffffcde6, L0x7fffffcde8, L0x7fffffcdea, L0x7fffffcdec, L0x7fffffcdee] [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077];
mov [L0x7fffffcdf0, L0x7fffffcdf2, L0x7fffffcdf4, L0x7fffffcdf6, L0x7fffffcdf8, L0x7fffffcdfa, L0x7fffffcdfc, L0x7fffffcdfe] [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087];
mov [L0x7fffffce00, L0x7fffffce02, L0x7fffffce04, L0x7fffffce06, L0x7fffffce08, L0x7fffffce0a, L0x7fffffce0c, L0x7fffffce0e] [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107];
mov [L0x7fffffce10, L0x7fffffce12, L0x7fffffce14, L0x7fffffce16, L0x7fffffce18, L0x7fffffce1a, L0x7fffffce1c, L0x7fffffce1e] [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117];
mov [L0x7fffffce20, L0x7fffffce22, L0x7fffffce24, L0x7fffffce26, L0x7fffffce28, L0x7fffffce2a, L0x7fffffce2c, L0x7fffffce2e] [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127];
mov [L0x7fffffce30, L0x7fffffce32, L0x7fffffce34, L0x7fffffce36, L0x7fffffce38, L0x7fffffce3a, L0x7fffffce3c, L0x7fffffce3e] [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137];
mov [L0x7fffffce40, L0x7fffffce42, L0x7fffffce44, L0x7fffffce46, L0x7fffffce48, L0x7fffffce4a, L0x7fffffce4c, L0x7fffffce4e] [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147];
mov [L0x7fffffce50, L0x7fffffce52, L0x7fffffce54, L0x7fffffce56, L0x7fffffce58, L0x7fffffce5a, L0x7fffffce5c, L0x7fffffce5e] [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157];
mov [L0x7fffffce60, L0x7fffffce62, L0x7fffffce64, L0x7fffffce66, L0x7fffffce68, L0x7fffffce6a, L0x7fffffce6c, L0x7fffffce6e] [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167];
mov [L0x7fffffce70, L0x7fffffce72, L0x7fffffce74, L0x7fffffce76, L0x7fffffce78, L0x7fffffce7a, L0x7fffffce7c, L0x7fffffce7e] [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177];
mov [L0x7fffffce80, L0x7fffffce82, L0x7fffffce84, L0x7fffffce86, L0x7fffffce88, L0x7fffffce8a, L0x7fffffce8c, L0x7fffffce8e] [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187];

nondet %v8@sint16[8];
nondet %v9@sint16[8];


# prologue

(* #! -> SP = 0x7fffffc320 *)
#! 0x7fffffc320 = 0x7fffffc320;

cut (* 0 *)
    [L0x7fffffc350, L0x7fffffc352, L0x7fffffc354, L0x7fffffc356, L0x7fffffc358, L0x7fffffc35a, L0x7fffffc35c, L0x7fffffc35e] = [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] /\
    [L0x7fffffc360, L0x7fffffc362, L0x7fffffc364, L0x7fffffc366, L0x7fffffc368, L0x7fffffc36a, L0x7fffffc36c, L0x7fffffc36e] = [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] /\
    [L0x7fffffc370, L0x7fffffc372, L0x7fffffc374, L0x7fffffc376, L0x7fffffc378, L0x7fffffc37a, L0x7fffffc37c, L0x7fffffc37e] = [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] /\
    [L0x7fffffc380, L0x7fffffc382, L0x7fffffc384, L0x7fffffc386, L0x7fffffc388, L0x7fffffc38a, L0x7fffffc38c, L0x7fffffc38e] = [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] /\
    [L0x7fffffc390, L0x7fffffc392, L0x7fffffc394, L0x7fffffc396, L0x7fffffc398, L0x7fffffc39a, L0x7fffffc39c, L0x7fffffc39e] = [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] /\
    [L0x7fffffc3a0, L0x7fffffc3a2, L0x7fffffc3a4, L0x7fffffc3a6, L0x7fffffc3a8, L0x7fffffc3aa, L0x7fffffc3ac, L0x7fffffc3ae] = [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] /\
    [L0x7fffffc3b0, L0x7fffffc3b2, L0x7fffffc3b4, L0x7fffffc3b6, L0x7fffffc3b8, L0x7fffffc3ba, L0x7fffffc3bc, L0x7fffffc3be] = [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] /\
    [L0x7fffffc3c0, L0x7fffffc3c2, L0x7fffffc3c4, L0x7fffffc3c6, L0x7fffffc3c8, L0x7fffffc3ca, L0x7fffffc3cc, L0x7fffffc3ce] = [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] /\
    [L0x7fffffc3d0, L0x7fffffc3d2, L0x7fffffc3d4, L0x7fffffc3d6, L0x7fffffc3d8, L0x7fffffc3da, L0x7fffffc3dc, L0x7fffffc3de] = [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] /\
    [L0x7fffffc3e0, L0x7fffffc3e2, L0x7fffffc3e4, L0x7fffffc3e6, L0x7fffffc3e8, L0x7fffffc3ea, L0x7fffffc3ec, L0x7fffffc3ee] = [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] /\
    [L0x7fffffc3f0, L0x7fffffc3f2, L0x7fffffc3f4, L0x7fffffc3f6, L0x7fffffc3f8, L0x7fffffc3fa, L0x7fffffc3fc, L0x7fffffc3fe] = [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] /\
    [L0x7fffffc400, L0x7fffffc402, L0x7fffffc404, L0x7fffffc406, L0x7fffffc408, L0x7fffffc40a, L0x7fffffc40c, L0x7fffffc40e] = [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] /\
    [L0x7fffffc410, L0x7fffffc412, L0x7fffffc414, L0x7fffffc416, L0x7fffffc418, L0x7fffffc41a, L0x7fffffc41c, L0x7fffffc41e] = [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] /\
    [L0x7fffffc420, L0x7fffffc422, L0x7fffffc424, L0x7fffffc426, L0x7fffffc428, L0x7fffffc42a, L0x7fffffc42c, L0x7fffffc42e] = [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] /\
    [L0x7fffffc430, L0x7fffffc432, L0x7fffffc434, L0x7fffffc436, L0x7fffffc438, L0x7fffffc43a, L0x7fffffc43c, L0x7fffffc43e] = [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] /\
    [L0x7fffffc440, L0x7fffffc442, L0x7fffffc444, L0x7fffffc446, L0x7fffffc448, L0x7fffffc44a, L0x7fffffc44c, L0x7fffffc44e] = [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] /\
    [L0x7fffffc450, L0x7fffffc452, L0x7fffffc454, L0x7fffffc456, L0x7fffffc458, L0x7fffffc45a, L0x7fffffc45c, L0x7fffffc45e] = [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] /\
    [L0x7fffffc460, L0x7fffffc462, L0x7fffffc464, L0x7fffffc466, L0x7fffffc468, L0x7fffffc46a, L0x7fffffc46c, L0x7fffffc46e] = [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] /\
    [L0x7fffffc470, L0x7fffffc472, L0x7fffffc474, L0x7fffffc476, L0x7fffffc478, L0x7fffffc47a, L0x7fffffc47c, L0x7fffffc47e] = [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] /\
    [L0x7fffffc480, L0x7fffffc482, L0x7fffffc484, L0x7fffffc486, L0x7fffffc488, L0x7fffffc48a, L0x7fffffc48c, L0x7fffffc48e] = [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] /\
    [L0x7fffffc490, L0x7fffffc492, L0x7fffffc494, L0x7fffffc496, L0x7fffffc498, L0x7fffffc49a, L0x7fffffc49c, L0x7fffffc49e] = [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] /\
    [L0x7fffffc4a0, L0x7fffffc4a2, L0x7fffffc4a4, L0x7fffffc4a6, L0x7fffffc4a8, L0x7fffffc4aa, L0x7fffffc4ac, L0x7fffffc4ae] = [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] /\
    [L0x7fffffc4b0, L0x7fffffc4b2, L0x7fffffc4b4, L0x7fffffc4b6, L0x7fffffc4b8, L0x7fffffc4ba, L0x7fffffc4bc, L0x7fffffc4be] = [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] /\
    [L0x7fffffc4c0, L0x7fffffc4c2, L0x7fffffc4c4, L0x7fffffc4c6, L0x7fffffc4c8, L0x7fffffc4ca, L0x7fffffc4cc, L0x7fffffc4ce] = [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] /\
    [L0x7fffffc4d0, L0x7fffffc4d2, L0x7fffffc4d4, L0x7fffffc4d6, L0x7fffffc4d8, L0x7fffffc4da, L0x7fffffc4dc, L0x7fffffc4de] = [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] /\
    [L0x7fffffc4e0, L0x7fffffc4e2, L0x7fffffc4e4, L0x7fffffc4e6, L0x7fffffc4e8, L0x7fffffc4ea, L0x7fffffc4ec, L0x7fffffc4ee] = [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] /\
    [L0x7fffffc4f0, L0x7fffffc4f2, L0x7fffffc4f4, L0x7fffffc4f6, L0x7fffffc4f8, L0x7fffffc4fa, L0x7fffffc4fc, L0x7fffffc4fe] = [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] /\
    [L0x7fffffc500, L0x7fffffc502, L0x7fffffc504, L0x7fffffc506, L0x7fffffc508, L0x7fffffc50a, L0x7fffffc50c, L0x7fffffc50e] = [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] /\
    [L0x7fffffc510, L0x7fffffc512, L0x7fffffc514, L0x7fffffc516, L0x7fffffc518, L0x7fffffc51a, L0x7fffffc51c, L0x7fffffc51e] = [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] /\
    [L0x7fffffc520, L0x7fffffc522, L0x7fffffc524, L0x7fffffc526, L0x7fffffc528, L0x7fffffc52a, L0x7fffffc52c, L0x7fffffc52e] = [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] /\
    [L0x7fffffc530, L0x7fffffc532, L0x7fffffc534, L0x7fffffc536, L0x7fffffc538, L0x7fffffc53a, L0x7fffffc53c, L0x7fffffc53e] = [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] /\
    [L0x7fffffc540, L0x7fffffc542, L0x7fffffc544, L0x7fffffc546, L0x7fffffc548, L0x7fffffc54a, L0x7fffffc54c, L0x7fffffc54e] = [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] /\
    [L0x7fffffc550, L0x7fffffc552, L0x7fffffc554, L0x7fffffc556, L0x7fffffc558, L0x7fffffc55a, L0x7fffffc55c, L0x7fffffc55e] = [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] /\
    [L0x7fffffc560, L0x7fffffc562, L0x7fffffc564, L0x7fffffc566, L0x7fffffc568, L0x7fffffc56a, L0x7fffffc56c, L0x7fffffc56e] = [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] /\
    [L0x7fffffc570, L0x7fffffc572, L0x7fffffc574, L0x7fffffc576, L0x7fffffc578, L0x7fffffc57a, L0x7fffffc57c, L0x7fffffc57e] = [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] /\
    [L0x7fffffc580, L0x7fffffc582, L0x7fffffc584, L0x7fffffc586, L0x7fffffc588, L0x7fffffc58a, L0x7fffffc58c, L0x7fffffc58e] = [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] /\
    [L0x7fffffc590, L0x7fffffc592, L0x7fffffc594, L0x7fffffc596, L0x7fffffc598, L0x7fffffc59a, L0x7fffffc59c, L0x7fffffc59e] = [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] /\
    [L0x7fffffc5a0, L0x7fffffc5a2, L0x7fffffc5a4, L0x7fffffc5a6, L0x7fffffc5a8, L0x7fffffc5aa, L0x7fffffc5ac, L0x7fffffc5ae] = [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] /\
    [L0x7fffffc5b0, L0x7fffffc5b2, L0x7fffffc5b4, L0x7fffffc5b6, L0x7fffffc5b8, L0x7fffffc5ba, L0x7fffffc5bc, L0x7fffffc5be] = [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] /\
    [L0x7fffffc5c0, L0x7fffffc5c2, L0x7fffffc5c4, L0x7fffffc5c6, L0x7fffffc5c8, L0x7fffffc5ca, L0x7fffffc5cc, L0x7fffffc5ce] = [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] /\
    [L0x7fffffc5d0, L0x7fffffc5d2, L0x7fffffc5d4, L0x7fffffc5d6, L0x7fffffc5d8, L0x7fffffc5da, L0x7fffffc5dc, L0x7fffffc5de] = [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] /\
    [L0x7fffffc5e0, L0x7fffffc5e2, L0x7fffffc5e4, L0x7fffffc5e6, L0x7fffffc5e8, L0x7fffffc5ea, L0x7fffffc5ec, L0x7fffffc5ee] = [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] /\
    [L0x7fffffc5f0, L0x7fffffc5f2, L0x7fffffc5f4, L0x7fffffc5f6, L0x7fffffc5f8, L0x7fffffc5fa, L0x7fffffc5fc, L0x7fffffc5fe] = [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] /\
    [L0x7fffffc600, L0x7fffffc602, L0x7fffffc604, L0x7fffffc606, L0x7fffffc608, L0x7fffffc60a, L0x7fffffc60c, L0x7fffffc60e] = [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] /\
    [L0x7fffffc610, L0x7fffffc612, L0x7fffffc614, L0x7fffffc616, L0x7fffffc618, L0x7fffffc61a, L0x7fffffc61c, L0x7fffffc61e] = [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] /\
    [L0x7fffffc620, L0x7fffffc622, L0x7fffffc624, L0x7fffffc626, L0x7fffffc628, L0x7fffffc62a, L0x7fffffc62c, L0x7fffffc62e] = [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] /\
    [L0x7fffffc630, L0x7fffffc632, L0x7fffffc634, L0x7fffffc636, L0x7fffffc638, L0x7fffffc63a, L0x7fffffc63c, L0x7fffffc63e] = [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] /\
    [L0x7fffffc640, L0x7fffffc642, L0x7fffffc644, L0x7fffffc646, L0x7fffffc648, L0x7fffffc64a, L0x7fffffc64c, L0x7fffffc64e] = [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] /\
    [L0x7fffffc650, L0x7fffffc652, L0x7fffffc654, L0x7fffffc656, L0x7fffffc658, L0x7fffffc65a, L0x7fffffc65c, L0x7fffffc65e] = [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] /\
    [L0x7fffffc660, L0x7fffffc662, L0x7fffffc664, L0x7fffffc666, L0x7fffffc668, L0x7fffffc66a, L0x7fffffc66c, L0x7fffffc66e] = [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] /\
    [L0x7fffffc670, L0x7fffffc672, L0x7fffffc674, L0x7fffffc676, L0x7fffffc678, L0x7fffffc67a, L0x7fffffc67c, L0x7fffffc67e] = [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] /\
    [L0x7fffffc680, L0x7fffffc682, L0x7fffffc684, L0x7fffffc686, L0x7fffffc688, L0x7fffffc68a, L0x7fffffc68c, L0x7fffffc68e] = [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] /\
    [L0x7fffffc690, L0x7fffffc692, L0x7fffffc694, L0x7fffffc696, L0x7fffffc698, L0x7fffffc69a, L0x7fffffc69c, L0x7fffffc69e] = [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] /\
    [L0x7fffffc6a0, L0x7fffffc6a2, L0x7fffffc6a4, L0x7fffffc6a6, L0x7fffffc6a8, L0x7fffffc6aa, L0x7fffffc6ac, L0x7fffffc6ae] = [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] /\
    [L0x7fffffc6b0, L0x7fffffc6b2, L0x7fffffc6b4, L0x7fffffc6b6, L0x7fffffc6b8, L0x7fffffc6ba, L0x7fffffc6bc, L0x7fffffc6be] = [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] /\
    [L0x7fffffc6c0, L0x7fffffc6c2, L0x7fffffc6c4, L0x7fffffc6c6, L0x7fffffc6c8, L0x7fffffc6ca, L0x7fffffc6cc, L0x7fffffc6ce] = [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] /\
    [L0x7fffffc6d0, L0x7fffffc6d2, L0x7fffffc6d4, L0x7fffffc6d6, L0x7fffffc6d8, L0x7fffffc6da, L0x7fffffc6dc, L0x7fffffc6de] = [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] /\
    [L0x7fffffc6e0, L0x7fffffc6e2, L0x7fffffc6e4, L0x7fffffc6e6, L0x7fffffc6e8, L0x7fffffc6ea, L0x7fffffc6ec, L0x7fffffc6ee] = [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] /\
    [L0x7fffffc6f0, L0x7fffffc6f2, L0x7fffffc6f4, L0x7fffffc6f6, L0x7fffffc6f8, L0x7fffffc6fa, L0x7fffffc6fc, L0x7fffffc6fe] = [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] /\
    [L0x7fffffc700, L0x7fffffc702, L0x7fffffc704, L0x7fffffc706, L0x7fffffc708, L0x7fffffc70a, L0x7fffffc70c, L0x7fffffc70e] = [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] /\
    [L0x7fffffc710, L0x7fffffc712, L0x7fffffc714, L0x7fffffc716, L0x7fffffc718, L0x7fffffc71a, L0x7fffffc71c, L0x7fffffc71e] = [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] /\
    [L0x7fffffc720, L0x7fffffc722, L0x7fffffc724, L0x7fffffc726, L0x7fffffc728, L0x7fffffc72a, L0x7fffffc72c, L0x7fffffc72e] = [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] /\
    [L0x7fffffc730, L0x7fffffc732, L0x7fffffc734, L0x7fffffc736, L0x7fffffc738, L0x7fffffc73a, L0x7fffffc73c, L0x7fffffc73e] = [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] /\
    [L0x7fffffc740, L0x7fffffc742, L0x7fffffc744, L0x7fffffc746, L0x7fffffc748, L0x7fffffc74a, L0x7fffffc74c, L0x7fffffc74e] = [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] /\
    [L0x7fffffc750, L0x7fffffc752, L0x7fffffc754, L0x7fffffc756, L0x7fffffc758, L0x7fffffc75a, L0x7fffffc75c, L0x7fffffc75e] = [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] /\
    [L0x7fffffc760, L0x7fffffc762, L0x7fffffc764, L0x7fffffc766, L0x7fffffc768, L0x7fffffc76a, L0x7fffffc76c, L0x7fffffc76e] = [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] /\
    [L0x7fffffc770, L0x7fffffc772, L0x7fffffc774, L0x7fffffc776, L0x7fffffc778, L0x7fffffc77a, L0x7fffffc77c, L0x7fffffc77e] = [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] /\
    [L0x7fffffc780, L0x7fffffc782, L0x7fffffc784, L0x7fffffc786, L0x7fffffc788, L0x7fffffc78a, L0x7fffffc78c, L0x7fffffc78e] = [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] /\
    [L0x7fffffc790, L0x7fffffc792, L0x7fffffc794, L0x7fffffc796, L0x7fffffc798, L0x7fffffc79a, L0x7fffffc79c, L0x7fffffc79e] = [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] /\
    [L0x7fffffc7a0, L0x7fffffc7a2, L0x7fffffc7a4, L0x7fffffc7a6, L0x7fffffc7a8, L0x7fffffc7aa, L0x7fffffc7ac, L0x7fffffc7ae] = [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] /\
    [L0x7fffffc7b0, L0x7fffffc7b2, L0x7fffffc7b4, L0x7fffffc7b6, L0x7fffffc7b8, L0x7fffffc7ba, L0x7fffffc7bc, L0x7fffffc7be] = [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] /\
    [L0x7fffffc7c0, L0x7fffffc7c2, L0x7fffffc7c4, L0x7fffffc7c6, L0x7fffffc7c8, L0x7fffffc7ca, L0x7fffffc7cc, L0x7fffffc7ce] = [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] /\
    [L0x7fffffc7d0, L0x7fffffc7d2, L0x7fffffc7d4, L0x7fffffc7d6, L0x7fffffc7d8, L0x7fffffc7da, L0x7fffffc7dc, L0x7fffffc7de] = [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] /\
    [L0x7fffffc7e0, L0x7fffffc7e2, L0x7fffffc7e4, L0x7fffffc7e6, L0x7fffffc7e8, L0x7fffffc7ea, L0x7fffffc7ec, L0x7fffffc7ee] = [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] /\
    [L0x7fffffc7f0, L0x7fffffc7f2, L0x7fffffc7f4, L0x7fffffc7f6, L0x7fffffc7f8, L0x7fffffc7fa, L0x7fffffc7fc, L0x7fffffc7fe] = [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] /\
    [L0x7fffffc800, L0x7fffffc802, L0x7fffffc804, L0x7fffffc806, L0x7fffffc808, L0x7fffffc80a, L0x7fffffc80c, L0x7fffffc80e] = [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] /\
    [L0x7fffffc810, L0x7fffffc812, L0x7fffffc814, L0x7fffffc816, L0x7fffffc818, L0x7fffffc81a, L0x7fffffc81c, L0x7fffffc81e] = [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] /\
    [L0x7fffffc820, L0x7fffffc822, L0x7fffffc824, L0x7fffffc826, L0x7fffffc828, L0x7fffffc82a, L0x7fffffc82c, L0x7fffffc82e] = [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] /\
    [L0x7fffffc830, L0x7fffffc832, L0x7fffffc834, L0x7fffffc836, L0x7fffffc838, L0x7fffffc83a, L0x7fffffc83c, L0x7fffffc83e] = [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] /\
    [L0x7fffffc840, L0x7fffffc842, L0x7fffffc844, L0x7fffffc846, L0x7fffffc848, L0x7fffffc84a, L0x7fffffc84c, L0x7fffffc84e] = [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] /\
    [L0x7fffffc850, L0x7fffffc852, L0x7fffffc854, L0x7fffffc856, L0x7fffffc858, L0x7fffffc85a, L0x7fffffc85c, L0x7fffffc85e] = [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] /\
    [L0x7fffffc860, L0x7fffffc862, L0x7fffffc864, L0x7fffffc866, L0x7fffffc868, L0x7fffffc86a, L0x7fffffc86c, L0x7fffffc86e] = [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] /\
    [L0x7fffffc870, L0x7fffffc872, L0x7fffffc874, L0x7fffffc876, L0x7fffffc878, L0x7fffffc87a, L0x7fffffc87c, L0x7fffffc87e] = [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] /\
    [L0x7fffffc880, L0x7fffffc882, L0x7fffffc884, L0x7fffffc886, L0x7fffffc888, L0x7fffffc88a, L0x7fffffc88c, L0x7fffffc88e] = [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] /\
    [L0x7fffffc890, L0x7fffffc892, L0x7fffffc894, L0x7fffffc896, L0x7fffffc898, L0x7fffffc89a, L0x7fffffc89c, L0x7fffffc89e] = [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] /\
    [L0x7fffffc8a0, L0x7fffffc8a2, L0x7fffffc8a4, L0x7fffffc8a6, L0x7fffffc8a8, L0x7fffffc8aa, L0x7fffffc8ac, L0x7fffffc8ae] = [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] /\
    [L0x7fffffc8b0, L0x7fffffc8b2, L0x7fffffc8b4, L0x7fffffc8b6, L0x7fffffc8b8, L0x7fffffc8ba, L0x7fffffc8bc, L0x7fffffc8be] = [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] /\
    [L0x7fffffc8c0, L0x7fffffc8c2, L0x7fffffc8c4, L0x7fffffc8c6, L0x7fffffc8c8, L0x7fffffc8ca, L0x7fffffc8cc, L0x7fffffc8ce] = [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] /\
    [L0x7fffffc8d0, L0x7fffffc8d2, L0x7fffffc8d4, L0x7fffffc8d6, L0x7fffffc8d8, L0x7fffffc8da, L0x7fffffc8dc, L0x7fffffc8de] = [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] /\
    [L0x7fffffc8e0, L0x7fffffc8e2, L0x7fffffc8e4, L0x7fffffc8e6, L0x7fffffc8e8, L0x7fffffc8ea, L0x7fffffc8ec, L0x7fffffc8ee] = [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] /\
    [L0x7fffffc8f0, L0x7fffffc8f2, L0x7fffffc8f4, L0x7fffffc8f6, L0x7fffffc8f8, L0x7fffffc8fa, L0x7fffffc8fc, L0x7fffffc8fe] = [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] /\
    [L0x7fffffc900, L0x7fffffc902, L0x7fffffc904, L0x7fffffc906, L0x7fffffc908, L0x7fffffc90a, L0x7fffffc90c, L0x7fffffc90e] = [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] /\
    [L0x7fffffc910, L0x7fffffc912, L0x7fffffc914, L0x7fffffc916, L0x7fffffc918, L0x7fffffc91a, L0x7fffffc91c, L0x7fffffc91e] = [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] /\
    [L0x7fffffc920, L0x7fffffc922, L0x7fffffc924, L0x7fffffc926, L0x7fffffc928, L0x7fffffc92a, L0x7fffffc92c, L0x7fffffc92e] = [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] /\
    [L0x7fffffc930, L0x7fffffc932, L0x7fffffc934, L0x7fffffc936, L0x7fffffc938, L0x7fffffc93a, L0x7fffffc93c, L0x7fffffc93e] = [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] /\
    [L0x7fffffc940, L0x7fffffc942, L0x7fffffc944, L0x7fffffc946, L0x7fffffc948, L0x7fffffc94a, L0x7fffffc94c, L0x7fffffc94e] = [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] /\
    [L0x7fffffc950, L0x7fffffc952, L0x7fffffc954, L0x7fffffc956, L0x7fffffc958, L0x7fffffc95a, L0x7fffffc95c, L0x7fffffc95e] = [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] /\
    [L0x7fffffc960, L0x7fffffc962, L0x7fffffc964, L0x7fffffc966, L0x7fffffc968, L0x7fffffc96a, L0x7fffffc96c, L0x7fffffc96e] = [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] /\
    [L0x7fffffc970, L0x7fffffc972, L0x7fffffc974, L0x7fffffc976, L0x7fffffc978, L0x7fffffc97a, L0x7fffffc97c, L0x7fffffc97e] = [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] /\
    [L0x7fffffc980, L0x7fffffc982, L0x7fffffc984, L0x7fffffc986, L0x7fffffc988, L0x7fffffc98a, L0x7fffffc98c, L0x7fffffc98e] = [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] /\
    [L0x7fffffc990, L0x7fffffc992, L0x7fffffc994, L0x7fffffc996, L0x7fffffc998, L0x7fffffc99a, L0x7fffffc99c, L0x7fffffc99e] = [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] /\
    [L0x7fffffc9a0, L0x7fffffc9a2, L0x7fffffc9a4, L0x7fffffc9a6, L0x7fffffc9a8, L0x7fffffc9aa, L0x7fffffc9ac, L0x7fffffc9ae] = [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] /\
    [L0x7fffffc9b0, L0x7fffffc9b2, L0x7fffffc9b4, L0x7fffffc9b6, L0x7fffffc9b8, L0x7fffffc9ba, L0x7fffffc9bc, L0x7fffffc9be] = [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] /\
    [L0x7fffffc9c0, L0x7fffffc9c2, L0x7fffffc9c4, L0x7fffffc9c6, L0x7fffffc9c8, L0x7fffffc9ca, L0x7fffffc9cc, L0x7fffffc9ce] = [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] /\
    [L0x7fffffc9d0, L0x7fffffc9d2, L0x7fffffc9d4, L0x7fffffc9d6, L0x7fffffc9d8, L0x7fffffc9da, L0x7fffffc9dc, L0x7fffffc9de] = [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] /\
    [L0x7fffffc9e0, L0x7fffffc9e2, L0x7fffffc9e4, L0x7fffffc9e6, L0x7fffffc9e8, L0x7fffffc9ea, L0x7fffffc9ec, L0x7fffffc9ee] = [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] /\
    [L0x7fffffc9f0, L0x7fffffc9f2, L0x7fffffc9f4, L0x7fffffc9f6, L0x7fffffc9f8, L0x7fffffc9fa, L0x7fffffc9fc, L0x7fffffc9fe] = [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] /\
    [L0x7fffffca00, L0x7fffffca02, L0x7fffffca04, L0x7fffffca06, L0x7fffffca08, L0x7fffffca0a, L0x7fffffca0c, L0x7fffffca0e] = [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] /\
    [L0x7fffffca10, L0x7fffffca12, L0x7fffffca14, L0x7fffffca16, L0x7fffffca18, L0x7fffffca1a, L0x7fffffca1c, L0x7fffffca1e] = [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] /\
    [L0x7fffffca20, L0x7fffffca22, L0x7fffffca24, L0x7fffffca26, L0x7fffffca28, L0x7fffffca2a, L0x7fffffca2c, L0x7fffffca2e] = [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] /\
    [L0x7fffffca30, L0x7fffffca32, L0x7fffffca34, L0x7fffffca36, L0x7fffffca38, L0x7fffffca3a, L0x7fffffca3c, L0x7fffffca3e] = [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] /\
    [L0x7fffffca40, L0x7fffffca42, L0x7fffffca44, L0x7fffffca46, L0x7fffffca48, L0x7fffffca4a, L0x7fffffca4c, L0x7fffffca4e] = [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] /\
    [L0x7fffffca50, L0x7fffffca52, L0x7fffffca54, L0x7fffffca56, L0x7fffffca58, L0x7fffffca5a, L0x7fffffca5c, L0x7fffffca5e] = [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] /\
    [L0x7fffffca60, L0x7fffffca62, L0x7fffffca64, L0x7fffffca66, L0x7fffffca68, L0x7fffffca6a, L0x7fffffca6c, L0x7fffffca6e] = [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] /\
    [L0x7fffffca70, L0x7fffffca72, L0x7fffffca74, L0x7fffffca76, L0x7fffffca78, L0x7fffffca7a, L0x7fffffca7c, L0x7fffffca7e] = [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] /\
    [L0x7fffffca80, L0x7fffffca82, L0x7fffffca84, L0x7fffffca86, L0x7fffffca88, L0x7fffffca8a, L0x7fffffca8c, L0x7fffffca8e] = [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] /\
    [L0x7fffffca90, L0x7fffffca92, L0x7fffffca94, L0x7fffffca96, L0x7fffffca98, L0x7fffffca9a, L0x7fffffca9c, L0x7fffffca9e] = [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] /\
    [L0x7fffffcaa0, L0x7fffffcaa2, L0x7fffffcaa4, L0x7fffffcaa6, L0x7fffffcaa8, L0x7fffffcaaa, L0x7fffffcaac, L0x7fffffcaae] = [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] /\
    [L0x7fffffcab0, L0x7fffffcab2, L0x7fffffcab4, L0x7fffffcab6, L0x7fffffcab8, L0x7fffffcaba, L0x7fffffcabc, L0x7fffffcabe] = [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] /\
    [L0x7fffffcac0, L0x7fffffcac2, L0x7fffffcac4, L0x7fffffcac6, L0x7fffffcac8, L0x7fffffcaca, L0x7fffffcacc, L0x7fffffcace] = [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] /\
    [L0x7fffffcad0, L0x7fffffcad2, L0x7fffffcad4, L0x7fffffcad6, L0x7fffffcad8, L0x7fffffcada, L0x7fffffcadc, L0x7fffffcade] = [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] /\
    [L0x7fffffcae0, L0x7fffffcae2, L0x7fffffcae4, L0x7fffffcae6, L0x7fffffcae8, L0x7fffffcaea, L0x7fffffcaec, L0x7fffffcaee] = [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] /\
    [L0x7fffffcaf0, L0x7fffffcaf2, L0x7fffffcaf4, L0x7fffffcaf6, L0x7fffffcaf8, L0x7fffffcafa, L0x7fffffcafc, L0x7fffffcafe] = [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] /\
    [L0x7fffffcb00, L0x7fffffcb02, L0x7fffffcb04, L0x7fffffcb06, L0x7fffffcb08, L0x7fffffcb0a, L0x7fffffcb0c, L0x7fffffcb0e] = [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] /\
    [L0x7fffffcb10, L0x7fffffcb12, L0x7fffffcb14, L0x7fffffcb16, L0x7fffffcb18, L0x7fffffcb1a, L0x7fffffcb1c, L0x7fffffcb1e] = [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] /\
    [L0x7fffffcb20, L0x7fffffcb22, L0x7fffffcb24, L0x7fffffcb26, L0x7fffffcb28, L0x7fffffcb2a, L0x7fffffcb2c, L0x7fffffcb2e] = [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] /\
    [L0x7fffffcb30, L0x7fffffcb32, L0x7fffffcb34, L0x7fffffcb36, L0x7fffffcb38, L0x7fffffcb3a, L0x7fffffcb3c, L0x7fffffcb3e] = [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] /\
    [L0x7fffffcb40, L0x7fffffcb42, L0x7fffffcb44, L0x7fffffcb46, L0x7fffffcb48, L0x7fffffcb4a, L0x7fffffcb4c, L0x7fffffcb4e] = [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] /\
    [L0x7fffffcb50, L0x7fffffcb52, L0x7fffffcb54, L0x7fffffcb56, L0x7fffffcb58, L0x7fffffcb5a, L0x7fffffcb5c, L0x7fffffcb5e] = [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] /\
    [L0x7fffffcb60, L0x7fffffcb62, L0x7fffffcb64, L0x7fffffcb66, L0x7fffffcb68, L0x7fffffcb6a, L0x7fffffcb6c, L0x7fffffcb6e] = [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] /\
    [L0x7fffffcb70, L0x7fffffcb72, L0x7fffffcb74, L0x7fffffcb76, L0x7fffffcb78, L0x7fffffcb7a, L0x7fffffcb7c, L0x7fffffcb7e] = [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] /\
    [L0x7fffffcb80, L0x7fffffcb82, L0x7fffffcb84, L0x7fffffcb86, L0x7fffffcb88, L0x7fffffcb8a, L0x7fffffcb8c, L0x7fffffcb8e] = [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] /\
    [L0x7fffffcb90, L0x7fffffcb92, L0x7fffffcb94, L0x7fffffcb96, L0x7fffffcb98, L0x7fffffcb9a, L0x7fffffcb9c, L0x7fffffcb9e] = [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] /\
    [L0x7fffffcba0, L0x7fffffcba2, L0x7fffffcba4, L0x7fffffcba6, L0x7fffffcba8, L0x7fffffcbaa, L0x7fffffcbac, L0x7fffffcbae] = [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] /\
    [L0x7fffffcbb0, L0x7fffffcbb2, L0x7fffffcbb4, L0x7fffffcbb6, L0x7fffffcbb8, L0x7fffffcbba, L0x7fffffcbbc, L0x7fffffcbbe] = [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] /\
    [L0x7fffffcbc0, L0x7fffffcbc2, L0x7fffffcbc4, L0x7fffffcbc6, L0x7fffffcbc8, L0x7fffffcbca, L0x7fffffcbcc, L0x7fffffcbce] = [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] /\
    [L0x7fffffcbd0, L0x7fffffcbd2, L0x7fffffcbd4, L0x7fffffcbd6, L0x7fffffcbd8, L0x7fffffcbda, L0x7fffffcbdc, L0x7fffffcbde] = [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] /\
    [L0x7fffffcbe0, L0x7fffffcbe2, L0x7fffffcbe4, L0x7fffffcbe6, L0x7fffffcbe8, L0x7fffffcbea, L0x7fffffcbec, L0x7fffffcbee] = [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] /\
    [L0x7fffffcbf0, L0x7fffffcbf2, L0x7fffffcbf4, L0x7fffffcbf6, L0x7fffffcbf8, L0x7fffffcbfa, L0x7fffffcbfc, L0x7fffffcbfe] = [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] /\
    [L0x7fffffcc00, L0x7fffffcc02, L0x7fffffcc04, L0x7fffffcc06, L0x7fffffcc08, L0x7fffffcc0a, L0x7fffffcc0c, L0x7fffffcc0e] = [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] /\
    [L0x7fffffcc10, L0x7fffffcc12, L0x7fffffcc14, L0x7fffffcc16, L0x7fffffcc18, L0x7fffffcc1a, L0x7fffffcc1c, L0x7fffffcc1e] = [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] /\
    [L0x7fffffcc20, L0x7fffffcc22, L0x7fffffcc24, L0x7fffffcc26, L0x7fffffcc28, L0x7fffffcc2a, L0x7fffffcc2c, L0x7fffffcc2e] = [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] /\
    [L0x7fffffcc30, L0x7fffffcc32, L0x7fffffcc34, L0x7fffffcc36, L0x7fffffcc38, L0x7fffffcc3a, L0x7fffffcc3c, L0x7fffffcc3e] = [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] /\
    [L0x7fffffcc40, L0x7fffffcc42, L0x7fffffcc44, L0x7fffffcc46, L0x7fffffcc48, L0x7fffffcc4a, L0x7fffffcc4c, L0x7fffffcc4e] = [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] /\
    [L0x7fffffcc50, L0x7fffffcc52, L0x7fffffcc54, L0x7fffffcc56, L0x7fffffcc58, L0x7fffffcc5a, L0x7fffffcc5c, L0x7fffffcc5e] = [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] /\
    [L0x7fffffcc60, L0x7fffffcc62, L0x7fffffcc64, L0x7fffffcc66, L0x7fffffcc68, L0x7fffffcc6a, L0x7fffffcc6c, L0x7fffffcc6e] = [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] /\
    [L0x7fffffcc70, L0x7fffffcc72, L0x7fffffcc74, L0x7fffffcc76, L0x7fffffcc78, L0x7fffffcc7a, L0x7fffffcc7c, L0x7fffffcc7e] = [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] /\
    [L0x7fffffcc80, L0x7fffffcc82, L0x7fffffcc84, L0x7fffffcc86, L0x7fffffcc88, L0x7fffffcc8a, L0x7fffffcc8c, L0x7fffffcc8e] = [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] /\
    [L0x7fffffcc90, L0x7fffffcc92, L0x7fffffcc94, L0x7fffffcc96, L0x7fffffcc98, L0x7fffffcc9a, L0x7fffffcc9c, L0x7fffffcc9e] = [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] /\
    [L0x7fffffcca0, L0x7fffffcca2, L0x7fffffcca4, L0x7fffffcca6, L0x7fffffcca8, L0x7fffffccaa, L0x7fffffccac, L0x7fffffccae] = [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] /\
    [L0x7fffffccb0, L0x7fffffccb2, L0x7fffffccb4, L0x7fffffccb6, L0x7fffffccb8, L0x7fffffccba, L0x7fffffccbc, L0x7fffffccbe] = [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] /\
    [L0x7fffffccc0, L0x7fffffccc2, L0x7fffffccc4, L0x7fffffccc6, L0x7fffffccc8, L0x7fffffccca, L0x7fffffcccc, L0x7fffffccce] = [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] /\
    [L0x7fffffccd0, L0x7fffffccd2, L0x7fffffccd4, L0x7fffffccd6, L0x7fffffccd8, L0x7fffffccda, L0x7fffffccdc, L0x7fffffccde] = [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] /\
    [L0x7fffffcce0, L0x7fffffcce2, L0x7fffffcce4, L0x7fffffcce6, L0x7fffffcce8, L0x7fffffccea, L0x7fffffccec, L0x7fffffccee] = [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] /\
    [L0x7fffffccf0, L0x7fffffccf2, L0x7fffffccf4, L0x7fffffccf6, L0x7fffffccf8, L0x7fffffccfa, L0x7fffffccfc, L0x7fffffccfe] = [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] /\
    [L0x7fffffcd00, L0x7fffffcd02, L0x7fffffcd04, L0x7fffffcd06, L0x7fffffcd08, L0x7fffffcd0a, L0x7fffffcd0c, L0x7fffffcd0e] = [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] /\
    [L0x7fffffcd10, L0x7fffffcd12, L0x7fffffcd14, L0x7fffffcd16, L0x7fffffcd18, L0x7fffffcd1a, L0x7fffffcd1c, L0x7fffffcd1e] = [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] /\
    [L0x7fffffcd20, L0x7fffffcd22, L0x7fffffcd24, L0x7fffffcd26, L0x7fffffcd28, L0x7fffffcd2a, L0x7fffffcd2c, L0x7fffffcd2e] = [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] /\
    [L0x7fffffcd30, L0x7fffffcd32, L0x7fffffcd34, L0x7fffffcd36, L0x7fffffcd38, L0x7fffffcd3a, L0x7fffffcd3c, L0x7fffffcd3e] = [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] /\
    [L0x7fffffcd40, L0x7fffffcd42, L0x7fffffcd44, L0x7fffffcd46, L0x7fffffcd48, L0x7fffffcd4a, L0x7fffffcd4c, L0x7fffffcd4e] = [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] /\
    [L0x7fffffcd50, L0x7fffffcd52, L0x7fffffcd54, L0x7fffffcd56, L0x7fffffcd58, L0x7fffffcd5a, L0x7fffffcd5c, L0x7fffffcd5e] = [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] /\
    [L0x7fffffcd60, L0x7fffffcd62, L0x7fffffcd64, L0x7fffffcd66, L0x7fffffcd68, L0x7fffffcd6a, L0x7fffffcd6c, L0x7fffffcd6e] = [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] /\
    [L0x7fffffcd70, L0x7fffffcd72, L0x7fffffcd74, L0x7fffffcd76, L0x7fffffcd78, L0x7fffffcd7a, L0x7fffffcd7c, L0x7fffffcd7e] = [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] /\
    [L0x7fffffcd80, L0x7fffffcd82, L0x7fffffcd84, L0x7fffffcd86, L0x7fffffcd88, L0x7fffffcd8a, L0x7fffffcd8c, L0x7fffffcd8e] = [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] /\
    [L0x7fffffcd90, L0x7fffffcd92, L0x7fffffcd94, L0x7fffffcd96, L0x7fffffcd98, L0x7fffffcd9a, L0x7fffffcd9c, L0x7fffffcd9e] = [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] /\
    [L0x7fffffcda0, L0x7fffffcda2, L0x7fffffcda4, L0x7fffffcda6, L0x7fffffcda8, L0x7fffffcdaa, L0x7fffffcdac, L0x7fffffcdae] = [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] /\
    [L0x7fffffcdb0, L0x7fffffcdb2, L0x7fffffcdb4, L0x7fffffcdb6, L0x7fffffcdb8, L0x7fffffcdba, L0x7fffffcdbc, L0x7fffffcdbe] = [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] /\
    [L0x7fffffcdc0, L0x7fffffcdc2, L0x7fffffcdc4, L0x7fffffcdc6, L0x7fffffcdc8, L0x7fffffcdca, L0x7fffffcdcc, L0x7fffffcdce] = [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] /\
    [L0x7fffffcdd0, L0x7fffffcdd2, L0x7fffffcdd4, L0x7fffffcdd6, L0x7fffffcdd8, L0x7fffffcdda, L0x7fffffcddc, L0x7fffffcdde] = [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] /\
    [L0x7fffffcde0, L0x7fffffcde2, L0x7fffffcde4, L0x7fffffcde6, L0x7fffffcde8, L0x7fffffcdea, L0x7fffffcdec, L0x7fffffcdee] = [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] /\
    [L0x7fffffcdf0, L0x7fffffcdf2, L0x7fffffcdf4, L0x7fffffcdf6, L0x7fffffcdf8, L0x7fffffcdfa, L0x7fffffcdfc, L0x7fffffcdfe] = [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] /\
    [L0x7fffffce00, L0x7fffffce02, L0x7fffffce04, L0x7fffffce06, L0x7fffffce08, L0x7fffffce0a, L0x7fffffce0c, L0x7fffffce0e] = [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] /\
    [L0x7fffffce10, L0x7fffffce12, L0x7fffffce14, L0x7fffffce16, L0x7fffffce18, L0x7fffffce1a, L0x7fffffce1c, L0x7fffffce1e] = [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] /\
    [L0x7fffffce20, L0x7fffffce22, L0x7fffffce24, L0x7fffffce26, L0x7fffffce28, L0x7fffffce2a, L0x7fffffce2c, L0x7fffffce2e] = [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] /\
    [L0x7fffffce30, L0x7fffffce32, L0x7fffffce34, L0x7fffffce36, L0x7fffffce38, L0x7fffffce3a, L0x7fffffce3c, L0x7fffffce3e] = [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] /\
    [L0x7fffffce40, L0x7fffffce42, L0x7fffffce44, L0x7fffffce46, L0x7fffffce48, L0x7fffffce4a, L0x7fffffce4c, L0x7fffffce4e] = [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] /\
    [L0x7fffffce50, L0x7fffffce52, L0x7fffffce54, L0x7fffffce56, L0x7fffffce58, L0x7fffffce5a, L0x7fffffce5c, L0x7fffffce5e] = [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] /\
    [L0x7fffffce60, L0x7fffffce62, L0x7fffffce64, L0x7fffffce66, L0x7fffffce68, L0x7fffffce6a, L0x7fffffce6c, L0x7fffffce6e] = [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] /\
    [L0x7fffffce70, L0x7fffffce72, L0x7fffffce74, L0x7fffffce76, L0x7fffffce78, L0x7fffffce7a, L0x7fffffce7c, L0x7fffffce7e] = [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] /\
    [L0x7fffffce80, L0x7fffffce82, L0x7fffffce84, L0x7fffffce86, L0x7fffffce88, L0x7fffffce8a, L0x7fffffce8c, L0x7fffffce8e] = [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] /\

    [L0x5555552810, L0x5555552812, L0x5555552814, L0x5555552816, L0x5555552818, L0x555555281a, L0x555555281c, L0x555555281e, L0x5555552820, L0x5555552822, L0x5555552824, L0x5555552826, L0x5555552828, L0x555555282a, L0x555555282c, L0x555555282e] = [4591, 29235, 0, 0, 0, 0, 0, 0, -621, 1891, -803, 0, -4432, 13497, -5731, -17729]
  &&
    [L0x7fffffc350, L0x7fffffc352, L0x7fffffc354, L0x7fffffc356, L0x7fffffc358, L0x7fffffc35a, L0x7fffffc35c, L0x7fffffc35e] = [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] /\
    [L0x7fffffc360, L0x7fffffc362, L0x7fffffc364, L0x7fffffc366, L0x7fffffc368, L0x7fffffc36a, L0x7fffffc36c, L0x7fffffc36e] = [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] /\
    [L0x7fffffc370, L0x7fffffc372, L0x7fffffc374, L0x7fffffc376, L0x7fffffc378, L0x7fffffc37a, L0x7fffffc37c, L0x7fffffc37e] = [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] /\
    [L0x7fffffc380, L0x7fffffc382, L0x7fffffc384, L0x7fffffc386, L0x7fffffc388, L0x7fffffc38a, L0x7fffffc38c, L0x7fffffc38e] = [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] /\
    [L0x7fffffc390, L0x7fffffc392, L0x7fffffc394, L0x7fffffc396, L0x7fffffc398, L0x7fffffc39a, L0x7fffffc39c, L0x7fffffc39e] = [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] /\
    [L0x7fffffc3a0, L0x7fffffc3a2, L0x7fffffc3a4, L0x7fffffc3a6, L0x7fffffc3a8, L0x7fffffc3aa, L0x7fffffc3ac, L0x7fffffc3ae] = [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] /\
    [L0x7fffffc3b0, L0x7fffffc3b2, L0x7fffffc3b4, L0x7fffffc3b6, L0x7fffffc3b8, L0x7fffffc3ba, L0x7fffffc3bc, L0x7fffffc3be] = [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] /\
    [L0x7fffffc3c0, L0x7fffffc3c2, L0x7fffffc3c4, L0x7fffffc3c6, L0x7fffffc3c8, L0x7fffffc3ca, L0x7fffffc3cc, L0x7fffffc3ce] = [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] /\
    [L0x7fffffc3d0, L0x7fffffc3d2, L0x7fffffc3d4, L0x7fffffc3d6, L0x7fffffc3d8, L0x7fffffc3da, L0x7fffffc3dc, L0x7fffffc3de] = [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] /\
    [L0x7fffffc3e0, L0x7fffffc3e2, L0x7fffffc3e4, L0x7fffffc3e6, L0x7fffffc3e8, L0x7fffffc3ea, L0x7fffffc3ec, L0x7fffffc3ee] = [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] /\
    [L0x7fffffc3f0, L0x7fffffc3f2, L0x7fffffc3f4, L0x7fffffc3f6, L0x7fffffc3f8, L0x7fffffc3fa, L0x7fffffc3fc, L0x7fffffc3fe] = [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] /\
    [L0x7fffffc400, L0x7fffffc402, L0x7fffffc404, L0x7fffffc406, L0x7fffffc408, L0x7fffffc40a, L0x7fffffc40c, L0x7fffffc40e] = [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] /\
    [L0x7fffffc410, L0x7fffffc412, L0x7fffffc414, L0x7fffffc416, L0x7fffffc418, L0x7fffffc41a, L0x7fffffc41c, L0x7fffffc41e] = [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] /\
    [L0x7fffffc420, L0x7fffffc422, L0x7fffffc424, L0x7fffffc426, L0x7fffffc428, L0x7fffffc42a, L0x7fffffc42c, L0x7fffffc42e] = [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] /\
    [L0x7fffffc430, L0x7fffffc432, L0x7fffffc434, L0x7fffffc436, L0x7fffffc438, L0x7fffffc43a, L0x7fffffc43c, L0x7fffffc43e] = [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] /\
    [L0x7fffffc440, L0x7fffffc442, L0x7fffffc444, L0x7fffffc446, L0x7fffffc448, L0x7fffffc44a, L0x7fffffc44c, L0x7fffffc44e] = [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] /\
    [L0x7fffffc450, L0x7fffffc452, L0x7fffffc454, L0x7fffffc456, L0x7fffffc458, L0x7fffffc45a, L0x7fffffc45c, L0x7fffffc45e] = [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] /\
    [L0x7fffffc460, L0x7fffffc462, L0x7fffffc464, L0x7fffffc466, L0x7fffffc468, L0x7fffffc46a, L0x7fffffc46c, L0x7fffffc46e] = [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] /\
    [L0x7fffffc470, L0x7fffffc472, L0x7fffffc474, L0x7fffffc476, L0x7fffffc478, L0x7fffffc47a, L0x7fffffc47c, L0x7fffffc47e] = [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] /\
    [L0x7fffffc480, L0x7fffffc482, L0x7fffffc484, L0x7fffffc486, L0x7fffffc488, L0x7fffffc48a, L0x7fffffc48c, L0x7fffffc48e] = [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] /\
    [L0x7fffffc490, L0x7fffffc492, L0x7fffffc494, L0x7fffffc496, L0x7fffffc498, L0x7fffffc49a, L0x7fffffc49c, L0x7fffffc49e] = [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] /\
    [L0x7fffffc4a0, L0x7fffffc4a2, L0x7fffffc4a4, L0x7fffffc4a6, L0x7fffffc4a8, L0x7fffffc4aa, L0x7fffffc4ac, L0x7fffffc4ae] = [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] /\
    [L0x7fffffc4b0, L0x7fffffc4b2, L0x7fffffc4b4, L0x7fffffc4b6, L0x7fffffc4b8, L0x7fffffc4ba, L0x7fffffc4bc, L0x7fffffc4be] = [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] /\
    [L0x7fffffc4c0, L0x7fffffc4c2, L0x7fffffc4c4, L0x7fffffc4c6, L0x7fffffc4c8, L0x7fffffc4ca, L0x7fffffc4cc, L0x7fffffc4ce] = [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] /\
    [L0x7fffffc4d0, L0x7fffffc4d2, L0x7fffffc4d4, L0x7fffffc4d6, L0x7fffffc4d8, L0x7fffffc4da, L0x7fffffc4dc, L0x7fffffc4de] = [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] /\
    [L0x7fffffc4e0, L0x7fffffc4e2, L0x7fffffc4e4, L0x7fffffc4e6, L0x7fffffc4e8, L0x7fffffc4ea, L0x7fffffc4ec, L0x7fffffc4ee] = [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] /\
    [L0x7fffffc4f0, L0x7fffffc4f2, L0x7fffffc4f4, L0x7fffffc4f6, L0x7fffffc4f8, L0x7fffffc4fa, L0x7fffffc4fc, L0x7fffffc4fe] = [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] /\
    [L0x7fffffc500, L0x7fffffc502, L0x7fffffc504, L0x7fffffc506, L0x7fffffc508, L0x7fffffc50a, L0x7fffffc50c, L0x7fffffc50e] = [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] /\
    [L0x7fffffc510, L0x7fffffc512, L0x7fffffc514, L0x7fffffc516, L0x7fffffc518, L0x7fffffc51a, L0x7fffffc51c, L0x7fffffc51e] = [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] /\
    [L0x7fffffc520, L0x7fffffc522, L0x7fffffc524, L0x7fffffc526, L0x7fffffc528, L0x7fffffc52a, L0x7fffffc52c, L0x7fffffc52e] = [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] /\
    [L0x7fffffc530, L0x7fffffc532, L0x7fffffc534, L0x7fffffc536, L0x7fffffc538, L0x7fffffc53a, L0x7fffffc53c, L0x7fffffc53e] = [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] /\
    [L0x7fffffc540, L0x7fffffc542, L0x7fffffc544, L0x7fffffc546, L0x7fffffc548, L0x7fffffc54a, L0x7fffffc54c, L0x7fffffc54e] = [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] /\
    [L0x7fffffc550, L0x7fffffc552, L0x7fffffc554, L0x7fffffc556, L0x7fffffc558, L0x7fffffc55a, L0x7fffffc55c, L0x7fffffc55e] = [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] /\
    [L0x7fffffc560, L0x7fffffc562, L0x7fffffc564, L0x7fffffc566, L0x7fffffc568, L0x7fffffc56a, L0x7fffffc56c, L0x7fffffc56e] = [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] /\
    [L0x7fffffc570, L0x7fffffc572, L0x7fffffc574, L0x7fffffc576, L0x7fffffc578, L0x7fffffc57a, L0x7fffffc57c, L0x7fffffc57e] = [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] /\
    [L0x7fffffc580, L0x7fffffc582, L0x7fffffc584, L0x7fffffc586, L0x7fffffc588, L0x7fffffc58a, L0x7fffffc58c, L0x7fffffc58e] = [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] /\
    [L0x7fffffc590, L0x7fffffc592, L0x7fffffc594, L0x7fffffc596, L0x7fffffc598, L0x7fffffc59a, L0x7fffffc59c, L0x7fffffc59e] = [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] /\
    [L0x7fffffc5a0, L0x7fffffc5a2, L0x7fffffc5a4, L0x7fffffc5a6, L0x7fffffc5a8, L0x7fffffc5aa, L0x7fffffc5ac, L0x7fffffc5ae] = [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] /\
    [L0x7fffffc5b0, L0x7fffffc5b2, L0x7fffffc5b4, L0x7fffffc5b6, L0x7fffffc5b8, L0x7fffffc5ba, L0x7fffffc5bc, L0x7fffffc5be] = [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] /\
    [L0x7fffffc5c0, L0x7fffffc5c2, L0x7fffffc5c4, L0x7fffffc5c6, L0x7fffffc5c8, L0x7fffffc5ca, L0x7fffffc5cc, L0x7fffffc5ce] = [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] /\
    [L0x7fffffc5d0, L0x7fffffc5d2, L0x7fffffc5d4, L0x7fffffc5d6, L0x7fffffc5d8, L0x7fffffc5da, L0x7fffffc5dc, L0x7fffffc5de] = [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] /\
    [L0x7fffffc5e0, L0x7fffffc5e2, L0x7fffffc5e4, L0x7fffffc5e6, L0x7fffffc5e8, L0x7fffffc5ea, L0x7fffffc5ec, L0x7fffffc5ee] = [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] /\
    [L0x7fffffc5f0, L0x7fffffc5f2, L0x7fffffc5f4, L0x7fffffc5f6, L0x7fffffc5f8, L0x7fffffc5fa, L0x7fffffc5fc, L0x7fffffc5fe] = [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] /\
    [L0x7fffffc600, L0x7fffffc602, L0x7fffffc604, L0x7fffffc606, L0x7fffffc608, L0x7fffffc60a, L0x7fffffc60c, L0x7fffffc60e] = [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] /\
    [L0x7fffffc610, L0x7fffffc612, L0x7fffffc614, L0x7fffffc616, L0x7fffffc618, L0x7fffffc61a, L0x7fffffc61c, L0x7fffffc61e] = [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] /\
    [L0x7fffffc620, L0x7fffffc622, L0x7fffffc624, L0x7fffffc626, L0x7fffffc628, L0x7fffffc62a, L0x7fffffc62c, L0x7fffffc62e] = [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] /\
    [L0x7fffffc630, L0x7fffffc632, L0x7fffffc634, L0x7fffffc636, L0x7fffffc638, L0x7fffffc63a, L0x7fffffc63c, L0x7fffffc63e] = [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] /\
    [L0x7fffffc640, L0x7fffffc642, L0x7fffffc644, L0x7fffffc646, L0x7fffffc648, L0x7fffffc64a, L0x7fffffc64c, L0x7fffffc64e] = [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] /\
    [L0x7fffffc650, L0x7fffffc652, L0x7fffffc654, L0x7fffffc656, L0x7fffffc658, L0x7fffffc65a, L0x7fffffc65c, L0x7fffffc65e] = [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] /\
    [L0x7fffffc660, L0x7fffffc662, L0x7fffffc664, L0x7fffffc666, L0x7fffffc668, L0x7fffffc66a, L0x7fffffc66c, L0x7fffffc66e] = [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] /\
    [L0x7fffffc670, L0x7fffffc672, L0x7fffffc674, L0x7fffffc676, L0x7fffffc678, L0x7fffffc67a, L0x7fffffc67c, L0x7fffffc67e] = [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] /\
    [L0x7fffffc680, L0x7fffffc682, L0x7fffffc684, L0x7fffffc686, L0x7fffffc688, L0x7fffffc68a, L0x7fffffc68c, L0x7fffffc68e] = [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] /\
    [L0x7fffffc690, L0x7fffffc692, L0x7fffffc694, L0x7fffffc696, L0x7fffffc698, L0x7fffffc69a, L0x7fffffc69c, L0x7fffffc69e] = [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] /\
    [L0x7fffffc6a0, L0x7fffffc6a2, L0x7fffffc6a4, L0x7fffffc6a6, L0x7fffffc6a8, L0x7fffffc6aa, L0x7fffffc6ac, L0x7fffffc6ae] = [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] /\
    [L0x7fffffc6b0, L0x7fffffc6b2, L0x7fffffc6b4, L0x7fffffc6b6, L0x7fffffc6b8, L0x7fffffc6ba, L0x7fffffc6bc, L0x7fffffc6be] = [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] /\
    [L0x7fffffc6c0, L0x7fffffc6c2, L0x7fffffc6c4, L0x7fffffc6c6, L0x7fffffc6c8, L0x7fffffc6ca, L0x7fffffc6cc, L0x7fffffc6ce] = [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] /\
    [L0x7fffffc6d0, L0x7fffffc6d2, L0x7fffffc6d4, L0x7fffffc6d6, L0x7fffffc6d8, L0x7fffffc6da, L0x7fffffc6dc, L0x7fffffc6de] = [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] /\
    [L0x7fffffc6e0, L0x7fffffc6e2, L0x7fffffc6e4, L0x7fffffc6e6, L0x7fffffc6e8, L0x7fffffc6ea, L0x7fffffc6ec, L0x7fffffc6ee] = [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] /\
    [L0x7fffffc6f0, L0x7fffffc6f2, L0x7fffffc6f4, L0x7fffffc6f6, L0x7fffffc6f8, L0x7fffffc6fa, L0x7fffffc6fc, L0x7fffffc6fe] = [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] /\
    [L0x7fffffc700, L0x7fffffc702, L0x7fffffc704, L0x7fffffc706, L0x7fffffc708, L0x7fffffc70a, L0x7fffffc70c, L0x7fffffc70e] = [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] /\
    [L0x7fffffc710, L0x7fffffc712, L0x7fffffc714, L0x7fffffc716, L0x7fffffc718, L0x7fffffc71a, L0x7fffffc71c, L0x7fffffc71e] = [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] /\
    [L0x7fffffc720, L0x7fffffc722, L0x7fffffc724, L0x7fffffc726, L0x7fffffc728, L0x7fffffc72a, L0x7fffffc72c, L0x7fffffc72e] = [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] /\
    [L0x7fffffc730, L0x7fffffc732, L0x7fffffc734, L0x7fffffc736, L0x7fffffc738, L0x7fffffc73a, L0x7fffffc73c, L0x7fffffc73e] = [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] /\
    [L0x7fffffc740, L0x7fffffc742, L0x7fffffc744, L0x7fffffc746, L0x7fffffc748, L0x7fffffc74a, L0x7fffffc74c, L0x7fffffc74e] = [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] /\
    [L0x7fffffc750, L0x7fffffc752, L0x7fffffc754, L0x7fffffc756, L0x7fffffc758, L0x7fffffc75a, L0x7fffffc75c, L0x7fffffc75e] = [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] /\
    [L0x7fffffc760, L0x7fffffc762, L0x7fffffc764, L0x7fffffc766, L0x7fffffc768, L0x7fffffc76a, L0x7fffffc76c, L0x7fffffc76e] = [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] /\
    [L0x7fffffc770, L0x7fffffc772, L0x7fffffc774, L0x7fffffc776, L0x7fffffc778, L0x7fffffc77a, L0x7fffffc77c, L0x7fffffc77e] = [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] /\
    [L0x7fffffc780, L0x7fffffc782, L0x7fffffc784, L0x7fffffc786, L0x7fffffc788, L0x7fffffc78a, L0x7fffffc78c, L0x7fffffc78e] = [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] /\
    [L0x7fffffc790, L0x7fffffc792, L0x7fffffc794, L0x7fffffc796, L0x7fffffc798, L0x7fffffc79a, L0x7fffffc79c, L0x7fffffc79e] = [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] /\
    [L0x7fffffc7a0, L0x7fffffc7a2, L0x7fffffc7a4, L0x7fffffc7a6, L0x7fffffc7a8, L0x7fffffc7aa, L0x7fffffc7ac, L0x7fffffc7ae] = [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] /\
    [L0x7fffffc7b0, L0x7fffffc7b2, L0x7fffffc7b4, L0x7fffffc7b6, L0x7fffffc7b8, L0x7fffffc7ba, L0x7fffffc7bc, L0x7fffffc7be] = [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] /\
    [L0x7fffffc7c0, L0x7fffffc7c2, L0x7fffffc7c4, L0x7fffffc7c6, L0x7fffffc7c8, L0x7fffffc7ca, L0x7fffffc7cc, L0x7fffffc7ce] = [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] /\
    [L0x7fffffc7d0, L0x7fffffc7d2, L0x7fffffc7d4, L0x7fffffc7d6, L0x7fffffc7d8, L0x7fffffc7da, L0x7fffffc7dc, L0x7fffffc7de] = [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] /\
    [L0x7fffffc7e0, L0x7fffffc7e2, L0x7fffffc7e4, L0x7fffffc7e6, L0x7fffffc7e8, L0x7fffffc7ea, L0x7fffffc7ec, L0x7fffffc7ee] = [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] /\
    [L0x7fffffc7f0, L0x7fffffc7f2, L0x7fffffc7f4, L0x7fffffc7f6, L0x7fffffc7f8, L0x7fffffc7fa, L0x7fffffc7fc, L0x7fffffc7fe] = [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] /\
    [L0x7fffffc800, L0x7fffffc802, L0x7fffffc804, L0x7fffffc806, L0x7fffffc808, L0x7fffffc80a, L0x7fffffc80c, L0x7fffffc80e] = [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] /\
    [L0x7fffffc810, L0x7fffffc812, L0x7fffffc814, L0x7fffffc816, L0x7fffffc818, L0x7fffffc81a, L0x7fffffc81c, L0x7fffffc81e] = [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] /\
    [L0x7fffffc820, L0x7fffffc822, L0x7fffffc824, L0x7fffffc826, L0x7fffffc828, L0x7fffffc82a, L0x7fffffc82c, L0x7fffffc82e] = [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] /\
    [L0x7fffffc830, L0x7fffffc832, L0x7fffffc834, L0x7fffffc836, L0x7fffffc838, L0x7fffffc83a, L0x7fffffc83c, L0x7fffffc83e] = [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] /\
    [L0x7fffffc840, L0x7fffffc842, L0x7fffffc844, L0x7fffffc846, L0x7fffffc848, L0x7fffffc84a, L0x7fffffc84c, L0x7fffffc84e] = [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] /\
    [L0x7fffffc850, L0x7fffffc852, L0x7fffffc854, L0x7fffffc856, L0x7fffffc858, L0x7fffffc85a, L0x7fffffc85c, L0x7fffffc85e] = [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] /\
    [L0x7fffffc860, L0x7fffffc862, L0x7fffffc864, L0x7fffffc866, L0x7fffffc868, L0x7fffffc86a, L0x7fffffc86c, L0x7fffffc86e] = [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] /\
    [L0x7fffffc870, L0x7fffffc872, L0x7fffffc874, L0x7fffffc876, L0x7fffffc878, L0x7fffffc87a, L0x7fffffc87c, L0x7fffffc87e] = [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] /\
    [L0x7fffffc880, L0x7fffffc882, L0x7fffffc884, L0x7fffffc886, L0x7fffffc888, L0x7fffffc88a, L0x7fffffc88c, L0x7fffffc88e] = [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] /\
    [L0x7fffffc890, L0x7fffffc892, L0x7fffffc894, L0x7fffffc896, L0x7fffffc898, L0x7fffffc89a, L0x7fffffc89c, L0x7fffffc89e] = [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] /\
    [L0x7fffffc8a0, L0x7fffffc8a2, L0x7fffffc8a4, L0x7fffffc8a6, L0x7fffffc8a8, L0x7fffffc8aa, L0x7fffffc8ac, L0x7fffffc8ae] = [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] /\
    [L0x7fffffc8b0, L0x7fffffc8b2, L0x7fffffc8b4, L0x7fffffc8b6, L0x7fffffc8b8, L0x7fffffc8ba, L0x7fffffc8bc, L0x7fffffc8be] = [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] /\
    [L0x7fffffc8c0, L0x7fffffc8c2, L0x7fffffc8c4, L0x7fffffc8c6, L0x7fffffc8c8, L0x7fffffc8ca, L0x7fffffc8cc, L0x7fffffc8ce] = [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] /\
    [L0x7fffffc8d0, L0x7fffffc8d2, L0x7fffffc8d4, L0x7fffffc8d6, L0x7fffffc8d8, L0x7fffffc8da, L0x7fffffc8dc, L0x7fffffc8de] = [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] /\
    [L0x7fffffc8e0, L0x7fffffc8e2, L0x7fffffc8e4, L0x7fffffc8e6, L0x7fffffc8e8, L0x7fffffc8ea, L0x7fffffc8ec, L0x7fffffc8ee] = [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] /\
    [L0x7fffffc8f0, L0x7fffffc8f2, L0x7fffffc8f4, L0x7fffffc8f6, L0x7fffffc8f8, L0x7fffffc8fa, L0x7fffffc8fc, L0x7fffffc8fe] = [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] /\
    [L0x7fffffc900, L0x7fffffc902, L0x7fffffc904, L0x7fffffc906, L0x7fffffc908, L0x7fffffc90a, L0x7fffffc90c, L0x7fffffc90e] = [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] /\
    [L0x7fffffc910, L0x7fffffc912, L0x7fffffc914, L0x7fffffc916, L0x7fffffc918, L0x7fffffc91a, L0x7fffffc91c, L0x7fffffc91e] = [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] /\
    [L0x7fffffc920, L0x7fffffc922, L0x7fffffc924, L0x7fffffc926, L0x7fffffc928, L0x7fffffc92a, L0x7fffffc92c, L0x7fffffc92e] = [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] /\
    [L0x7fffffc930, L0x7fffffc932, L0x7fffffc934, L0x7fffffc936, L0x7fffffc938, L0x7fffffc93a, L0x7fffffc93c, L0x7fffffc93e] = [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] /\
    [L0x7fffffc940, L0x7fffffc942, L0x7fffffc944, L0x7fffffc946, L0x7fffffc948, L0x7fffffc94a, L0x7fffffc94c, L0x7fffffc94e] = [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] /\
    [L0x7fffffc950, L0x7fffffc952, L0x7fffffc954, L0x7fffffc956, L0x7fffffc958, L0x7fffffc95a, L0x7fffffc95c, L0x7fffffc95e] = [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] /\
    [L0x7fffffc960, L0x7fffffc962, L0x7fffffc964, L0x7fffffc966, L0x7fffffc968, L0x7fffffc96a, L0x7fffffc96c, L0x7fffffc96e] = [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] /\
    [L0x7fffffc970, L0x7fffffc972, L0x7fffffc974, L0x7fffffc976, L0x7fffffc978, L0x7fffffc97a, L0x7fffffc97c, L0x7fffffc97e] = [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] /\
    [L0x7fffffc980, L0x7fffffc982, L0x7fffffc984, L0x7fffffc986, L0x7fffffc988, L0x7fffffc98a, L0x7fffffc98c, L0x7fffffc98e] = [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] /\
    [L0x7fffffc990, L0x7fffffc992, L0x7fffffc994, L0x7fffffc996, L0x7fffffc998, L0x7fffffc99a, L0x7fffffc99c, L0x7fffffc99e] = [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] /\
    [L0x7fffffc9a0, L0x7fffffc9a2, L0x7fffffc9a4, L0x7fffffc9a6, L0x7fffffc9a8, L0x7fffffc9aa, L0x7fffffc9ac, L0x7fffffc9ae] = [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] /\
    [L0x7fffffc9b0, L0x7fffffc9b2, L0x7fffffc9b4, L0x7fffffc9b6, L0x7fffffc9b8, L0x7fffffc9ba, L0x7fffffc9bc, L0x7fffffc9be] = [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] /\
    [L0x7fffffc9c0, L0x7fffffc9c2, L0x7fffffc9c4, L0x7fffffc9c6, L0x7fffffc9c8, L0x7fffffc9ca, L0x7fffffc9cc, L0x7fffffc9ce] = [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] /\
    [L0x7fffffc9d0, L0x7fffffc9d2, L0x7fffffc9d4, L0x7fffffc9d6, L0x7fffffc9d8, L0x7fffffc9da, L0x7fffffc9dc, L0x7fffffc9de] = [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] /\
    [L0x7fffffc9e0, L0x7fffffc9e2, L0x7fffffc9e4, L0x7fffffc9e6, L0x7fffffc9e8, L0x7fffffc9ea, L0x7fffffc9ec, L0x7fffffc9ee] = [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] /\
    [L0x7fffffc9f0, L0x7fffffc9f2, L0x7fffffc9f4, L0x7fffffc9f6, L0x7fffffc9f8, L0x7fffffc9fa, L0x7fffffc9fc, L0x7fffffc9fe] = [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] /\
    [L0x7fffffca00, L0x7fffffca02, L0x7fffffca04, L0x7fffffca06, L0x7fffffca08, L0x7fffffca0a, L0x7fffffca0c, L0x7fffffca0e] = [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] /\
    [L0x7fffffca10, L0x7fffffca12, L0x7fffffca14, L0x7fffffca16, L0x7fffffca18, L0x7fffffca1a, L0x7fffffca1c, L0x7fffffca1e] = [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] /\
    [L0x7fffffca20, L0x7fffffca22, L0x7fffffca24, L0x7fffffca26, L0x7fffffca28, L0x7fffffca2a, L0x7fffffca2c, L0x7fffffca2e] = [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] /\
    [L0x7fffffca30, L0x7fffffca32, L0x7fffffca34, L0x7fffffca36, L0x7fffffca38, L0x7fffffca3a, L0x7fffffca3c, L0x7fffffca3e] = [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] /\
    [L0x7fffffca40, L0x7fffffca42, L0x7fffffca44, L0x7fffffca46, L0x7fffffca48, L0x7fffffca4a, L0x7fffffca4c, L0x7fffffca4e] = [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] /\
    [L0x7fffffca50, L0x7fffffca52, L0x7fffffca54, L0x7fffffca56, L0x7fffffca58, L0x7fffffca5a, L0x7fffffca5c, L0x7fffffca5e] = [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] /\
    [L0x7fffffca60, L0x7fffffca62, L0x7fffffca64, L0x7fffffca66, L0x7fffffca68, L0x7fffffca6a, L0x7fffffca6c, L0x7fffffca6e] = [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] /\
    [L0x7fffffca70, L0x7fffffca72, L0x7fffffca74, L0x7fffffca76, L0x7fffffca78, L0x7fffffca7a, L0x7fffffca7c, L0x7fffffca7e] = [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] /\
    [L0x7fffffca80, L0x7fffffca82, L0x7fffffca84, L0x7fffffca86, L0x7fffffca88, L0x7fffffca8a, L0x7fffffca8c, L0x7fffffca8e] = [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] /\
    [L0x7fffffca90, L0x7fffffca92, L0x7fffffca94, L0x7fffffca96, L0x7fffffca98, L0x7fffffca9a, L0x7fffffca9c, L0x7fffffca9e] = [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] /\
    [L0x7fffffcaa0, L0x7fffffcaa2, L0x7fffffcaa4, L0x7fffffcaa6, L0x7fffffcaa8, L0x7fffffcaaa, L0x7fffffcaac, L0x7fffffcaae] = [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] /\
    [L0x7fffffcab0, L0x7fffffcab2, L0x7fffffcab4, L0x7fffffcab6, L0x7fffffcab8, L0x7fffffcaba, L0x7fffffcabc, L0x7fffffcabe] = [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] /\
    [L0x7fffffcac0, L0x7fffffcac2, L0x7fffffcac4, L0x7fffffcac6, L0x7fffffcac8, L0x7fffffcaca, L0x7fffffcacc, L0x7fffffcace] = [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] /\
    [L0x7fffffcad0, L0x7fffffcad2, L0x7fffffcad4, L0x7fffffcad6, L0x7fffffcad8, L0x7fffffcada, L0x7fffffcadc, L0x7fffffcade] = [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] /\
    [L0x7fffffcae0, L0x7fffffcae2, L0x7fffffcae4, L0x7fffffcae6, L0x7fffffcae8, L0x7fffffcaea, L0x7fffffcaec, L0x7fffffcaee] = [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] /\
    [L0x7fffffcaf0, L0x7fffffcaf2, L0x7fffffcaf4, L0x7fffffcaf6, L0x7fffffcaf8, L0x7fffffcafa, L0x7fffffcafc, L0x7fffffcafe] = [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] /\
    [L0x7fffffcb00, L0x7fffffcb02, L0x7fffffcb04, L0x7fffffcb06, L0x7fffffcb08, L0x7fffffcb0a, L0x7fffffcb0c, L0x7fffffcb0e] = [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] /\
    [L0x7fffffcb10, L0x7fffffcb12, L0x7fffffcb14, L0x7fffffcb16, L0x7fffffcb18, L0x7fffffcb1a, L0x7fffffcb1c, L0x7fffffcb1e] = [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] /\
    [L0x7fffffcb20, L0x7fffffcb22, L0x7fffffcb24, L0x7fffffcb26, L0x7fffffcb28, L0x7fffffcb2a, L0x7fffffcb2c, L0x7fffffcb2e] = [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] /\
    [L0x7fffffcb30, L0x7fffffcb32, L0x7fffffcb34, L0x7fffffcb36, L0x7fffffcb38, L0x7fffffcb3a, L0x7fffffcb3c, L0x7fffffcb3e] = [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] /\
    [L0x7fffffcb40, L0x7fffffcb42, L0x7fffffcb44, L0x7fffffcb46, L0x7fffffcb48, L0x7fffffcb4a, L0x7fffffcb4c, L0x7fffffcb4e] = [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] /\
    [L0x7fffffcb50, L0x7fffffcb52, L0x7fffffcb54, L0x7fffffcb56, L0x7fffffcb58, L0x7fffffcb5a, L0x7fffffcb5c, L0x7fffffcb5e] = [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] /\
    [L0x7fffffcb60, L0x7fffffcb62, L0x7fffffcb64, L0x7fffffcb66, L0x7fffffcb68, L0x7fffffcb6a, L0x7fffffcb6c, L0x7fffffcb6e] = [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] /\
    [L0x7fffffcb70, L0x7fffffcb72, L0x7fffffcb74, L0x7fffffcb76, L0x7fffffcb78, L0x7fffffcb7a, L0x7fffffcb7c, L0x7fffffcb7e] = [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] /\
    [L0x7fffffcb80, L0x7fffffcb82, L0x7fffffcb84, L0x7fffffcb86, L0x7fffffcb88, L0x7fffffcb8a, L0x7fffffcb8c, L0x7fffffcb8e] = [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] /\
    [L0x7fffffcb90, L0x7fffffcb92, L0x7fffffcb94, L0x7fffffcb96, L0x7fffffcb98, L0x7fffffcb9a, L0x7fffffcb9c, L0x7fffffcb9e] = [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] /\
    [L0x7fffffcba0, L0x7fffffcba2, L0x7fffffcba4, L0x7fffffcba6, L0x7fffffcba8, L0x7fffffcbaa, L0x7fffffcbac, L0x7fffffcbae] = [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] /\
    [L0x7fffffcbb0, L0x7fffffcbb2, L0x7fffffcbb4, L0x7fffffcbb6, L0x7fffffcbb8, L0x7fffffcbba, L0x7fffffcbbc, L0x7fffffcbbe] = [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] /\
    [L0x7fffffcbc0, L0x7fffffcbc2, L0x7fffffcbc4, L0x7fffffcbc6, L0x7fffffcbc8, L0x7fffffcbca, L0x7fffffcbcc, L0x7fffffcbce] = [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] /\
    [L0x7fffffcbd0, L0x7fffffcbd2, L0x7fffffcbd4, L0x7fffffcbd6, L0x7fffffcbd8, L0x7fffffcbda, L0x7fffffcbdc, L0x7fffffcbde] = [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] /\
    [L0x7fffffcbe0, L0x7fffffcbe2, L0x7fffffcbe4, L0x7fffffcbe6, L0x7fffffcbe8, L0x7fffffcbea, L0x7fffffcbec, L0x7fffffcbee] = [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] /\
    [L0x7fffffcbf0, L0x7fffffcbf2, L0x7fffffcbf4, L0x7fffffcbf6, L0x7fffffcbf8, L0x7fffffcbfa, L0x7fffffcbfc, L0x7fffffcbfe] = [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] /\
    [L0x7fffffcc00, L0x7fffffcc02, L0x7fffffcc04, L0x7fffffcc06, L0x7fffffcc08, L0x7fffffcc0a, L0x7fffffcc0c, L0x7fffffcc0e] = [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] /\
    [L0x7fffffcc10, L0x7fffffcc12, L0x7fffffcc14, L0x7fffffcc16, L0x7fffffcc18, L0x7fffffcc1a, L0x7fffffcc1c, L0x7fffffcc1e] = [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] /\
    [L0x7fffffcc20, L0x7fffffcc22, L0x7fffffcc24, L0x7fffffcc26, L0x7fffffcc28, L0x7fffffcc2a, L0x7fffffcc2c, L0x7fffffcc2e] = [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] /\
    [L0x7fffffcc30, L0x7fffffcc32, L0x7fffffcc34, L0x7fffffcc36, L0x7fffffcc38, L0x7fffffcc3a, L0x7fffffcc3c, L0x7fffffcc3e] = [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] /\
    [L0x7fffffcc40, L0x7fffffcc42, L0x7fffffcc44, L0x7fffffcc46, L0x7fffffcc48, L0x7fffffcc4a, L0x7fffffcc4c, L0x7fffffcc4e] = [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] /\
    [L0x7fffffcc50, L0x7fffffcc52, L0x7fffffcc54, L0x7fffffcc56, L0x7fffffcc58, L0x7fffffcc5a, L0x7fffffcc5c, L0x7fffffcc5e] = [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] /\
    [L0x7fffffcc60, L0x7fffffcc62, L0x7fffffcc64, L0x7fffffcc66, L0x7fffffcc68, L0x7fffffcc6a, L0x7fffffcc6c, L0x7fffffcc6e] = [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] /\
    [L0x7fffffcc70, L0x7fffffcc72, L0x7fffffcc74, L0x7fffffcc76, L0x7fffffcc78, L0x7fffffcc7a, L0x7fffffcc7c, L0x7fffffcc7e] = [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] /\
    [L0x7fffffcc80, L0x7fffffcc82, L0x7fffffcc84, L0x7fffffcc86, L0x7fffffcc88, L0x7fffffcc8a, L0x7fffffcc8c, L0x7fffffcc8e] = [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] /\
    [L0x7fffffcc90, L0x7fffffcc92, L0x7fffffcc94, L0x7fffffcc96, L0x7fffffcc98, L0x7fffffcc9a, L0x7fffffcc9c, L0x7fffffcc9e] = [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] /\
    [L0x7fffffcca0, L0x7fffffcca2, L0x7fffffcca4, L0x7fffffcca6, L0x7fffffcca8, L0x7fffffccaa, L0x7fffffccac, L0x7fffffccae] = [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] /\
    [L0x7fffffccb0, L0x7fffffccb2, L0x7fffffccb4, L0x7fffffccb6, L0x7fffffccb8, L0x7fffffccba, L0x7fffffccbc, L0x7fffffccbe] = [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] /\
    [L0x7fffffccc0, L0x7fffffccc2, L0x7fffffccc4, L0x7fffffccc6, L0x7fffffccc8, L0x7fffffccca, L0x7fffffcccc, L0x7fffffccce] = [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] /\
    [L0x7fffffccd0, L0x7fffffccd2, L0x7fffffccd4, L0x7fffffccd6, L0x7fffffccd8, L0x7fffffccda, L0x7fffffccdc, L0x7fffffccde] = [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] /\
    [L0x7fffffcce0, L0x7fffffcce2, L0x7fffffcce4, L0x7fffffcce6, L0x7fffffcce8, L0x7fffffccea, L0x7fffffccec, L0x7fffffccee] = [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] /\
    [L0x7fffffccf0, L0x7fffffccf2, L0x7fffffccf4, L0x7fffffccf6, L0x7fffffccf8, L0x7fffffccfa, L0x7fffffccfc, L0x7fffffccfe] = [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] /\
    [L0x7fffffcd00, L0x7fffffcd02, L0x7fffffcd04, L0x7fffffcd06, L0x7fffffcd08, L0x7fffffcd0a, L0x7fffffcd0c, L0x7fffffcd0e] = [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] /\
    [L0x7fffffcd10, L0x7fffffcd12, L0x7fffffcd14, L0x7fffffcd16, L0x7fffffcd18, L0x7fffffcd1a, L0x7fffffcd1c, L0x7fffffcd1e] = [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] /\
    [L0x7fffffcd20, L0x7fffffcd22, L0x7fffffcd24, L0x7fffffcd26, L0x7fffffcd28, L0x7fffffcd2a, L0x7fffffcd2c, L0x7fffffcd2e] = [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] /\
    [L0x7fffffcd30, L0x7fffffcd32, L0x7fffffcd34, L0x7fffffcd36, L0x7fffffcd38, L0x7fffffcd3a, L0x7fffffcd3c, L0x7fffffcd3e] = [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] /\
    [L0x7fffffcd40, L0x7fffffcd42, L0x7fffffcd44, L0x7fffffcd46, L0x7fffffcd48, L0x7fffffcd4a, L0x7fffffcd4c, L0x7fffffcd4e] = [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] /\
    [L0x7fffffcd50, L0x7fffffcd52, L0x7fffffcd54, L0x7fffffcd56, L0x7fffffcd58, L0x7fffffcd5a, L0x7fffffcd5c, L0x7fffffcd5e] = [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] /\
    [L0x7fffffcd60, L0x7fffffcd62, L0x7fffffcd64, L0x7fffffcd66, L0x7fffffcd68, L0x7fffffcd6a, L0x7fffffcd6c, L0x7fffffcd6e] = [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] /\
    [L0x7fffffcd70, L0x7fffffcd72, L0x7fffffcd74, L0x7fffffcd76, L0x7fffffcd78, L0x7fffffcd7a, L0x7fffffcd7c, L0x7fffffcd7e] = [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] /\
    [L0x7fffffcd80, L0x7fffffcd82, L0x7fffffcd84, L0x7fffffcd86, L0x7fffffcd88, L0x7fffffcd8a, L0x7fffffcd8c, L0x7fffffcd8e] = [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] /\
    [L0x7fffffcd90, L0x7fffffcd92, L0x7fffffcd94, L0x7fffffcd96, L0x7fffffcd98, L0x7fffffcd9a, L0x7fffffcd9c, L0x7fffffcd9e] = [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] /\
    [L0x7fffffcda0, L0x7fffffcda2, L0x7fffffcda4, L0x7fffffcda6, L0x7fffffcda8, L0x7fffffcdaa, L0x7fffffcdac, L0x7fffffcdae] = [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] /\
    [L0x7fffffcdb0, L0x7fffffcdb2, L0x7fffffcdb4, L0x7fffffcdb6, L0x7fffffcdb8, L0x7fffffcdba, L0x7fffffcdbc, L0x7fffffcdbe] = [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] /\
    [L0x7fffffcdc0, L0x7fffffcdc2, L0x7fffffcdc4, L0x7fffffcdc6, L0x7fffffcdc8, L0x7fffffcdca, L0x7fffffcdcc, L0x7fffffcdce] = [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] /\
    [L0x7fffffcdd0, L0x7fffffcdd2, L0x7fffffcdd4, L0x7fffffcdd6, L0x7fffffcdd8, L0x7fffffcdda, L0x7fffffcddc, L0x7fffffcdde] = [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] /\
    [L0x7fffffcde0, L0x7fffffcde2, L0x7fffffcde4, L0x7fffffcde6, L0x7fffffcde8, L0x7fffffcdea, L0x7fffffcdec, L0x7fffffcdee] = [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] /\
    [L0x7fffffcdf0, L0x7fffffcdf2, L0x7fffffcdf4, L0x7fffffcdf6, L0x7fffffcdf8, L0x7fffffcdfa, L0x7fffffcdfc, L0x7fffffcdfe] = [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] /\
    [L0x7fffffce00, L0x7fffffce02, L0x7fffffce04, L0x7fffffce06, L0x7fffffce08, L0x7fffffce0a, L0x7fffffce0c, L0x7fffffce0e] = [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] /\
    [L0x7fffffce10, L0x7fffffce12, L0x7fffffce14, L0x7fffffce16, L0x7fffffce18, L0x7fffffce1a, L0x7fffffce1c, L0x7fffffce1e] = [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] /\
    [L0x7fffffce20, L0x7fffffce22, L0x7fffffce24, L0x7fffffce26, L0x7fffffce28, L0x7fffffce2a, L0x7fffffce2c, L0x7fffffce2e] = [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] /\
    [L0x7fffffce30, L0x7fffffce32, L0x7fffffce34, L0x7fffffce36, L0x7fffffce38, L0x7fffffce3a, L0x7fffffce3c, L0x7fffffce3e] = [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] /\
    [L0x7fffffce40, L0x7fffffce42, L0x7fffffce44, L0x7fffffce46, L0x7fffffce48, L0x7fffffce4a, L0x7fffffce4c, L0x7fffffce4e] = [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] /\
    [L0x7fffffce50, L0x7fffffce52, L0x7fffffce54, L0x7fffffce56, L0x7fffffce58, L0x7fffffce5a, L0x7fffffce5c, L0x7fffffce5e] = [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] /\
    [L0x7fffffce60, L0x7fffffce62, L0x7fffffce64, L0x7fffffce66, L0x7fffffce68, L0x7fffffce6a, L0x7fffffce6c, L0x7fffffce6e] = [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] /\
    [L0x7fffffce70, L0x7fffffce72, L0x7fffffce74, L0x7fffffce76, L0x7fffffce78, L0x7fffffce7a, L0x7fffffce7c, L0x7fffffce7e] = [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] /\
    [L0x7fffffce80, L0x7fffffce82, L0x7fffffce84, L0x7fffffce86, L0x7fffffce88, L0x7fffffce8a, L0x7fffffce8c, L0x7fffffce8e] = [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] /\

    [L0x5555552810, L0x5555552812, L0x5555552814, L0x5555552816, L0x5555552818, L0x555555281a, L0x555555281c, L0x555555281e, L0x5555552820, L0x5555552822, L0x5555552824, L0x5555552826, L0x5555552828, L0x555555282a, L0x555555282c, L0x555555282e] = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16, (-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16];

## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc370; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc370,L0x7fffffc372,L0x7fffffc374,L0x7fffffc376,L0x7fffffc378,L0x7fffffc37a,L0x7fffffc37c,L0x7fffffc37e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc380; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc380,L0x7fffffc382,L0x7fffffc384,L0x7fffffc386,L0x7fffffc388,L0x7fffffc38a,L0x7fffffc38c,L0x7fffffc38e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc390; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc390,L0x7fffffc392,L0x7fffffc394,L0x7fffffc396,L0x7fffffc398,L0x7fffffc39a,L0x7fffffc39c,L0x7fffffc39e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc3a0; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc3a0,L0x7fffffc3a2,L0x7fffffc3a4,L0x7fffffc3a6,L0x7fffffc3a8,L0x7fffffc3aa,L0x7fffffc3ac,L0x7fffffc3ae];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc3b0; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc3b0,L0x7fffffc3b2,L0x7fffffc3b4,L0x7fffffc3b6,L0x7fffffc3b8,L0x7fffffc3ba,L0x7fffffc3bc,L0x7fffffc3be];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc3c0; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc3c0,L0x7fffffc3c2,L0x7fffffc3c4,L0x7fffffc3c6,L0x7fffffc3c8,L0x7fffffc3ca,L0x7fffffc3cc,L0x7fffffc3ce];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc3d0; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc3d0,L0x7fffffc3d2,L0x7fffffc3d4,L0x7fffffc3d6,L0x7fffffc3d8,L0x7fffffc3da,L0x7fffffc3dc,L0x7fffffc3de];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc360; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc360,L0x7fffffc362,L0x7fffffc364,L0x7fffffc366,L0x7fffffc368,L0x7fffffc36a,L0x7fffffc36c,L0x7fffffc36e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc350; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc350,L0x7fffffc352,L0x7fffffc354,L0x7fffffc356,L0x7fffffc358,L0x7fffffc35a,L0x7fffffc35c,L0x7fffffc35e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_00@sint16[8], %fa1_00@sint16[8], %fa2_00@sint16[8], %fa3_00@sint16[8], %fa4_00@sint16[8], %fa5_00@sint16[8], %fa6_00@sint16[8], %fa7_00@sint16[8], %fa8_00@sint16[8] :
    %fa0_00 = %v0 /\ %fa1_00 = %v1 /\ %fa2_00 = %v2 /\ %fa3_00 = %v3 /\
    %fa4_00 = %v4 /\ %fa5_00 = %v5 /\ %fa6_00 = %v6 /\ %fa7_00 = %v7 /\
    %fa8_00 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_00 = %v0 /\ %fa1_00 = %v1 /\ %fa2_00 = %v2 /\ %fa3_00 = %v3 /\
    %fa4_00 = %v4 /\ %fa5_00 = %v5 /\ %fa6_00 = %v6 /\ %fa7_00 = %v7 /\
    %fa8_00 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 1 *)
    %fa0_00 = %v0 /\ %fa1_00 = %v1 /\ %fa2_00 = %v2 /\ %fa3_00 = %v3 /\
    %fa4_00 = %v4 /\ %fa5_00 = %v5 /\ %fa6_00 = %v6 /\ %fa7_00 = %v7 /\
    %fa8_00 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_00 = [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] /\
    %fa1_00 = [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] /\
    %fa2_00 = [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] /\
    %fa3_00 = [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] /\
    %fa4_00 = [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] /\
    %fa5_00 = [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] /\
    %fa6_00 = [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] /\
    %fa7_00 = [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] /\
    %fa8_00 = [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_00 = %v0 /\ %fa1_00 = %v1 /\ %fa2_00 = %v2 /\ %fa3_00 = %v3 /\
    %fa4_00 = %v4 /\ %fa5_00 = %v5 /\ %fa6_00 = %v6 /\ %fa7_00 = %v7 /\
    %fa8_00 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_00 = [arr0000, arr0001, arr0002, arr0003, arr0004, arr0005, arr0006, arr0007] /\
    %fa1_00 = [arr0080, arr0081, arr0082, arr0083, arr0084, arr0085, arr0086, arr0087] /\
    %fa2_00 = [arr0070, arr0071, arr0072, arr0073, arr0074, arr0075, arr0076, arr0077] /\
    %fa3_00 = [arr0060, arr0061, arr0062, arr0063, arr0064, arr0065, arr0066, arr0067] /\
    %fa4_00 = [arr0050, arr0051, arr0052, arr0053, arr0054, arr0055, arr0056, arr0057] /\
    %fa5_00 = [arr0040, arr0041, arr0042, arr0043, arr0044, arr0045, arr0046, arr0047] /\
    %fa6_00 = [arr0030, arr0031, arr0032, arr0033, arr0034, arr0035, arr0036, arr0037] /\
    %fa7_00 = [arr0020, arr0021, arr0022, arr0023, arr0024, arr0025, arr0026, arr0027] /\
    %fa8_00 = [arr0010, arr0011, arr0012, arr0013, arr0014, arr0015, arr0016, arr0017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_00 + %fa3_00 + %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_00 + %fa3_00 + %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_00 + %fa3_00 + %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_00 + %fa3_00 + %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_00 - %fa3_00 - %fa6_00
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_00 - %fa6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_00 - %fa4_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_00 - %fa4_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_00 - %fa4_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_00 - %fa4_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_00 - %fa5_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_00 - %fa5_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_00 - %fa5_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_00 - %fa5_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 2 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        %fa3_00 +
        %fa6_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_00 +
        %fa7_00 +
        %fa4_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_00 +
        %fa2_00 +
        %fa5_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_00 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_00 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 1], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 1]];


ghost %fb0_00@sint16[8], %fb1_00@sint16[8], %fb2_00@sint16[8], %fb3_00@sint16[8], %fb4_00@sint16[8], %fb5_00@sint16[8], %fb6_00@sint16[8], %fb7_00@sint16[8], %fb8_00@sint16[8] :
    %fb0_00 = %v17 /\ %fb1_00 = %v16 /\ %fb2_00 = %v0  /\ %fb3_00 = %v6  /\
    %fb4_00 = %v3  /\ %fb5_00 = %v7  /\ %fb6_00 = %v19 /\ %fb7_00 = %v1  /\ %fb8_00 = %v2
  &&
    %fb0_00 = %v17 /\ %fb1_00 = %v16 /\ %fb2_00 = %v0  /\ %fb3_00 = %v6  /\
    %fb4_00 = %v3  /\ %fb5_00 = %v7  /\ %fb6_00 = %v19 /\ %fb7_00 = %v1  /\ %fb8_00 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_00 + %fb6_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_00 + %fb6_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_00 + %fb6_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_00 + %fb6_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_00 - %fb6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_00 - %fb6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_00 - %fb6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_00 - %fb6_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_00@sint16[8] : %b1i1_00 = %v1 && %b1i1_00 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_00@sint16[8] : %b1i2_00 = %v5 && %b1i2_00 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_00 + %b1i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_00 + %b1i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_00 + %b1i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_00 + %b1i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_00 - %b1i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_00 - %b1i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_00 - %b1i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_00 - %b1i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_00@sint16[8] : %b2i1_00 = %v3 && %b2i1_00 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_00@sint16[8] : %b2i2_00 = %v1 && %b2i2_00 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_00 + %b2i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_00 + %b2i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_00 + %b2i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_00 + %b2i2_00
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_00 - %b2i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_00 - %b2i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_00 - %b2i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_00 - %b2i2_00)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 3 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        %fa1_00 +
        %fa2_00 +
        %fa3_00 +
        %fa4_00 +
        %fa5_00 +
        %fa6_00 +
        %fa7_00 +
        %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_00 +
        %fa3_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_00 +
        %fa6_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_00 +
        %fa3_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_00 +
        %fa6_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_00 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_00 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_00 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_00 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_00 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_00 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_00 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_00 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_00
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 1]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 1], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 1]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc350; PC = 0x55555514f8 *)
mov [L0x7fffffc350,L0x7fffffc352,L0x7fffffc354,L0x7fffffc356,L0x7fffffc358,L0x7fffffc35a,L0x7fffffc35c,L0x7fffffc35e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc360; PC = 0x5555551500 *)
mov [L0x7fffffc360,L0x7fffffc362,L0x7fffffc364,L0x7fffffc366,L0x7fffffc368,L0x7fffffc36a,L0x7fffffc36c,L0x7fffffc36e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc370; PC = 0x5555551508 *)
mov [L0x7fffffc370,L0x7fffffc372,L0x7fffffc374,L0x7fffffc376,L0x7fffffc378,L0x7fffffc37a,L0x7fffffc37c,L0x7fffffc37e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc380; PC = 0x5555551510 *)
mov [L0x7fffffc380,L0x7fffffc382,L0x7fffffc384,L0x7fffffc386,L0x7fffffc388,L0x7fffffc38a,L0x7fffffc38c,L0x7fffffc38e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc390; PC = 0x5555551518 *)
mov [L0x7fffffc390,L0x7fffffc392,L0x7fffffc394,L0x7fffffc396,L0x7fffffc398,L0x7fffffc39a,L0x7fffffc39c,L0x7fffffc39e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc3a0; PC = 0x5555551520 *)
mov [L0x7fffffc3a0,L0x7fffffc3a2,L0x7fffffc3a4,L0x7fffffc3a6,L0x7fffffc3a8,L0x7fffffc3aa,L0x7fffffc3ac,L0x7fffffc3ae] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc3b0; PC = 0x5555551528 *)
mov [L0x7fffffc3b0,L0x7fffffc3b2,L0x7fffffc3b4,L0x7fffffc3b6,L0x7fffffc3b8,L0x7fffffc3ba,L0x7fffffc3bc,L0x7fffffc3be] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc3c0; PC = 0x5555551530 *)
mov [L0x7fffffc3c0,L0x7fffffc3c2,L0x7fffffc3c4,L0x7fffffc3c6,L0x7fffffc3c8,L0x7fffffc3ca,L0x7fffffc3cc,L0x7fffffc3ce] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc3d0; PC = 0x5555551538 *)
mov [L0x7fffffc3d0,L0x7fffffc3d2,L0x7fffffc3d4,L0x7fffffc3d6,L0x7fffffc3d8,L0x7fffffc3da,L0x7fffffc3dc,L0x7fffffc3de] %v0;

cut (* 4 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc400; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc400,L0x7fffffc402,L0x7fffffc404,L0x7fffffc406,L0x7fffffc408,L0x7fffffc40a,L0x7fffffc40c,L0x7fffffc40e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc410; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc410,L0x7fffffc412,L0x7fffffc414,L0x7fffffc416,L0x7fffffc418,L0x7fffffc41a,L0x7fffffc41c,L0x7fffffc41e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc420; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc420,L0x7fffffc422,L0x7fffffc424,L0x7fffffc426,L0x7fffffc428,L0x7fffffc42a,L0x7fffffc42c,L0x7fffffc42e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc430; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc430,L0x7fffffc432,L0x7fffffc434,L0x7fffffc436,L0x7fffffc438,L0x7fffffc43a,L0x7fffffc43c,L0x7fffffc43e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc440; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc440,L0x7fffffc442,L0x7fffffc444,L0x7fffffc446,L0x7fffffc448,L0x7fffffc44a,L0x7fffffc44c,L0x7fffffc44e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc450; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc450,L0x7fffffc452,L0x7fffffc454,L0x7fffffc456,L0x7fffffc458,L0x7fffffc45a,L0x7fffffc45c,L0x7fffffc45e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffc460; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffc460,L0x7fffffc462,L0x7fffffc464,L0x7fffffc466,L0x7fffffc468,L0x7fffffc46a,L0x7fffffc46c,L0x7fffffc46e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc3e0; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc3e0,L0x7fffffc3e2,L0x7fffffc3e4,L0x7fffffc3e6,L0x7fffffc3e8,L0x7fffffc3ea,L0x7fffffc3ec,L0x7fffffc3ee];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc3f0; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc3f0,L0x7fffffc3f2,L0x7fffffc3f4,L0x7fffffc3f6,L0x7fffffc3f8,L0x7fffffc3fa,L0x7fffffc3fc,L0x7fffffc3fe];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_01@sint16[8], %fa1_01@sint16[8], %fa2_01@sint16[8], %fa3_01@sint16[8], %fa4_01@sint16[8], %fa5_01@sint16[8], %fa6_01@sint16[8], %fa7_01@sint16[8], %fa8_01@sint16[8] :
    %fa0_01 = %v0 /\ %fa1_01 = %v1 /\ %fa2_01 = %v2 /\ %fa3_01 = %v3 /\
    %fa4_01 = %v4 /\ %fa5_01 = %v5 /\ %fa6_01 = %v6 /\ %fa7_01 = %v7 /\
    %fa8_01 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_01 = %v0 /\ %fa1_01 = %v1 /\ %fa2_01 = %v2 /\ %fa3_01 = %v3 /\
    %fa4_01 = %v4 /\ %fa5_01 = %v5 /\ %fa6_01 = %v6 /\ %fa7_01 = %v7 /\
    %fa8_01 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 5 *)
    %fa0_01 = %v0 /\ %fa1_01 = %v1 /\ %fa2_01 = %v2 /\ %fa3_01 = %v3 /\
    %fa4_01 = %v4 /\ %fa5_01 = %v5 /\ %fa6_01 = %v6 /\ %fa7_01 = %v7 /\
    %fa8_01 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_01 = [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] /\
    %fa1_01 = [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] /\
    %fa2_01 = [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] /\
    %fa3_01 = [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] /\
    %fa4_01 = [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] /\
    %fa5_01 = [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] /\
    %fa6_01 = [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] /\
    %fa7_01 = [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] /\
    %fa8_01 = [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_01 = %v0 /\ %fa1_01 = %v1 /\ %fa2_01 = %v2 /\ %fa3_01 = %v3 /\
    %fa4_01 = %v4 /\ %fa5_01 = %v5 /\ %fa6_01 = %v6 /\ %fa7_01 = %v7 /\
    %fa8_01 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_01 = [arr0100, arr0101, arr0102, arr0103, arr0104, arr0105, arr0106, arr0107] /\
    %fa1_01 = [arr0180, arr0181, arr0182, arr0183, arr0184, arr0185, arr0186, arr0187] /\
    %fa2_01 = [arr0170, arr0171, arr0172, arr0173, arr0174, arr0175, arr0176, arr0177] /\
    %fa3_01 = [arr0160, arr0161, arr0162, arr0163, arr0164, arr0165, arr0166, arr0167] /\
    %fa4_01 = [arr0150, arr0151, arr0152, arr0153, arr0154, arr0155, arr0156, arr0157] /\
    %fa5_01 = [arr0140, arr0141, arr0142, arr0143, arr0144, arr0145, arr0146, arr0147] /\
    %fa6_01 = [arr0130, arr0131, arr0132, arr0133, arr0134, arr0135, arr0136, arr0137] /\
    %fa7_01 = [arr0120, arr0121, arr0122, arr0123, arr0124, arr0125, arr0126, arr0127] /\
    %fa8_01 = [arr0110, arr0111, arr0112, arr0113, arr0114, arr0115, arr0116, arr0117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_01 + %fa3_01 + %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_01 + %fa3_01 + %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_01 + %fa3_01 + %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_01 + %fa3_01 + %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_01 - %fa3_01 - %fa6_01
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_01 - %fa6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_01 - %fa4_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_01 - %fa4_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_01 - %fa4_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_01 - %fa4_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_01 - %fa5_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_01 - %fa5_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_01 - %fa5_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_01 - %fa5_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 6 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        %fa3_01 +
        %fa6_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_01 +
        %fa7_01 +
        %fa4_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_01 +
        %fa2_01 +
        %fa5_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_01 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_01 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 5], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 5]];


ghost %fb0_01@sint16[8], %fb1_01@sint16[8], %fb2_01@sint16[8], %fb3_01@sint16[8], %fb4_01@sint16[8], %fb5_01@sint16[8], %fb6_01@sint16[8], %fb7_01@sint16[8], %fb8_01@sint16[8] :
    %fb0_01 = %v17 /\ %fb1_01 = %v16 /\ %fb2_01 = %v0  /\ %fb3_01 = %v6  /\
    %fb4_01 = %v3  /\ %fb5_01 = %v7  /\ %fb6_01 = %v19 /\ %fb7_01 = %v1  /\ %fb8_01 = %v2
  &&
    %fb0_01 = %v17 /\ %fb1_01 = %v16 /\ %fb2_01 = %v0  /\ %fb3_01 = %v6  /\
    %fb4_01 = %v3  /\ %fb5_01 = %v7  /\ %fb6_01 = %v19 /\ %fb7_01 = %v1  /\ %fb8_01 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_01 + %fb6_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_01 + %fb6_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_01 + %fb6_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_01 + %fb6_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_01 - %fb6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_01 - %fb6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_01 - %fb6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_01 - %fb6_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_01@sint16[8] : %b1i1_01 = %v1 && %b1i1_01 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_01@sint16[8] : %b1i2_01 = %v5 && %b1i2_01 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_01 + %b1i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_01 + %b1i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_01 + %b1i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_01 + %b1i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_01 - %b1i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_01 - %b1i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_01 - %b1i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_01 - %b1i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_01@sint16[8] : %b2i1_01 = %v3 && %b2i1_01 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_01@sint16[8] : %b2i2_01 = %v1 && %b2i2_01 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_01 + %b2i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_01 + %b2i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_01 + %b2i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_01 + %b2i2_01
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_01 - %b2i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_01 - %b2i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_01 - %b2i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_01 - %b2i2_01)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 7 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        %fa1_01 +
        %fa2_01 +
        %fa3_01 +
        %fa4_01 +
        %fa5_01 +
        %fa6_01 +
        %fa7_01 +
        %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_01 +
        %fa3_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_01 +
        %fa6_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_01 +
        %fa3_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_01 +
        %fa6_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_01 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_01 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_01 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_01 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_01 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_01 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_01 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_01 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_01
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 5]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 5], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 5]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc3e0; PC = 0x5555551590 *)
mov [L0x7fffffc3e0,L0x7fffffc3e2,L0x7fffffc3e4,L0x7fffffc3e6,L0x7fffffc3e8,L0x7fffffc3ea,L0x7fffffc3ec,L0x7fffffc3ee] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc3f0; PC = 0x5555551598 *)
mov [L0x7fffffc3f0,L0x7fffffc3f2,L0x7fffffc3f4,L0x7fffffc3f6,L0x7fffffc3f8,L0x7fffffc3fa,L0x7fffffc3fc,L0x7fffffc3fe] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc400; PC = 0x55555515a0 *)
mov [L0x7fffffc400,L0x7fffffc402,L0x7fffffc404,L0x7fffffc406,L0x7fffffc408,L0x7fffffc40a,L0x7fffffc40c,L0x7fffffc40e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc410; PC = 0x55555515a8 *)
mov [L0x7fffffc410,L0x7fffffc412,L0x7fffffc414,L0x7fffffc416,L0x7fffffc418,L0x7fffffc41a,L0x7fffffc41c,L0x7fffffc41e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc420; PC = 0x55555515b0 *)
mov [L0x7fffffc420,L0x7fffffc422,L0x7fffffc424,L0x7fffffc426,L0x7fffffc428,L0x7fffffc42a,L0x7fffffc42c,L0x7fffffc42e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc430; PC = 0x55555515b8 *)
mov [L0x7fffffc430,L0x7fffffc432,L0x7fffffc434,L0x7fffffc436,L0x7fffffc438,L0x7fffffc43a,L0x7fffffc43c,L0x7fffffc43e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc440; PC = 0x55555515c0 *)
mov [L0x7fffffc440,L0x7fffffc442,L0x7fffffc444,L0x7fffffc446,L0x7fffffc448,L0x7fffffc44a,L0x7fffffc44c,L0x7fffffc44e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc450; PC = 0x55555515c8 *)
mov [L0x7fffffc450,L0x7fffffc452,L0x7fffffc454,L0x7fffffc456,L0x7fffffc458,L0x7fffffc45a,L0x7fffffc45c,L0x7fffffc45e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffc460; PC = 0x55555515d0 *)
mov [L0x7fffffc460,L0x7fffffc462,L0x7fffffc464,L0x7fffffc466,L0x7fffffc468,L0x7fffffc46a,L0x7fffffc46c,L0x7fffffc46e] %v0;

cut (* 8 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc490; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc490,L0x7fffffc492,L0x7fffffc494,L0x7fffffc496,L0x7fffffc498,L0x7fffffc49a,L0x7fffffc49c,L0x7fffffc49e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc4a0; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc4a0,L0x7fffffc4a2,L0x7fffffc4a4,L0x7fffffc4a6,L0x7fffffc4a8,L0x7fffffc4aa,L0x7fffffc4ac,L0x7fffffc4ae];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc4b0; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc4b0,L0x7fffffc4b2,L0x7fffffc4b4,L0x7fffffc4b6,L0x7fffffc4b8,L0x7fffffc4ba,L0x7fffffc4bc,L0x7fffffc4be];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc4c0; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc4c0,L0x7fffffc4c2,L0x7fffffc4c4,L0x7fffffc4c6,L0x7fffffc4c8,L0x7fffffc4ca,L0x7fffffc4cc,L0x7fffffc4ce];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc4d0; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc4d0,L0x7fffffc4d2,L0x7fffffc4d4,L0x7fffffc4d6,L0x7fffffc4d8,L0x7fffffc4da,L0x7fffffc4dc,L0x7fffffc4de];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc4e0; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc4e0,L0x7fffffc4e2,L0x7fffffc4e4,L0x7fffffc4e6,L0x7fffffc4e8,L0x7fffffc4ea,L0x7fffffc4ec,L0x7fffffc4ee];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc4f0; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc4f0,L0x7fffffc4f2,L0x7fffffc4f4,L0x7fffffc4f6,L0x7fffffc4f8,L0x7fffffc4fa,L0x7fffffc4fc,L0x7fffffc4fe];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc480; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc480,L0x7fffffc482,L0x7fffffc484,L0x7fffffc486,L0x7fffffc488,L0x7fffffc48a,L0x7fffffc48c,L0x7fffffc48e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc470; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc470,L0x7fffffc472,L0x7fffffc474,L0x7fffffc476,L0x7fffffc478,L0x7fffffc47a,L0x7fffffc47c,L0x7fffffc47e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_10@sint16[8], %fa1_10@sint16[8], %fa2_10@sint16[8], %fa3_10@sint16[8], %fa4_10@sint16[8], %fa5_10@sint16[8], %fa6_10@sint16[8], %fa7_10@sint16[8], %fa8_10@sint16[8] :
    %fa0_10 = %v0 /\ %fa1_10 = %v1 /\ %fa2_10 = %v2 /\ %fa3_10 = %v3 /\
    %fa4_10 = %v4 /\ %fa5_10 = %v5 /\ %fa6_10 = %v6 /\ %fa7_10 = %v7 /\
    %fa8_10 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_10 = %v0 /\ %fa1_10 = %v1 /\ %fa2_10 = %v2 /\ %fa3_10 = %v3 /\
    %fa4_10 = %v4 /\ %fa5_10 = %v5 /\ %fa6_10 = %v6 /\ %fa7_10 = %v7 /\
    %fa8_10 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 9 *)
    %fa0_10 = %v0 /\ %fa1_10 = %v1 /\ %fa2_10 = %v2 /\ %fa3_10 = %v3 /\
    %fa4_10 = %v4 /\ %fa5_10 = %v5 /\ %fa6_10 = %v6 /\ %fa7_10 = %v7 /\
    %fa8_10 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_10 = [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] /\
    %fa1_10 = [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] /\
    %fa2_10 = [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] /\
    %fa3_10 = [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] /\
    %fa4_10 = [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] /\
    %fa5_10 = [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] /\
    %fa6_10 = [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] /\
    %fa7_10 = [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] /\
    %fa8_10 = [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_10 = %v0 /\ %fa1_10 = %v1 /\ %fa2_10 = %v2 /\ %fa3_10 = %v3 /\
    %fa4_10 = %v4 /\ %fa5_10 = %v5 /\ %fa6_10 = %v6 /\ %fa7_10 = %v7 /\
    %fa8_10 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_10 = [arr1000, arr1001, arr1002, arr1003, arr1004, arr1005, arr1006, arr1007] /\
    %fa1_10 = [arr1080, arr1081, arr1082, arr1083, arr1084, arr1085, arr1086, arr1087] /\
    %fa2_10 = [arr1070, arr1071, arr1072, arr1073, arr1074, arr1075, arr1076, arr1077] /\
    %fa3_10 = [arr1060, arr1061, arr1062, arr1063, arr1064, arr1065, arr1066, arr1067] /\
    %fa4_10 = [arr1050, arr1051, arr1052, arr1053, arr1054, arr1055, arr1056, arr1057] /\
    %fa5_10 = [arr1040, arr1041, arr1042, arr1043, arr1044, arr1045, arr1046, arr1047] /\
    %fa6_10 = [arr1030, arr1031, arr1032, arr1033, arr1034, arr1035, arr1036, arr1037] /\
    %fa7_10 = [arr1020, arr1021, arr1022, arr1023, arr1024, arr1025, arr1026, arr1027] /\
    %fa8_10 = [arr1010, arr1011, arr1012, arr1013, arr1014, arr1015, arr1016, arr1017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_10 + %fa3_10 + %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_10 + %fa3_10 + %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_10 + %fa3_10 + %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_10 + %fa3_10 + %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_10 - %fa3_10 - %fa6_10
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_10 - %fa6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_10 - %fa4_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_10 - %fa4_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_10 - %fa4_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_10 - %fa4_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_10 - %fa5_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_10 - %fa5_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_10 - %fa5_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_10 - %fa5_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 10 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        %fa3_10 +
        %fa6_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_10 +
        %fa7_10 +
        %fa4_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_10 +
        %fa2_10 +
        %fa5_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_10 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_10 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 9], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 9]];


ghost %fb0_10@sint16[8], %fb1_10@sint16[8], %fb2_10@sint16[8], %fb3_10@sint16[8], %fb4_10@sint16[8], %fb5_10@sint16[8], %fb6_10@sint16[8], %fb7_10@sint16[8], %fb8_10@sint16[8] :
    %fb0_10 = %v17 /\ %fb1_10 = %v16 /\ %fb2_10 = %v0  /\ %fb3_10 = %v6  /\
    %fb4_10 = %v3  /\ %fb5_10 = %v7  /\ %fb6_10 = %v19 /\ %fb7_10 = %v1  /\ %fb8_10 = %v2
  &&
    %fb0_10 = %v17 /\ %fb1_10 = %v16 /\ %fb2_10 = %v0  /\ %fb3_10 = %v6  /\
    %fb4_10 = %v3  /\ %fb5_10 = %v7  /\ %fb6_10 = %v19 /\ %fb7_10 = %v1  /\ %fb8_10 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_10 + %fb6_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_10 + %fb6_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_10 + %fb6_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_10 + %fb6_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_10 - %fb6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_10 - %fb6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_10 - %fb6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_10 - %fb6_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_10@sint16[8] : %b1i1_10 = %v1 && %b1i1_10 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_10@sint16[8] : %b1i2_10 = %v5 && %b1i2_10 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_10 + %b1i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_10 + %b1i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_10 + %b1i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_10 + %b1i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_10 - %b1i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_10 - %b1i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_10 - %b1i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_10 - %b1i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_10@sint16[8] : %b2i1_10 = %v3 && %b2i1_10 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_10@sint16[8] : %b2i2_10 = %v1 && %b2i2_10 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_10 + %b2i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_10 + %b2i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_10 + %b2i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_10 + %b2i2_10
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_10 - %b2i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_10 - %b2i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_10 - %b2i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_10 - %b2i2_10)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 11 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        %fa1_10 +
        %fa2_10 +
        %fa3_10 +
        %fa4_10 +
        %fa5_10 +
        %fa6_10 +
        %fa7_10 +
        %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_10 +
        %fa3_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_10 +
        %fa6_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_10 +
        %fa3_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_10 +
        %fa6_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_10 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_10 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_10 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_10 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_10 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_10 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_10 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_10 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_10
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 9]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 9], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 9]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc470; PC = 0x55555514f8 *)
mov [L0x7fffffc470,L0x7fffffc472,L0x7fffffc474,L0x7fffffc476,L0x7fffffc478,L0x7fffffc47a,L0x7fffffc47c,L0x7fffffc47e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc480; PC = 0x5555551500 *)
mov [L0x7fffffc480,L0x7fffffc482,L0x7fffffc484,L0x7fffffc486,L0x7fffffc488,L0x7fffffc48a,L0x7fffffc48c,L0x7fffffc48e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc490; PC = 0x5555551508 *)
mov [L0x7fffffc490,L0x7fffffc492,L0x7fffffc494,L0x7fffffc496,L0x7fffffc498,L0x7fffffc49a,L0x7fffffc49c,L0x7fffffc49e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc4a0; PC = 0x5555551510 *)
mov [L0x7fffffc4a0,L0x7fffffc4a2,L0x7fffffc4a4,L0x7fffffc4a6,L0x7fffffc4a8,L0x7fffffc4aa,L0x7fffffc4ac,L0x7fffffc4ae] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc4b0; PC = 0x5555551518 *)
mov [L0x7fffffc4b0,L0x7fffffc4b2,L0x7fffffc4b4,L0x7fffffc4b6,L0x7fffffc4b8,L0x7fffffc4ba,L0x7fffffc4bc,L0x7fffffc4be] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc4c0; PC = 0x5555551520 *)
mov [L0x7fffffc4c0,L0x7fffffc4c2,L0x7fffffc4c4,L0x7fffffc4c6,L0x7fffffc4c8,L0x7fffffc4ca,L0x7fffffc4cc,L0x7fffffc4ce] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc4d0; PC = 0x5555551528 *)
mov [L0x7fffffc4d0,L0x7fffffc4d2,L0x7fffffc4d4,L0x7fffffc4d6,L0x7fffffc4d8,L0x7fffffc4da,L0x7fffffc4dc,L0x7fffffc4de] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc4e0; PC = 0x5555551530 *)
mov [L0x7fffffc4e0,L0x7fffffc4e2,L0x7fffffc4e4,L0x7fffffc4e6,L0x7fffffc4e8,L0x7fffffc4ea,L0x7fffffc4ec,L0x7fffffc4ee] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc4f0; PC = 0x5555551538 *)
mov [L0x7fffffc4f0,L0x7fffffc4f2,L0x7fffffc4f4,L0x7fffffc4f6,L0x7fffffc4f8,L0x7fffffc4fa,L0x7fffffc4fc,L0x7fffffc4fe] %v0;

cut (* 12 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc520; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc520,L0x7fffffc522,L0x7fffffc524,L0x7fffffc526,L0x7fffffc528,L0x7fffffc52a,L0x7fffffc52c,L0x7fffffc52e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc530; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc530,L0x7fffffc532,L0x7fffffc534,L0x7fffffc536,L0x7fffffc538,L0x7fffffc53a,L0x7fffffc53c,L0x7fffffc53e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc540; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc540,L0x7fffffc542,L0x7fffffc544,L0x7fffffc546,L0x7fffffc548,L0x7fffffc54a,L0x7fffffc54c,L0x7fffffc54e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc550; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc550,L0x7fffffc552,L0x7fffffc554,L0x7fffffc556,L0x7fffffc558,L0x7fffffc55a,L0x7fffffc55c,L0x7fffffc55e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc560; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc560,L0x7fffffc562,L0x7fffffc564,L0x7fffffc566,L0x7fffffc568,L0x7fffffc56a,L0x7fffffc56c,L0x7fffffc56e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc570; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc570,L0x7fffffc572,L0x7fffffc574,L0x7fffffc576,L0x7fffffc578,L0x7fffffc57a,L0x7fffffc57c,L0x7fffffc57e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffc580; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffc580,L0x7fffffc582,L0x7fffffc584,L0x7fffffc586,L0x7fffffc588,L0x7fffffc58a,L0x7fffffc58c,L0x7fffffc58e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc500; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc500,L0x7fffffc502,L0x7fffffc504,L0x7fffffc506,L0x7fffffc508,L0x7fffffc50a,L0x7fffffc50c,L0x7fffffc50e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc510; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc510,L0x7fffffc512,L0x7fffffc514,L0x7fffffc516,L0x7fffffc518,L0x7fffffc51a,L0x7fffffc51c,L0x7fffffc51e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_11@sint16[8], %fa1_11@sint16[8], %fa2_11@sint16[8], %fa3_11@sint16[8], %fa4_11@sint16[8], %fa5_11@sint16[8], %fa6_11@sint16[8], %fa7_11@sint16[8], %fa8_11@sint16[8] :
    %fa0_11 = %v0 /\ %fa1_11 = %v1 /\ %fa2_11 = %v2 /\ %fa3_11 = %v3 /\
    %fa4_11 = %v4 /\ %fa5_11 = %v5 /\ %fa6_11 = %v6 /\ %fa7_11 = %v7 /\
    %fa8_11 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_11 = %v0 /\ %fa1_11 = %v1 /\ %fa2_11 = %v2 /\ %fa3_11 = %v3 /\
    %fa4_11 = %v4 /\ %fa5_11 = %v5 /\ %fa6_11 = %v6 /\ %fa7_11 = %v7 /\
    %fa8_11 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 13 *)
    %fa0_11 = %v0 /\ %fa1_11 = %v1 /\ %fa2_11 = %v2 /\ %fa3_11 = %v3 /\
    %fa4_11 = %v4 /\ %fa5_11 = %v5 /\ %fa6_11 = %v6 /\ %fa7_11 = %v7 /\
    %fa8_11 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_11 = [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] /\
    %fa1_11 = [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] /\
    %fa2_11 = [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] /\
    %fa3_11 = [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] /\
    %fa4_11 = [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] /\
    %fa5_11 = [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] /\
    %fa6_11 = [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] /\
    %fa7_11 = [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] /\
    %fa8_11 = [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_11 = %v0 /\ %fa1_11 = %v1 /\ %fa2_11 = %v2 /\ %fa3_11 = %v3 /\
    %fa4_11 = %v4 /\ %fa5_11 = %v5 /\ %fa6_11 = %v6 /\ %fa7_11 = %v7 /\
    %fa8_11 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_11 = [arr1100, arr1101, arr1102, arr1103, arr1104, arr1105, arr1106, arr1107] /\
    %fa1_11 = [arr1180, arr1181, arr1182, arr1183, arr1184, arr1185, arr1186, arr1187] /\
    %fa2_11 = [arr1170, arr1171, arr1172, arr1173, arr1174, arr1175, arr1176, arr1177] /\
    %fa3_11 = [arr1160, arr1161, arr1162, arr1163, arr1164, arr1165, arr1166, arr1167] /\
    %fa4_11 = [arr1150, arr1151, arr1152, arr1153, arr1154, arr1155, arr1156, arr1157] /\
    %fa5_11 = [arr1140, arr1141, arr1142, arr1143, arr1144, arr1145, arr1146, arr1147] /\
    %fa6_11 = [arr1130, arr1131, arr1132, arr1133, arr1134, arr1135, arr1136, arr1137] /\
    %fa7_11 = [arr1120, arr1121, arr1122, arr1123, arr1124, arr1125, arr1126, arr1127] /\
    %fa8_11 = [arr1110, arr1111, arr1112, arr1113, arr1114, arr1115, arr1116, arr1117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_11 + %fa3_11 + %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_11 + %fa3_11 + %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_11 + %fa3_11 + %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_11 + %fa3_11 + %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_11 - %fa3_11 - %fa6_11
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_11 - %fa6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_11 - %fa4_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_11 - %fa4_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_11 - %fa4_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_11 - %fa4_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_11 - %fa5_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_11 - %fa5_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_11 - %fa5_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_11 - %fa5_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 14 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        %fa3_11 +
        %fa6_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_11 +
        %fa7_11 +
        %fa4_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_11 +
        %fa2_11 +
        %fa5_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_11 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_11 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 13], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 13]];


ghost %fb0_11@sint16[8], %fb1_11@sint16[8], %fb2_11@sint16[8], %fb3_11@sint16[8], %fb4_11@sint16[8], %fb5_11@sint16[8], %fb6_11@sint16[8], %fb7_11@sint16[8], %fb8_11@sint16[8] :
    %fb0_11 = %v17 /\ %fb1_11 = %v16 /\ %fb2_11 = %v0  /\ %fb3_11 = %v6  /\
    %fb4_11 = %v3  /\ %fb5_11 = %v7  /\ %fb6_11 = %v19 /\ %fb7_11 = %v1  /\ %fb8_11 = %v2
  &&
    %fb0_11 = %v17 /\ %fb1_11 = %v16 /\ %fb2_11 = %v0  /\ %fb3_11 = %v6  /\
    %fb4_11 = %v3  /\ %fb5_11 = %v7  /\ %fb6_11 = %v19 /\ %fb7_11 = %v1  /\ %fb8_11 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_11 + %fb6_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_11 + %fb6_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_11 + %fb6_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_11 + %fb6_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_11 - %fb6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_11 - %fb6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_11 - %fb6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_11 - %fb6_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_11@sint16[8] : %b1i1_11 = %v1 && %b1i1_11 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_11@sint16[8] : %b1i2_11 = %v5 && %b1i2_11 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_11 + %b1i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_11 + %b1i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_11 + %b1i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_11 + %b1i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_11 - %b1i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_11 - %b1i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_11 - %b1i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_11 - %b1i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_11@sint16[8] : %b2i1_11 = %v3 && %b2i1_11 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_11@sint16[8] : %b2i2_11 = %v1 && %b2i2_11 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_11 + %b2i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_11 + %b2i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_11 + %b2i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_11 + %b2i2_11
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_11 - %b2i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_11 - %b2i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_11 - %b2i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_11 - %b2i2_11)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 15 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        %fa1_11 +
        %fa2_11 +
        %fa3_11 +
        %fa4_11 +
        %fa5_11 +
        %fa6_11 +
        %fa7_11 +
        %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_11 +
        %fa3_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_11 +
        %fa6_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_11 +
        %fa3_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_11 +
        %fa6_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_11 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_11 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_11 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_11 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_11 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_11 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_11 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_11 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_11
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 13]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 13], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 13]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc500; PC = 0x5555551590 *)
mov [L0x7fffffc500,L0x7fffffc502,L0x7fffffc504,L0x7fffffc506,L0x7fffffc508,L0x7fffffc50a,L0x7fffffc50c,L0x7fffffc50e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc510; PC = 0x5555551598 *)
mov [L0x7fffffc510,L0x7fffffc512,L0x7fffffc514,L0x7fffffc516,L0x7fffffc518,L0x7fffffc51a,L0x7fffffc51c,L0x7fffffc51e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc520; PC = 0x55555515a0 *)
mov [L0x7fffffc520,L0x7fffffc522,L0x7fffffc524,L0x7fffffc526,L0x7fffffc528,L0x7fffffc52a,L0x7fffffc52c,L0x7fffffc52e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc530; PC = 0x55555515a8 *)
mov [L0x7fffffc530,L0x7fffffc532,L0x7fffffc534,L0x7fffffc536,L0x7fffffc538,L0x7fffffc53a,L0x7fffffc53c,L0x7fffffc53e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc540; PC = 0x55555515b0 *)
mov [L0x7fffffc540,L0x7fffffc542,L0x7fffffc544,L0x7fffffc546,L0x7fffffc548,L0x7fffffc54a,L0x7fffffc54c,L0x7fffffc54e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc550; PC = 0x55555515b8 *)
mov [L0x7fffffc550,L0x7fffffc552,L0x7fffffc554,L0x7fffffc556,L0x7fffffc558,L0x7fffffc55a,L0x7fffffc55c,L0x7fffffc55e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc560; PC = 0x55555515c0 *)
mov [L0x7fffffc560,L0x7fffffc562,L0x7fffffc564,L0x7fffffc566,L0x7fffffc568,L0x7fffffc56a,L0x7fffffc56c,L0x7fffffc56e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc570; PC = 0x55555515c8 *)
mov [L0x7fffffc570,L0x7fffffc572,L0x7fffffc574,L0x7fffffc576,L0x7fffffc578,L0x7fffffc57a,L0x7fffffc57c,L0x7fffffc57e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffc580; PC = 0x55555515d0 *)
mov [L0x7fffffc580,L0x7fffffc582,L0x7fffffc584,L0x7fffffc586,L0x7fffffc588,L0x7fffffc58a,L0x7fffffc58c,L0x7fffffc58e] %v0;

cut (* 16 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc5b0; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc5b0,L0x7fffffc5b2,L0x7fffffc5b4,L0x7fffffc5b6,L0x7fffffc5b8,L0x7fffffc5ba,L0x7fffffc5bc,L0x7fffffc5be];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc5c0; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc5c0,L0x7fffffc5c2,L0x7fffffc5c4,L0x7fffffc5c6,L0x7fffffc5c8,L0x7fffffc5ca,L0x7fffffc5cc,L0x7fffffc5ce];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc5d0; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc5d0,L0x7fffffc5d2,L0x7fffffc5d4,L0x7fffffc5d6,L0x7fffffc5d8,L0x7fffffc5da,L0x7fffffc5dc,L0x7fffffc5de];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc5e0; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc5e0,L0x7fffffc5e2,L0x7fffffc5e4,L0x7fffffc5e6,L0x7fffffc5e8,L0x7fffffc5ea,L0x7fffffc5ec,L0x7fffffc5ee];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc5f0; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc5f0,L0x7fffffc5f2,L0x7fffffc5f4,L0x7fffffc5f6,L0x7fffffc5f8,L0x7fffffc5fa,L0x7fffffc5fc,L0x7fffffc5fe];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc600; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc600,L0x7fffffc602,L0x7fffffc604,L0x7fffffc606,L0x7fffffc608,L0x7fffffc60a,L0x7fffffc60c,L0x7fffffc60e];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc610; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc610,L0x7fffffc612,L0x7fffffc614,L0x7fffffc616,L0x7fffffc618,L0x7fffffc61a,L0x7fffffc61c,L0x7fffffc61e];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc5a0; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc5a0,L0x7fffffc5a2,L0x7fffffc5a4,L0x7fffffc5a6,L0x7fffffc5a8,L0x7fffffc5aa,L0x7fffffc5ac,L0x7fffffc5ae];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc590; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc590,L0x7fffffc592,L0x7fffffc594,L0x7fffffc596,L0x7fffffc598,L0x7fffffc59a,L0x7fffffc59c,L0x7fffffc59e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_20@sint16[8], %fa1_20@sint16[8], %fa2_20@sint16[8], %fa3_20@sint16[8], %fa4_20@sint16[8], %fa5_20@sint16[8], %fa6_20@sint16[8], %fa7_20@sint16[8], %fa8_20@sint16[8] :
    %fa0_20 = %v0 /\ %fa1_20 = %v1 /\ %fa2_20 = %v2 /\ %fa3_20 = %v3 /\
    %fa4_20 = %v4 /\ %fa5_20 = %v5 /\ %fa6_20 = %v6 /\ %fa7_20 = %v7 /\
    %fa8_20 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_20 = %v0 /\ %fa1_20 = %v1 /\ %fa2_20 = %v2 /\ %fa3_20 = %v3 /\
    %fa4_20 = %v4 /\ %fa5_20 = %v5 /\ %fa6_20 = %v6 /\ %fa7_20 = %v7 /\
    %fa8_20 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 17 *)
    %fa0_20 = %v0 /\ %fa1_20 = %v1 /\ %fa2_20 = %v2 /\ %fa3_20 = %v3 /\
    %fa4_20 = %v4 /\ %fa5_20 = %v5 /\ %fa6_20 = %v6 /\ %fa7_20 = %v7 /\
    %fa8_20 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_20 = [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] /\
    %fa1_20 = [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] /\
    %fa2_20 = [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] /\
    %fa3_20 = [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] /\
    %fa4_20 = [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] /\
    %fa5_20 = [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] /\
    %fa6_20 = [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] /\
    %fa7_20 = [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] /\
    %fa8_20 = [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_20 = %v0 /\ %fa1_20 = %v1 /\ %fa2_20 = %v2 /\ %fa3_20 = %v3 /\
    %fa4_20 = %v4 /\ %fa5_20 = %v5 /\ %fa6_20 = %v6 /\ %fa7_20 = %v7 /\
    %fa8_20 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_20 = [arr2000, arr2001, arr2002, arr2003, arr2004, arr2005, arr2006, arr2007] /\
    %fa1_20 = [arr2080, arr2081, arr2082, arr2083, arr2084, arr2085, arr2086, arr2087] /\
    %fa2_20 = [arr2070, arr2071, arr2072, arr2073, arr2074, arr2075, arr2076, arr2077] /\
    %fa3_20 = [arr2060, arr2061, arr2062, arr2063, arr2064, arr2065, arr2066, arr2067] /\
    %fa4_20 = [arr2050, arr2051, arr2052, arr2053, arr2054, arr2055, arr2056, arr2057] /\
    %fa5_20 = [arr2040, arr2041, arr2042, arr2043, arr2044, arr2045, arr2046, arr2047] /\
    %fa6_20 = [arr2030, arr2031, arr2032, arr2033, arr2034, arr2035, arr2036, arr2037] /\
    %fa7_20 = [arr2020, arr2021, arr2022, arr2023, arr2024, arr2025, arr2026, arr2027] /\
    %fa8_20 = [arr2010, arr2011, arr2012, arr2013, arr2014, arr2015, arr2016, arr2017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_20 + %fa3_20 + %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_20 + %fa3_20 + %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_20 + %fa3_20 + %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_20 + %fa3_20 + %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_20 - %fa3_20 - %fa6_20
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_20 - %fa6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_20 - %fa4_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_20 - %fa4_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_20 - %fa4_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_20 - %fa4_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_20 - %fa5_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_20 - %fa5_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_20 - %fa5_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_20 - %fa5_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 18 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        %fa3_20 +
        %fa6_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_20 +
        %fa7_20 +
        %fa4_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_20 +
        %fa2_20 +
        %fa5_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_20 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_20 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 17], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 17]];


ghost %fb0_20@sint16[8], %fb1_20@sint16[8], %fb2_20@sint16[8], %fb3_20@sint16[8], %fb4_20@sint16[8], %fb5_20@sint16[8], %fb6_20@sint16[8], %fb7_20@sint16[8], %fb8_20@sint16[8] :
    %fb0_20 = %v17 /\ %fb1_20 = %v16 /\ %fb2_20 = %v0  /\ %fb3_20 = %v6  /\
    %fb4_20 = %v3  /\ %fb5_20 = %v7  /\ %fb6_20 = %v19 /\ %fb7_20 = %v1  /\ %fb8_20 = %v2
  &&
    %fb0_20 = %v17 /\ %fb1_20 = %v16 /\ %fb2_20 = %v0  /\ %fb3_20 = %v6  /\
    %fb4_20 = %v3  /\ %fb5_20 = %v7  /\ %fb6_20 = %v19 /\ %fb7_20 = %v1  /\ %fb8_20 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_20 + %fb6_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_20 + %fb6_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_20 + %fb6_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_20 + %fb6_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_20 - %fb6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_20 - %fb6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_20 - %fb6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_20 - %fb6_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_20@sint16[8] : %b1i1_20 = %v1 && %b1i1_20 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_20@sint16[8] : %b1i2_20 = %v5 && %b1i2_20 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_20 + %b1i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_20 + %b1i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_20 + %b1i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_20 + %b1i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_20 - %b1i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_20 - %b1i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_20 - %b1i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_20 - %b1i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_20@sint16[8] : %b2i1_20 = %v3 && %b2i1_20 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_20@sint16[8] : %b2i2_20 = %v1 && %b2i2_20 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_20 + %b2i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_20 + %b2i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_20 + %b2i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_20 + %b2i2_20
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_20 - %b2i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_20 - %b2i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_20 - %b2i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_20 - %b2i2_20)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 19 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        %fa1_20 +
        %fa2_20 +
        %fa3_20 +
        %fa4_20 +
        %fa5_20 +
        %fa6_20 +
        %fa7_20 +
        %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_20 +
        %fa3_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_20 +
        %fa6_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_20 +
        %fa3_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_20 +
        %fa6_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_20 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_20 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_20 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_20 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_20 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_20 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_20 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_20 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_20
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 17]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 17], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 17]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc590; PC = 0x55555514f8 *)
mov [L0x7fffffc590,L0x7fffffc592,L0x7fffffc594,L0x7fffffc596,L0x7fffffc598,L0x7fffffc59a,L0x7fffffc59c,L0x7fffffc59e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc5a0; PC = 0x5555551500 *)
mov [L0x7fffffc5a0,L0x7fffffc5a2,L0x7fffffc5a4,L0x7fffffc5a6,L0x7fffffc5a8,L0x7fffffc5aa,L0x7fffffc5ac,L0x7fffffc5ae] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc5b0; PC = 0x5555551508 *)
mov [L0x7fffffc5b0,L0x7fffffc5b2,L0x7fffffc5b4,L0x7fffffc5b6,L0x7fffffc5b8,L0x7fffffc5ba,L0x7fffffc5bc,L0x7fffffc5be] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc5c0; PC = 0x5555551510 *)
mov [L0x7fffffc5c0,L0x7fffffc5c2,L0x7fffffc5c4,L0x7fffffc5c6,L0x7fffffc5c8,L0x7fffffc5ca,L0x7fffffc5cc,L0x7fffffc5ce] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc5d0; PC = 0x5555551518 *)
mov [L0x7fffffc5d0,L0x7fffffc5d2,L0x7fffffc5d4,L0x7fffffc5d6,L0x7fffffc5d8,L0x7fffffc5da,L0x7fffffc5dc,L0x7fffffc5de] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc5e0; PC = 0x5555551520 *)
mov [L0x7fffffc5e0,L0x7fffffc5e2,L0x7fffffc5e4,L0x7fffffc5e6,L0x7fffffc5e8,L0x7fffffc5ea,L0x7fffffc5ec,L0x7fffffc5ee] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc5f0; PC = 0x5555551528 *)
mov [L0x7fffffc5f0,L0x7fffffc5f2,L0x7fffffc5f4,L0x7fffffc5f6,L0x7fffffc5f8,L0x7fffffc5fa,L0x7fffffc5fc,L0x7fffffc5fe] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc600; PC = 0x5555551530 *)
mov [L0x7fffffc600,L0x7fffffc602,L0x7fffffc604,L0x7fffffc606,L0x7fffffc608,L0x7fffffc60a,L0x7fffffc60c,L0x7fffffc60e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc610; PC = 0x5555551538 *)
mov [L0x7fffffc610,L0x7fffffc612,L0x7fffffc614,L0x7fffffc616,L0x7fffffc618,L0x7fffffc61a,L0x7fffffc61c,L0x7fffffc61e] %v0;

cut (* 20 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc640; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc640,L0x7fffffc642,L0x7fffffc644,L0x7fffffc646,L0x7fffffc648,L0x7fffffc64a,L0x7fffffc64c,L0x7fffffc64e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc650; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc650,L0x7fffffc652,L0x7fffffc654,L0x7fffffc656,L0x7fffffc658,L0x7fffffc65a,L0x7fffffc65c,L0x7fffffc65e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc660; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc660,L0x7fffffc662,L0x7fffffc664,L0x7fffffc666,L0x7fffffc668,L0x7fffffc66a,L0x7fffffc66c,L0x7fffffc66e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc670; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc670,L0x7fffffc672,L0x7fffffc674,L0x7fffffc676,L0x7fffffc678,L0x7fffffc67a,L0x7fffffc67c,L0x7fffffc67e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc680; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc680,L0x7fffffc682,L0x7fffffc684,L0x7fffffc686,L0x7fffffc688,L0x7fffffc68a,L0x7fffffc68c,L0x7fffffc68e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc690; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc690,L0x7fffffc692,L0x7fffffc694,L0x7fffffc696,L0x7fffffc698,L0x7fffffc69a,L0x7fffffc69c,L0x7fffffc69e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffc6a0; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffc6a0,L0x7fffffc6a2,L0x7fffffc6a4,L0x7fffffc6a6,L0x7fffffc6a8,L0x7fffffc6aa,L0x7fffffc6ac,L0x7fffffc6ae];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc620; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc620,L0x7fffffc622,L0x7fffffc624,L0x7fffffc626,L0x7fffffc628,L0x7fffffc62a,L0x7fffffc62c,L0x7fffffc62e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc630; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc630,L0x7fffffc632,L0x7fffffc634,L0x7fffffc636,L0x7fffffc638,L0x7fffffc63a,L0x7fffffc63c,L0x7fffffc63e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_21@sint16[8], %fa1_21@sint16[8], %fa2_21@sint16[8], %fa3_21@sint16[8], %fa4_21@sint16[8], %fa5_21@sint16[8], %fa6_21@sint16[8], %fa7_21@sint16[8], %fa8_21@sint16[8] :
    %fa0_21 = %v0 /\ %fa1_21 = %v1 /\ %fa2_21 = %v2 /\ %fa3_21 = %v3 /\
    %fa4_21 = %v4 /\ %fa5_21 = %v5 /\ %fa6_21 = %v6 /\ %fa7_21 = %v7 /\
    %fa8_21 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_21 = %v0 /\ %fa1_21 = %v1 /\ %fa2_21 = %v2 /\ %fa3_21 = %v3 /\
    %fa4_21 = %v4 /\ %fa5_21 = %v5 /\ %fa6_21 = %v6 /\ %fa7_21 = %v7 /\
    %fa8_21 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 21 *)
    %fa0_21 = %v0 /\ %fa1_21 = %v1 /\ %fa2_21 = %v2 /\ %fa3_21 = %v3 /\
    %fa4_21 = %v4 /\ %fa5_21 = %v5 /\ %fa6_21 = %v6 /\ %fa7_21 = %v7 /\
    %fa8_21 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_21 = [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] /\
    %fa1_21 = [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] /\
    %fa2_21 = [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] /\
    %fa3_21 = [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] /\
    %fa4_21 = [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] /\
    %fa5_21 = [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] /\
    %fa6_21 = [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] /\
    %fa7_21 = [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] /\
    %fa8_21 = [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_21 = %v0 /\ %fa1_21 = %v1 /\ %fa2_21 = %v2 /\ %fa3_21 = %v3 /\
    %fa4_21 = %v4 /\ %fa5_21 = %v5 /\ %fa6_21 = %v6 /\ %fa7_21 = %v7 /\
    %fa8_21 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_21 = [arr2100, arr2101, arr2102, arr2103, arr2104, arr2105, arr2106, arr2107] /\
    %fa1_21 = [arr2180, arr2181, arr2182, arr2183, arr2184, arr2185, arr2186, arr2187] /\
    %fa2_21 = [arr2170, arr2171, arr2172, arr2173, arr2174, arr2175, arr2176, arr2177] /\
    %fa3_21 = [arr2160, arr2161, arr2162, arr2163, arr2164, arr2165, arr2166, arr2167] /\
    %fa4_21 = [arr2150, arr2151, arr2152, arr2153, arr2154, arr2155, arr2156, arr2157] /\
    %fa5_21 = [arr2140, arr2141, arr2142, arr2143, arr2144, arr2145, arr2146, arr2147] /\
    %fa6_21 = [arr2130, arr2131, arr2132, arr2133, arr2134, arr2135, arr2136, arr2137] /\
    %fa7_21 = [arr2120, arr2121, arr2122, arr2123, arr2124, arr2125, arr2126, arr2127] /\
    %fa8_21 = [arr2110, arr2111, arr2112, arr2113, arr2114, arr2115, arr2116, arr2117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_21 + %fa3_21 + %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_21 + %fa3_21 + %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_21 + %fa3_21 + %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_21 + %fa3_21 + %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_21 - %fa3_21 - %fa6_21
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_21 - %fa6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_21 - %fa4_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_21 - %fa4_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_21 - %fa4_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_21 - %fa4_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_21 - %fa5_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_21 - %fa5_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_21 - %fa5_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_21 - %fa5_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 22 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        %fa3_21 +
        %fa6_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_21 +
        %fa7_21 +
        %fa4_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_21 +
        %fa2_21 +
        %fa5_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_21 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_21 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 21], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 21]];


ghost %fb0_21@sint16[8], %fb1_21@sint16[8], %fb2_21@sint16[8], %fb3_21@sint16[8], %fb4_21@sint16[8], %fb5_21@sint16[8], %fb6_21@sint16[8], %fb7_21@sint16[8], %fb8_21@sint16[8] :
    %fb0_21 = %v17 /\ %fb1_21 = %v16 /\ %fb2_21 = %v0  /\ %fb3_21 = %v6  /\
    %fb4_21 = %v3  /\ %fb5_21 = %v7  /\ %fb6_21 = %v19 /\ %fb7_21 = %v1  /\ %fb8_21 = %v2
  &&
    %fb0_21 = %v17 /\ %fb1_21 = %v16 /\ %fb2_21 = %v0  /\ %fb3_21 = %v6  /\
    %fb4_21 = %v3  /\ %fb5_21 = %v7  /\ %fb6_21 = %v19 /\ %fb7_21 = %v1  /\ %fb8_21 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_21 + %fb6_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_21 + %fb6_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_21 + %fb6_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_21 + %fb6_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_21 - %fb6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_21 - %fb6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_21 - %fb6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_21 - %fb6_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_21@sint16[8] : %b1i1_21 = %v1 && %b1i1_21 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_21@sint16[8] : %b1i2_21 = %v5 && %b1i2_21 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_21 + %b1i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_21 + %b1i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_21 + %b1i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_21 + %b1i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_21 - %b1i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_21 - %b1i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_21 - %b1i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_21 - %b1i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_21@sint16[8] : %b2i1_21 = %v3 && %b2i1_21 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_21@sint16[8] : %b2i2_21 = %v1 && %b2i2_21 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_21 + %b2i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_21 + %b2i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_21 + %b2i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_21 + %b2i2_21
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_21 - %b2i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_21 - %b2i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_21 - %b2i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_21 - %b2i2_21)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 23 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        %fa1_21 +
        %fa2_21 +
        %fa3_21 +
        %fa4_21 +
        %fa5_21 +
        %fa6_21 +
        %fa7_21 +
        %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_21 +
        %fa3_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_21 +
        %fa6_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_21 +
        %fa3_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_21 +
        %fa6_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_21 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_21 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_21 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_21 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_21 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_21 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_21 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_21 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_21
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 21]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 21], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 21]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc620; PC = 0x5555551590 *)
mov [L0x7fffffc620,L0x7fffffc622,L0x7fffffc624,L0x7fffffc626,L0x7fffffc628,L0x7fffffc62a,L0x7fffffc62c,L0x7fffffc62e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc630; PC = 0x5555551598 *)
mov [L0x7fffffc630,L0x7fffffc632,L0x7fffffc634,L0x7fffffc636,L0x7fffffc638,L0x7fffffc63a,L0x7fffffc63c,L0x7fffffc63e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc640; PC = 0x55555515a0 *)
mov [L0x7fffffc640,L0x7fffffc642,L0x7fffffc644,L0x7fffffc646,L0x7fffffc648,L0x7fffffc64a,L0x7fffffc64c,L0x7fffffc64e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc650; PC = 0x55555515a8 *)
mov [L0x7fffffc650,L0x7fffffc652,L0x7fffffc654,L0x7fffffc656,L0x7fffffc658,L0x7fffffc65a,L0x7fffffc65c,L0x7fffffc65e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc660; PC = 0x55555515b0 *)
mov [L0x7fffffc660,L0x7fffffc662,L0x7fffffc664,L0x7fffffc666,L0x7fffffc668,L0x7fffffc66a,L0x7fffffc66c,L0x7fffffc66e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc670; PC = 0x55555515b8 *)
mov [L0x7fffffc670,L0x7fffffc672,L0x7fffffc674,L0x7fffffc676,L0x7fffffc678,L0x7fffffc67a,L0x7fffffc67c,L0x7fffffc67e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc680; PC = 0x55555515c0 *)
mov [L0x7fffffc680,L0x7fffffc682,L0x7fffffc684,L0x7fffffc686,L0x7fffffc688,L0x7fffffc68a,L0x7fffffc68c,L0x7fffffc68e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc690; PC = 0x55555515c8 *)
mov [L0x7fffffc690,L0x7fffffc692,L0x7fffffc694,L0x7fffffc696,L0x7fffffc698,L0x7fffffc69a,L0x7fffffc69c,L0x7fffffc69e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffc6a0; PC = 0x55555515d0 *)
mov [L0x7fffffc6a0,L0x7fffffc6a2,L0x7fffffc6a4,L0x7fffffc6a6,L0x7fffffc6a8,L0x7fffffc6aa,L0x7fffffc6ac,L0x7fffffc6ae] %v0;

cut (* 24 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc6d0; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc6d0,L0x7fffffc6d2,L0x7fffffc6d4,L0x7fffffc6d6,L0x7fffffc6d8,L0x7fffffc6da,L0x7fffffc6dc,L0x7fffffc6de];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc6e0; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc6e0,L0x7fffffc6e2,L0x7fffffc6e4,L0x7fffffc6e6,L0x7fffffc6e8,L0x7fffffc6ea,L0x7fffffc6ec,L0x7fffffc6ee];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc6f0; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc6f0,L0x7fffffc6f2,L0x7fffffc6f4,L0x7fffffc6f6,L0x7fffffc6f8,L0x7fffffc6fa,L0x7fffffc6fc,L0x7fffffc6fe];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc700; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc700,L0x7fffffc702,L0x7fffffc704,L0x7fffffc706,L0x7fffffc708,L0x7fffffc70a,L0x7fffffc70c,L0x7fffffc70e];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc710; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc710,L0x7fffffc712,L0x7fffffc714,L0x7fffffc716,L0x7fffffc718,L0x7fffffc71a,L0x7fffffc71c,L0x7fffffc71e];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc720; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc720,L0x7fffffc722,L0x7fffffc724,L0x7fffffc726,L0x7fffffc728,L0x7fffffc72a,L0x7fffffc72c,L0x7fffffc72e];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc730; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc730,L0x7fffffc732,L0x7fffffc734,L0x7fffffc736,L0x7fffffc738,L0x7fffffc73a,L0x7fffffc73c,L0x7fffffc73e];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc6c0; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc6c0,L0x7fffffc6c2,L0x7fffffc6c4,L0x7fffffc6c6,L0x7fffffc6c8,L0x7fffffc6ca,L0x7fffffc6cc,L0x7fffffc6ce];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc6b0; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc6b0,L0x7fffffc6b2,L0x7fffffc6b4,L0x7fffffc6b6,L0x7fffffc6b8,L0x7fffffc6ba,L0x7fffffc6bc,L0x7fffffc6be];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_30@sint16[8], %fa1_30@sint16[8], %fa2_30@sint16[8], %fa3_30@sint16[8], %fa4_30@sint16[8], %fa5_30@sint16[8], %fa6_30@sint16[8], %fa7_30@sint16[8], %fa8_30@sint16[8] :
    %fa0_30 = %v0 /\ %fa1_30 = %v1 /\ %fa2_30 = %v2 /\ %fa3_30 = %v3 /\
    %fa4_30 = %v4 /\ %fa5_30 = %v5 /\ %fa6_30 = %v6 /\ %fa7_30 = %v7 /\
    %fa8_30 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_30 = %v0 /\ %fa1_30 = %v1 /\ %fa2_30 = %v2 /\ %fa3_30 = %v3 /\
    %fa4_30 = %v4 /\ %fa5_30 = %v5 /\ %fa6_30 = %v6 /\ %fa7_30 = %v7 /\
    %fa8_30 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 25 *)
    %fa0_30 = %v0 /\ %fa1_30 = %v1 /\ %fa2_30 = %v2 /\ %fa3_30 = %v3 /\
    %fa4_30 = %v4 /\ %fa5_30 = %v5 /\ %fa6_30 = %v6 /\ %fa7_30 = %v7 /\
    %fa8_30 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_30 = [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] /\
    %fa1_30 = [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] /\
    %fa2_30 = [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] /\
    %fa3_30 = [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] /\
    %fa4_30 = [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] /\
    %fa5_30 = [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] /\
    %fa6_30 = [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] /\
    %fa7_30 = [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] /\
    %fa8_30 = [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_30 = %v0 /\ %fa1_30 = %v1 /\ %fa2_30 = %v2 /\ %fa3_30 = %v3 /\
    %fa4_30 = %v4 /\ %fa5_30 = %v5 /\ %fa6_30 = %v6 /\ %fa7_30 = %v7 /\
    %fa8_30 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_30 = [arr3000, arr3001, arr3002, arr3003, arr3004, arr3005, arr3006, arr3007] /\
    %fa1_30 = [arr3080, arr3081, arr3082, arr3083, arr3084, arr3085, arr3086, arr3087] /\
    %fa2_30 = [arr3070, arr3071, arr3072, arr3073, arr3074, arr3075, arr3076, arr3077] /\
    %fa3_30 = [arr3060, arr3061, arr3062, arr3063, arr3064, arr3065, arr3066, arr3067] /\
    %fa4_30 = [arr3050, arr3051, arr3052, arr3053, arr3054, arr3055, arr3056, arr3057] /\
    %fa5_30 = [arr3040, arr3041, arr3042, arr3043, arr3044, arr3045, arr3046, arr3047] /\
    %fa6_30 = [arr3030, arr3031, arr3032, arr3033, arr3034, arr3035, arr3036, arr3037] /\
    %fa7_30 = [arr3020, arr3021, arr3022, arr3023, arr3024, arr3025, arr3026, arr3027] /\
    %fa8_30 = [arr3010, arr3011, arr3012, arr3013, arr3014, arr3015, arr3016, arr3017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_30 + %fa3_30 + %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_30 + %fa3_30 + %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_30 + %fa3_30 + %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_30 + %fa3_30 + %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_30 - %fa3_30 - %fa6_30
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_30 - %fa6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_30 - %fa4_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_30 - %fa4_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_30 - %fa4_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_30 - %fa4_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_30 - %fa5_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_30 - %fa5_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_30 - %fa5_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_30 - %fa5_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 26 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        %fa3_30 +
        %fa6_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_30 +
        %fa7_30 +
        %fa4_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_30 +
        %fa2_30 +
        %fa5_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_30 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_30 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 25], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 25]];


ghost %fb0_30@sint16[8], %fb1_30@sint16[8], %fb2_30@sint16[8], %fb3_30@sint16[8], %fb4_30@sint16[8], %fb5_30@sint16[8], %fb6_30@sint16[8], %fb7_30@sint16[8], %fb8_30@sint16[8] :
    %fb0_30 = %v17 /\ %fb1_30 = %v16 /\ %fb2_30 = %v0  /\ %fb3_30 = %v6  /\
    %fb4_30 = %v3  /\ %fb5_30 = %v7  /\ %fb6_30 = %v19 /\ %fb7_30 = %v1  /\ %fb8_30 = %v2
  &&
    %fb0_30 = %v17 /\ %fb1_30 = %v16 /\ %fb2_30 = %v0  /\ %fb3_30 = %v6  /\
    %fb4_30 = %v3  /\ %fb5_30 = %v7  /\ %fb6_30 = %v19 /\ %fb7_30 = %v1  /\ %fb8_30 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_30 + %fb6_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_30 + %fb6_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_30 + %fb6_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_30 + %fb6_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_30 - %fb6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_30 - %fb6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_30 - %fb6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_30 - %fb6_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_30@sint16[8] : %b1i1_30 = %v1 && %b1i1_30 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_30@sint16[8] : %b1i2_30 = %v5 && %b1i2_30 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_30 + %b1i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_30 + %b1i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_30 + %b1i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_30 + %b1i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_30 - %b1i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_30 - %b1i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_30 - %b1i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_30 - %b1i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_30@sint16[8] : %b2i1_30 = %v3 && %b2i1_30 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_30@sint16[8] : %b2i2_30 = %v1 && %b2i2_30 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_30 + %b2i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_30 + %b2i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_30 + %b2i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_30 + %b2i2_30
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_30 - %b2i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_30 - %b2i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_30 - %b2i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_30 - %b2i2_30)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 27 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        %fa1_30 +
        %fa2_30 +
        %fa3_30 +
        %fa4_30 +
        %fa5_30 +
        %fa6_30 +
        %fa7_30 +
        %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_30 +
        %fa3_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_30 +
        %fa6_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_30 +
        %fa3_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_30 +
        %fa6_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_30 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_30 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_30 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_30 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_30 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_30 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_30 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_30 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_30
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 25]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 25], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 25]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc6b0; PC = 0x55555514f8 *)
mov [L0x7fffffc6b0,L0x7fffffc6b2,L0x7fffffc6b4,L0x7fffffc6b6,L0x7fffffc6b8,L0x7fffffc6ba,L0x7fffffc6bc,L0x7fffffc6be] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc6c0; PC = 0x5555551500 *)
mov [L0x7fffffc6c0,L0x7fffffc6c2,L0x7fffffc6c4,L0x7fffffc6c6,L0x7fffffc6c8,L0x7fffffc6ca,L0x7fffffc6cc,L0x7fffffc6ce] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc6d0; PC = 0x5555551508 *)
mov [L0x7fffffc6d0,L0x7fffffc6d2,L0x7fffffc6d4,L0x7fffffc6d6,L0x7fffffc6d8,L0x7fffffc6da,L0x7fffffc6dc,L0x7fffffc6de] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc6e0; PC = 0x5555551510 *)
mov [L0x7fffffc6e0,L0x7fffffc6e2,L0x7fffffc6e4,L0x7fffffc6e6,L0x7fffffc6e8,L0x7fffffc6ea,L0x7fffffc6ec,L0x7fffffc6ee] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc6f0; PC = 0x5555551518 *)
mov [L0x7fffffc6f0,L0x7fffffc6f2,L0x7fffffc6f4,L0x7fffffc6f6,L0x7fffffc6f8,L0x7fffffc6fa,L0x7fffffc6fc,L0x7fffffc6fe] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc700; PC = 0x5555551520 *)
mov [L0x7fffffc700,L0x7fffffc702,L0x7fffffc704,L0x7fffffc706,L0x7fffffc708,L0x7fffffc70a,L0x7fffffc70c,L0x7fffffc70e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc710; PC = 0x5555551528 *)
mov [L0x7fffffc710,L0x7fffffc712,L0x7fffffc714,L0x7fffffc716,L0x7fffffc718,L0x7fffffc71a,L0x7fffffc71c,L0x7fffffc71e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc720; PC = 0x5555551530 *)
mov [L0x7fffffc720,L0x7fffffc722,L0x7fffffc724,L0x7fffffc726,L0x7fffffc728,L0x7fffffc72a,L0x7fffffc72c,L0x7fffffc72e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc730; PC = 0x5555551538 *)
mov [L0x7fffffc730,L0x7fffffc732,L0x7fffffc734,L0x7fffffc736,L0x7fffffc738,L0x7fffffc73a,L0x7fffffc73c,L0x7fffffc73e] %v0;

cut (* 28 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc760; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc760,L0x7fffffc762,L0x7fffffc764,L0x7fffffc766,L0x7fffffc768,L0x7fffffc76a,L0x7fffffc76c,L0x7fffffc76e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc770; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc770,L0x7fffffc772,L0x7fffffc774,L0x7fffffc776,L0x7fffffc778,L0x7fffffc77a,L0x7fffffc77c,L0x7fffffc77e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc780; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc780,L0x7fffffc782,L0x7fffffc784,L0x7fffffc786,L0x7fffffc788,L0x7fffffc78a,L0x7fffffc78c,L0x7fffffc78e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc790; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc790,L0x7fffffc792,L0x7fffffc794,L0x7fffffc796,L0x7fffffc798,L0x7fffffc79a,L0x7fffffc79c,L0x7fffffc79e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc7a0; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc7a0,L0x7fffffc7a2,L0x7fffffc7a4,L0x7fffffc7a6,L0x7fffffc7a8,L0x7fffffc7aa,L0x7fffffc7ac,L0x7fffffc7ae];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc7b0; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc7b0,L0x7fffffc7b2,L0x7fffffc7b4,L0x7fffffc7b6,L0x7fffffc7b8,L0x7fffffc7ba,L0x7fffffc7bc,L0x7fffffc7be];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffc7c0; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffc7c0,L0x7fffffc7c2,L0x7fffffc7c4,L0x7fffffc7c6,L0x7fffffc7c8,L0x7fffffc7ca,L0x7fffffc7cc,L0x7fffffc7ce];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc740; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc740,L0x7fffffc742,L0x7fffffc744,L0x7fffffc746,L0x7fffffc748,L0x7fffffc74a,L0x7fffffc74c,L0x7fffffc74e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc750; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc750,L0x7fffffc752,L0x7fffffc754,L0x7fffffc756,L0x7fffffc758,L0x7fffffc75a,L0x7fffffc75c,L0x7fffffc75e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_31@sint16[8], %fa1_31@sint16[8], %fa2_31@sint16[8], %fa3_31@sint16[8], %fa4_31@sint16[8], %fa5_31@sint16[8], %fa6_31@sint16[8], %fa7_31@sint16[8], %fa8_31@sint16[8] :
    %fa0_31 = %v0 /\ %fa1_31 = %v1 /\ %fa2_31 = %v2 /\ %fa3_31 = %v3 /\
    %fa4_31 = %v4 /\ %fa5_31 = %v5 /\ %fa6_31 = %v6 /\ %fa7_31 = %v7 /\
    %fa8_31 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_31 = %v0 /\ %fa1_31 = %v1 /\ %fa2_31 = %v2 /\ %fa3_31 = %v3 /\
    %fa4_31 = %v4 /\ %fa5_31 = %v5 /\ %fa6_31 = %v6 /\ %fa7_31 = %v7 /\
    %fa8_31 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 29 *)
    %fa0_31 = %v0 /\ %fa1_31 = %v1 /\ %fa2_31 = %v2 /\ %fa3_31 = %v3 /\
    %fa4_31 = %v4 /\ %fa5_31 = %v5 /\ %fa6_31 = %v6 /\ %fa7_31 = %v7 /\
    %fa8_31 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_31 = [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] /\
    %fa1_31 = [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] /\
    %fa2_31 = [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] /\
    %fa3_31 = [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] /\
    %fa4_31 = [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] /\
    %fa5_31 = [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] /\
    %fa6_31 = [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] /\
    %fa7_31 = [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] /\
    %fa8_31 = [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_31 = %v0 /\ %fa1_31 = %v1 /\ %fa2_31 = %v2 /\ %fa3_31 = %v3 /\
    %fa4_31 = %v4 /\ %fa5_31 = %v5 /\ %fa6_31 = %v6 /\ %fa7_31 = %v7 /\
    %fa8_31 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_31 = [arr3100, arr3101, arr3102, arr3103, arr3104, arr3105, arr3106, arr3107] /\
    %fa1_31 = [arr3180, arr3181, arr3182, arr3183, arr3184, arr3185, arr3186, arr3187] /\
    %fa2_31 = [arr3170, arr3171, arr3172, arr3173, arr3174, arr3175, arr3176, arr3177] /\
    %fa3_31 = [arr3160, arr3161, arr3162, arr3163, arr3164, arr3165, arr3166, arr3167] /\
    %fa4_31 = [arr3150, arr3151, arr3152, arr3153, arr3154, arr3155, arr3156, arr3157] /\
    %fa5_31 = [arr3140, arr3141, arr3142, arr3143, arr3144, arr3145, arr3146, arr3147] /\
    %fa6_31 = [arr3130, arr3131, arr3132, arr3133, arr3134, arr3135, arr3136, arr3137] /\
    %fa7_31 = [arr3120, arr3121, arr3122, arr3123, arr3124, arr3125, arr3126, arr3127] /\
    %fa8_31 = [arr3110, arr3111, arr3112, arr3113, arr3114, arr3115, arr3116, arr3117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_31 + %fa3_31 + %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_31 + %fa3_31 + %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_31 + %fa3_31 + %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_31 + %fa3_31 + %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_31 - %fa3_31 - %fa6_31
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_31 - %fa6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_31 - %fa4_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_31 - %fa4_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_31 - %fa4_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_31 - %fa4_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_31 - %fa5_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_31 - %fa5_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_31 - %fa5_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_31 - %fa5_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 30 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        %fa3_31 +
        %fa6_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_31 +
        %fa7_31 +
        %fa4_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_31 +
        %fa2_31 +
        %fa5_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_31 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_31 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 29], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 29]];


ghost %fb0_31@sint16[8], %fb1_31@sint16[8], %fb2_31@sint16[8], %fb3_31@sint16[8], %fb4_31@sint16[8], %fb5_31@sint16[8], %fb6_31@sint16[8], %fb7_31@sint16[8], %fb8_31@sint16[8] :
    %fb0_31 = %v17 /\ %fb1_31 = %v16 /\ %fb2_31 = %v0  /\ %fb3_31 = %v6  /\
    %fb4_31 = %v3  /\ %fb5_31 = %v7  /\ %fb6_31 = %v19 /\ %fb7_31 = %v1  /\ %fb8_31 = %v2
  &&
    %fb0_31 = %v17 /\ %fb1_31 = %v16 /\ %fb2_31 = %v0  /\ %fb3_31 = %v6  /\
    %fb4_31 = %v3  /\ %fb5_31 = %v7  /\ %fb6_31 = %v19 /\ %fb7_31 = %v1  /\ %fb8_31 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_31 + %fb6_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_31 + %fb6_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_31 + %fb6_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_31 + %fb6_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_31 - %fb6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_31 - %fb6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_31 - %fb6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_31 - %fb6_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_31@sint16[8] : %b1i1_31 = %v1 && %b1i1_31 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_31@sint16[8] : %b1i2_31 = %v5 && %b1i2_31 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_31 + %b1i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_31 + %b1i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_31 + %b1i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_31 + %b1i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_31 - %b1i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_31 - %b1i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_31 - %b1i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_31 - %b1i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_31@sint16[8] : %b2i1_31 = %v3 && %b2i1_31 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_31@sint16[8] : %b2i2_31 = %v1 && %b2i2_31 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_31 + %b2i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_31 + %b2i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_31 + %b2i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_31 + %b2i2_31
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_31 - %b2i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_31 - %b2i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_31 - %b2i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_31 - %b2i2_31)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 31 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        %fa1_31 +
        %fa2_31 +
        %fa3_31 +
        %fa4_31 +
        %fa5_31 +
        %fa6_31 +
        %fa7_31 +
        %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_31 +
        %fa3_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_31 +
        %fa6_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_31 +
        %fa3_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_31 +
        %fa6_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_31 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_31 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_31 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_31 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_31 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_31 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_31 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_31 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_31
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 29]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 29], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 29]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc740; PC = 0x5555551590 *)
mov [L0x7fffffc740,L0x7fffffc742,L0x7fffffc744,L0x7fffffc746,L0x7fffffc748,L0x7fffffc74a,L0x7fffffc74c,L0x7fffffc74e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc750; PC = 0x5555551598 *)
mov [L0x7fffffc750,L0x7fffffc752,L0x7fffffc754,L0x7fffffc756,L0x7fffffc758,L0x7fffffc75a,L0x7fffffc75c,L0x7fffffc75e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc760; PC = 0x55555515a0 *)
mov [L0x7fffffc760,L0x7fffffc762,L0x7fffffc764,L0x7fffffc766,L0x7fffffc768,L0x7fffffc76a,L0x7fffffc76c,L0x7fffffc76e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc770; PC = 0x55555515a8 *)
mov [L0x7fffffc770,L0x7fffffc772,L0x7fffffc774,L0x7fffffc776,L0x7fffffc778,L0x7fffffc77a,L0x7fffffc77c,L0x7fffffc77e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc780; PC = 0x55555515b0 *)
mov [L0x7fffffc780,L0x7fffffc782,L0x7fffffc784,L0x7fffffc786,L0x7fffffc788,L0x7fffffc78a,L0x7fffffc78c,L0x7fffffc78e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc790; PC = 0x55555515b8 *)
mov [L0x7fffffc790,L0x7fffffc792,L0x7fffffc794,L0x7fffffc796,L0x7fffffc798,L0x7fffffc79a,L0x7fffffc79c,L0x7fffffc79e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc7a0; PC = 0x55555515c0 *)
mov [L0x7fffffc7a0,L0x7fffffc7a2,L0x7fffffc7a4,L0x7fffffc7a6,L0x7fffffc7a8,L0x7fffffc7aa,L0x7fffffc7ac,L0x7fffffc7ae] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc7b0; PC = 0x55555515c8 *)
mov [L0x7fffffc7b0,L0x7fffffc7b2,L0x7fffffc7b4,L0x7fffffc7b6,L0x7fffffc7b8,L0x7fffffc7ba,L0x7fffffc7bc,L0x7fffffc7be] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffc7c0; PC = 0x55555515d0 *)
mov [L0x7fffffc7c0,L0x7fffffc7c2,L0x7fffffc7c4,L0x7fffffc7c6,L0x7fffffc7c8,L0x7fffffc7ca,L0x7fffffc7cc,L0x7fffffc7ce] %v0;

cut (* 32 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc7f0; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc7f0,L0x7fffffc7f2,L0x7fffffc7f4,L0x7fffffc7f6,L0x7fffffc7f8,L0x7fffffc7fa,L0x7fffffc7fc,L0x7fffffc7fe];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc800; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc800,L0x7fffffc802,L0x7fffffc804,L0x7fffffc806,L0x7fffffc808,L0x7fffffc80a,L0x7fffffc80c,L0x7fffffc80e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc810; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc810,L0x7fffffc812,L0x7fffffc814,L0x7fffffc816,L0x7fffffc818,L0x7fffffc81a,L0x7fffffc81c,L0x7fffffc81e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc820; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc820,L0x7fffffc822,L0x7fffffc824,L0x7fffffc826,L0x7fffffc828,L0x7fffffc82a,L0x7fffffc82c,L0x7fffffc82e];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc830; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc830,L0x7fffffc832,L0x7fffffc834,L0x7fffffc836,L0x7fffffc838,L0x7fffffc83a,L0x7fffffc83c,L0x7fffffc83e];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc840; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc840,L0x7fffffc842,L0x7fffffc844,L0x7fffffc846,L0x7fffffc848,L0x7fffffc84a,L0x7fffffc84c,L0x7fffffc84e];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc850; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc850,L0x7fffffc852,L0x7fffffc854,L0x7fffffc856,L0x7fffffc858,L0x7fffffc85a,L0x7fffffc85c,L0x7fffffc85e];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc7e0; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc7e0,L0x7fffffc7e2,L0x7fffffc7e4,L0x7fffffc7e6,L0x7fffffc7e8,L0x7fffffc7ea,L0x7fffffc7ec,L0x7fffffc7ee];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc7d0; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc7d0,L0x7fffffc7d2,L0x7fffffc7d4,L0x7fffffc7d6,L0x7fffffc7d8,L0x7fffffc7da,L0x7fffffc7dc,L0x7fffffc7de];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_40@sint16[8], %fa1_40@sint16[8], %fa2_40@sint16[8], %fa3_40@sint16[8], %fa4_40@sint16[8], %fa5_40@sint16[8], %fa6_40@sint16[8], %fa7_40@sint16[8], %fa8_40@sint16[8] :
    %fa0_40 = %v0 /\ %fa1_40 = %v1 /\ %fa2_40 = %v2 /\ %fa3_40 = %v3 /\
    %fa4_40 = %v4 /\ %fa5_40 = %v5 /\ %fa6_40 = %v6 /\ %fa7_40 = %v7 /\
    %fa8_40 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_40 = %v0 /\ %fa1_40 = %v1 /\ %fa2_40 = %v2 /\ %fa3_40 = %v3 /\
    %fa4_40 = %v4 /\ %fa5_40 = %v5 /\ %fa6_40 = %v6 /\ %fa7_40 = %v7 /\
    %fa8_40 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 33 *)
    %fa0_40 = %v0 /\ %fa1_40 = %v1 /\ %fa2_40 = %v2 /\ %fa3_40 = %v3 /\
    %fa4_40 = %v4 /\ %fa5_40 = %v5 /\ %fa6_40 = %v6 /\ %fa7_40 = %v7 /\
    %fa8_40 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_40 = [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] /\
    %fa1_40 = [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] /\
    %fa2_40 = [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] /\
    %fa3_40 = [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] /\
    %fa4_40 = [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] /\
    %fa5_40 = [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] /\
    %fa6_40 = [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] /\
    %fa7_40 = [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] /\
    %fa8_40 = [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_40 = %v0 /\ %fa1_40 = %v1 /\ %fa2_40 = %v2 /\ %fa3_40 = %v3 /\
    %fa4_40 = %v4 /\ %fa5_40 = %v5 /\ %fa6_40 = %v6 /\ %fa7_40 = %v7 /\
    %fa8_40 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_40 = [arr4000, arr4001, arr4002, arr4003, arr4004, arr4005, arr4006, arr4007] /\
    %fa1_40 = [arr4080, arr4081, arr4082, arr4083, arr4084, arr4085, arr4086, arr4087] /\
    %fa2_40 = [arr4070, arr4071, arr4072, arr4073, arr4074, arr4075, arr4076, arr4077] /\
    %fa3_40 = [arr4060, arr4061, arr4062, arr4063, arr4064, arr4065, arr4066, arr4067] /\
    %fa4_40 = [arr4050, arr4051, arr4052, arr4053, arr4054, arr4055, arr4056, arr4057] /\
    %fa5_40 = [arr4040, arr4041, arr4042, arr4043, arr4044, arr4045, arr4046, arr4047] /\
    %fa6_40 = [arr4030, arr4031, arr4032, arr4033, arr4034, arr4035, arr4036, arr4037] /\
    %fa7_40 = [arr4020, arr4021, arr4022, arr4023, arr4024, arr4025, arr4026, arr4027] /\
    %fa8_40 = [arr4010, arr4011, arr4012, arr4013, arr4014, arr4015, arr4016, arr4017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_40 + %fa3_40 + %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_40 + %fa3_40 + %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_40 + %fa3_40 + %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_40 + %fa3_40 + %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_40 - %fa3_40 - %fa6_40
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_40 - %fa6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_40 - %fa4_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_40 - %fa4_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_40 - %fa4_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_40 - %fa4_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_40 - %fa5_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_40 - %fa5_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_40 - %fa5_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_40 - %fa5_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 34 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        %fa3_40 +
        %fa6_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_40 +
        %fa7_40 +
        %fa4_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_40 +
        %fa2_40 +
        %fa5_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_40 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_40 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 33], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 33]];


ghost %fb0_40@sint16[8], %fb1_40@sint16[8], %fb2_40@sint16[8], %fb3_40@sint16[8], %fb4_40@sint16[8], %fb5_40@sint16[8], %fb6_40@sint16[8], %fb7_40@sint16[8], %fb8_40@sint16[8] :
    %fb0_40 = %v17 /\ %fb1_40 = %v16 /\ %fb2_40 = %v0  /\ %fb3_40 = %v6  /\
    %fb4_40 = %v3  /\ %fb5_40 = %v7  /\ %fb6_40 = %v19 /\ %fb7_40 = %v1  /\ %fb8_40 = %v2
  &&
    %fb0_40 = %v17 /\ %fb1_40 = %v16 /\ %fb2_40 = %v0  /\ %fb3_40 = %v6  /\
    %fb4_40 = %v3  /\ %fb5_40 = %v7  /\ %fb6_40 = %v19 /\ %fb7_40 = %v1  /\ %fb8_40 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_40 + %fb6_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_40 + %fb6_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_40 + %fb6_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_40 + %fb6_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_40 - %fb6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_40 - %fb6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_40 - %fb6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_40 - %fb6_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_40@sint16[8] : %b1i1_40 = %v1 && %b1i1_40 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_40@sint16[8] : %b1i2_40 = %v5 && %b1i2_40 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_40 + %b1i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_40 + %b1i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_40 + %b1i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_40 + %b1i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_40 - %b1i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_40 - %b1i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_40 - %b1i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_40 - %b1i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_40@sint16[8] : %b2i1_40 = %v3 && %b2i1_40 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_40@sint16[8] : %b2i2_40 = %v1 && %b2i2_40 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_40 + %b2i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_40 + %b2i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_40 + %b2i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_40 + %b2i2_40
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_40 - %b2i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_40 - %b2i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_40 - %b2i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_40 - %b2i2_40)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 35 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        %fa1_40 +
        %fa2_40 +
        %fa3_40 +
        %fa4_40 +
        %fa5_40 +
        %fa6_40 +
        %fa7_40 +
        %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_40 +
        %fa3_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_40 +
        %fa6_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_40 +
        %fa3_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_40 +
        %fa6_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_40 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_40 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_40 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_40 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_40 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_40 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_40 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_40 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_40
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 33]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 33], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 33]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc7d0; PC = 0x55555514f8 *)
mov [L0x7fffffc7d0,L0x7fffffc7d2,L0x7fffffc7d4,L0x7fffffc7d6,L0x7fffffc7d8,L0x7fffffc7da,L0x7fffffc7dc,L0x7fffffc7de] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc7e0; PC = 0x5555551500 *)
mov [L0x7fffffc7e0,L0x7fffffc7e2,L0x7fffffc7e4,L0x7fffffc7e6,L0x7fffffc7e8,L0x7fffffc7ea,L0x7fffffc7ec,L0x7fffffc7ee] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc7f0; PC = 0x5555551508 *)
mov [L0x7fffffc7f0,L0x7fffffc7f2,L0x7fffffc7f4,L0x7fffffc7f6,L0x7fffffc7f8,L0x7fffffc7fa,L0x7fffffc7fc,L0x7fffffc7fe] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc800; PC = 0x5555551510 *)
mov [L0x7fffffc800,L0x7fffffc802,L0x7fffffc804,L0x7fffffc806,L0x7fffffc808,L0x7fffffc80a,L0x7fffffc80c,L0x7fffffc80e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc810; PC = 0x5555551518 *)
mov [L0x7fffffc810,L0x7fffffc812,L0x7fffffc814,L0x7fffffc816,L0x7fffffc818,L0x7fffffc81a,L0x7fffffc81c,L0x7fffffc81e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc820; PC = 0x5555551520 *)
mov [L0x7fffffc820,L0x7fffffc822,L0x7fffffc824,L0x7fffffc826,L0x7fffffc828,L0x7fffffc82a,L0x7fffffc82c,L0x7fffffc82e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc830; PC = 0x5555551528 *)
mov [L0x7fffffc830,L0x7fffffc832,L0x7fffffc834,L0x7fffffc836,L0x7fffffc838,L0x7fffffc83a,L0x7fffffc83c,L0x7fffffc83e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc840; PC = 0x5555551530 *)
mov [L0x7fffffc840,L0x7fffffc842,L0x7fffffc844,L0x7fffffc846,L0x7fffffc848,L0x7fffffc84a,L0x7fffffc84c,L0x7fffffc84e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc850; PC = 0x5555551538 *)
mov [L0x7fffffc850,L0x7fffffc852,L0x7fffffc854,L0x7fffffc856,L0x7fffffc858,L0x7fffffc85a,L0x7fffffc85c,L0x7fffffc85e] %v0;

cut (* 36 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc880; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc880,L0x7fffffc882,L0x7fffffc884,L0x7fffffc886,L0x7fffffc888,L0x7fffffc88a,L0x7fffffc88c,L0x7fffffc88e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc890; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc890,L0x7fffffc892,L0x7fffffc894,L0x7fffffc896,L0x7fffffc898,L0x7fffffc89a,L0x7fffffc89c,L0x7fffffc89e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc8a0; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc8a0,L0x7fffffc8a2,L0x7fffffc8a4,L0x7fffffc8a6,L0x7fffffc8a8,L0x7fffffc8aa,L0x7fffffc8ac,L0x7fffffc8ae];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc8b0; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc8b0,L0x7fffffc8b2,L0x7fffffc8b4,L0x7fffffc8b6,L0x7fffffc8b8,L0x7fffffc8ba,L0x7fffffc8bc,L0x7fffffc8be];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc8c0; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc8c0,L0x7fffffc8c2,L0x7fffffc8c4,L0x7fffffc8c6,L0x7fffffc8c8,L0x7fffffc8ca,L0x7fffffc8cc,L0x7fffffc8ce];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc8d0; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc8d0,L0x7fffffc8d2,L0x7fffffc8d4,L0x7fffffc8d6,L0x7fffffc8d8,L0x7fffffc8da,L0x7fffffc8dc,L0x7fffffc8de];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffc8e0; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffc8e0,L0x7fffffc8e2,L0x7fffffc8e4,L0x7fffffc8e6,L0x7fffffc8e8,L0x7fffffc8ea,L0x7fffffc8ec,L0x7fffffc8ee];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc860; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc860,L0x7fffffc862,L0x7fffffc864,L0x7fffffc866,L0x7fffffc868,L0x7fffffc86a,L0x7fffffc86c,L0x7fffffc86e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc870; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc870,L0x7fffffc872,L0x7fffffc874,L0x7fffffc876,L0x7fffffc878,L0x7fffffc87a,L0x7fffffc87c,L0x7fffffc87e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_41@sint16[8], %fa1_41@sint16[8], %fa2_41@sint16[8], %fa3_41@sint16[8], %fa4_41@sint16[8], %fa5_41@sint16[8], %fa6_41@sint16[8], %fa7_41@sint16[8], %fa8_41@sint16[8] :
    %fa0_41 = %v0 /\ %fa1_41 = %v1 /\ %fa2_41 = %v2 /\ %fa3_41 = %v3 /\
    %fa4_41 = %v4 /\ %fa5_41 = %v5 /\ %fa6_41 = %v6 /\ %fa7_41 = %v7 /\
    %fa8_41 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_41 = %v0 /\ %fa1_41 = %v1 /\ %fa2_41 = %v2 /\ %fa3_41 = %v3 /\
    %fa4_41 = %v4 /\ %fa5_41 = %v5 /\ %fa6_41 = %v6 /\ %fa7_41 = %v7 /\
    %fa8_41 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 37 *)
    %fa0_41 = %v0 /\ %fa1_41 = %v1 /\ %fa2_41 = %v2 /\ %fa3_41 = %v3 /\
    %fa4_41 = %v4 /\ %fa5_41 = %v5 /\ %fa6_41 = %v6 /\ %fa7_41 = %v7 /\
    %fa8_41 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_41 = [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] /\
    %fa1_41 = [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] /\
    %fa2_41 = [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] /\
    %fa3_41 = [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] /\
    %fa4_41 = [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] /\
    %fa5_41 = [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] /\
    %fa6_41 = [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] /\
    %fa7_41 = [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] /\
    %fa8_41 = [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_41 = %v0 /\ %fa1_41 = %v1 /\ %fa2_41 = %v2 /\ %fa3_41 = %v3 /\
    %fa4_41 = %v4 /\ %fa5_41 = %v5 /\ %fa6_41 = %v6 /\ %fa7_41 = %v7 /\
    %fa8_41 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_41 = [arr4100, arr4101, arr4102, arr4103, arr4104, arr4105, arr4106, arr4107] /\
    %fa1_41 = [arr4180, arr4181, arr4182, arr4183, arr4184, arr4185, arr4186, arr4187] /\
    %fa2_41 = [arr4170, arr4171, arr4172, arr4173, arr4174, arr4175, arr4176, arr4177] /\
    %fa3_41 = [arr4160, arr4161, arr4162, arr4163, arr4164, arr4165, arr4166, arr4167] /\
    %fa4_41 = [arr4150, arr4151, arr4152, arr4153, arr4154, arr4155, arr4156, arr4157] /\
    %fa5_41 = [arr4140, arr4141, arr4142, arr4143, arr4144, arr4145, arr4146, arr4147] /\
    %fa6_41 = [arr4130, arr4131, arr4132, arr4133, arr4134, arr4135, arr4136, arr4137] /\
    %fa7_41 = [arr4120, arr4121, arr4122, arr4123, arr4124, arr4125, arr4126, arr4127] /\
    %fa8_41 = [arr4110, arr4111, arr4112, arr4113, arr4114, arr4115, arr4116, arr4117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_41 + %fa3_41 + %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_41 + %fa3_41 + %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_41 + %fa3_41 + %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_41 + %fa3_41 + %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_41 - %fa3_41 - %fa6_41
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_41 - %fa6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_41 - %fa4_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_41 - %fa4_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_41 - %fa4_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_41 - %fa4_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_41 - %fa5_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_41 - %fa5_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_41 - %fa5_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_41 - %fa5_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 38 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        %fa3_41 +
        %fa6_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_41 +
        %fa7_41 +
        %fa4_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_41 +
        %fa2_41 +
        %fa5_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_41 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_41 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 37], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 37]];


ghost %fb0_41@sint16[8], %fb1_41@sint16[8], %fb2_41@sint16[8], %fb3_41@sint16[8], %fb4_41@sint16[8], %fb5_41@sint16[8], %fb6_41@sint16[8], %fb7_41@sint16[8], %fb8_41@sint16[8] :
    %fb0_41 = %v17 /\ %fb1_41 = %v16 /\ %fb2_41 = %v0  /\ %fb3_41 = %v6  /\
    %fb4_41 = %v3  /\ %fb5_41 = %v7  /\ %fb6_41 = %v19 /\ %fb7_41 = %v1  /\ %fb8_41 = %v2
  &&
    %fb0_41 = %v17 /\ %fb1_41 = %v16 /\ %fb2_41 = %v0  /\ %fb3_41 = %v6  /\
    %fb4_41 = %v3  /\ %fb5_41 = %v7  /\ %fb6_41 = %v19 /\ %fb7_41 = %v1  /\ %fb8_41 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_41 + %fb6_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_41 + %fb6_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_41 + %fb6_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_41 + %fb6_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_41 - %fb6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_41 - %fb6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_41 - %fb6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_41 - %fb6_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_41@sint16[8] : %b1i1_41 = %v1 && %b1i1_41 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_41@sint16[8] : %b1i2_41 = %v5 && %b1i2_41 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_41 + %b1i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_41 + %b1i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_41 + %b1i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_41 + %b1i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_41 - %b1i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_41 - %b1i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_41 - %b1i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_41 - %b1i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_41@sint16[8] : %b2i1_41 = %v3 && %b2i1_41 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_41@sint16[8] : %b2i2_41 = %v1 && %b2i2_41 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_41 + %b2i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_41 + %b2i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_41 + %b2i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_41 + %b2i2_41
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_41 - %b2i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_41 - %b2i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_41 - %b2i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_41 - %b2i2_41)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 39 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        %fa1_41 +
        %fa2_41 +
        %fa3_41 +
        %fa4_41 +
        %fa5_41 +
        %fa6_41 +
        %fa7_41 +
        %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_41 +
        %fa3_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_41 +
        %fa6_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_41 +
        %fa3_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_41 +
        %fa6_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_41 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_41 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_41 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_41 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_41 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_41 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_41 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_41 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_41
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 37]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 37], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 37]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc860; PC = 0x5555551590 *)
mov [L0x7fffffc860,L0x7fffffc862,L0x7fffffc864,L0x7fffffc866,L0x7fffffc868,L0x7fffffc86a,L0x7fffffc86c,L0x7fffffc86e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc870; PC = 0x5555551598 *)
mov [L0x7fffffc870,L0x7fffffc872,L0x7fffffc874,L0x7fffffc876,L0x7fffffc878,L0x7fffffc87a,L0x7fffffc87c,L0x7fffffc87e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc880; PC = 0x55555515a0 *)
mov [L0x7fffffc880,L0x7fffffc882,L0x7fffffc884,L0x7fffffc886,L0x7fffffc888,L0x7fffffc88a,L0x7fffffc88c,L0x7fffffc88e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc890; PC = 0x55555515a8 *)
mov [L0x7fffffc890,L0x7fffffc892,L0x7fffffc894,L0x7fffffc896,L0x7fffffc898,L0x7fffffc89a,L0x7fffffc89c,L0x7fffffc89e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc8a0; PC = 0x55555515b0 *)
mov [L0x7fffffc8a0,L0x7fffffc8a2,L0x7fffffc8a4,L0x7fffffc8a6,L0x7fffffc8a8,L0x7fffffc8aa,L0x7fffffc8ac,L0x7fffffc8ae] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc8b0; PC = 0x55555515b8 *)
mov [L0x7fffffc8b0,L0x7fffffc8b2,L0x7fffffc8b4,L0x7fffffc8b6,L0x7fffffc8b8,L0x7fffffc8ba,L0x7fffffc8bc,L0x7fffffc8be] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc8c0; PC = 0x55555515c0 *)
mov [L0x7fffffc8c0,L0x7fffffc8c2,L0x7fffffc8c4,L0x7fffffc8c6,L0x7fffffc8c8,L0x7fffffc8ca,L0x7fffffc8cc,L0x7fffffc8ce] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc8d0; PC = 0x55555515c8 *)
mov [L0x7fffffc8d0,L0x7fffffc8d2,L0x7fffffc8d4,L0x7fffffc8d6,L0x7fffffc8d8,L0x7fffffc8da,L0x7fffffc8dc,L0x7fffffc8de] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffc8e0; PC = 0x55555515d0 *)
mov [L0x7fffffc8e0,L0x7fffffc8e2,L0x7fffffc8e4,L0x7fffffc8e6,L0x7fffffc8e8,L0x7fffffc8ea,L0x7fffffc8ec,L0x7fffffc8ee] %v0;

cut (* 40 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffc910; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffc910,L0x7fffffc912,L0x7fffffc914,L0x7fffffc916,L0x7fffffc918,L0x7fffffc91a,L0x7fffffc91c,L0x7fffffc91e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffc920; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffc920,L0x7fffffc922,L0x7fffffc924,L0x7fffffc926,L0x7fffffc928,L0x7fffffc92a,L0x7fffffc92c,L0x7fffffc92e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffc930; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffc930,L0x7fffffc932,L0x7fffffc934,L0x7fffffc936,L0x7fffffc938,L0x7fffffc93a,L0x7fffffc93c,L0x7fffffc93e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffc940; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffc940,L0x7fffffc942,L0x7fffffc944,L0x7fffffc946,L0x7fffffc948,L0x7fffffc94a,L0x7fffffc94c,L0x7fffffc94e];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffc950; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffc950,L0x7fffffc952,L0x7fffffc954,L0x7fffffc956,L0x7fffffc958,L0x7fffffc95a,L0x7fffffc95c,L0x7fffffc95e];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffc960; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffc960,L0x7fffffc962,L0x7fffffc964,L0x7fffffc966,L0x7fffffc968,L0x7fffffc96a,L0x7fffffc96c,L0x7fffffc96e];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffc970; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffc970,L0x7fffffc972,L0x7fffffc974,L0x7fffffc976,L0x7fffffc978,L0x7fffffc97a,L0x7fffffc97c,L0x7fffffc97e];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffc900; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffc900,L0x7fffffc902,L0x7fffffc904,L0x7fffffc906,L0x7fffffc908,L0x7fffffc90a,L0x7fffffc90c,L0x7fffffc90e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffc8f0; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffc8f0,L0x7fffffc8f2,L0x7fffffc8f4,L0x7fffffc8f6,L0x7fffffc8f8,L0x7fffffc8fa,L0x7fffffc8fc,L0x7fffffc8fe];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_50@sint16[8], %fa1_50@sint16[8], %fa2_50@sint16[8], %fa3_50@sint16[8], %fa4_50@sint16[8], %fa5_50@sint16[8], %fa6_50@sint16[8], %fa7_50@sint16[8], %fa8_50@sint16[8] :
    %fa0_50 = %v0 /\ %fa1_50 = %v1 /\ %fa2_50 = %v2 /\ %fa3_50 = %v3 /\
    %fa4_50 = %v4 /\ %fa5_50 = %v5 /\ %fa6_50 = %v6 /\ %fa7_50 = %v7 /\
    %fa8_50 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_50 = %v0 /\ %fa1_50 = %v1 /\ %fa2_50 = %v2 /\ %fa3_50 = %v3 /\
    %fa4_50 = %v4 /\ %fa5_50 = %v5 /\ %fa6_50 = %v6 /\ %fa7_50 = %v7 /\
    %fa8_50 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 41 *)
    %fa0_50 = %v0 /\ %fa1_50 = %v1 /\ %fa2_50 = %v2 /\ %fa3_50 = %v3 /\
    %fa4_50 = %v4 /\ %fa5_50 = %v5 /\ %fa6_50 = %v6 /\ %fa7_50 = %v7 /\
    %fa8_50 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_50 = [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] /\
    %fa1_50 = [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] /\
    %fa2_50 = [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] /\
    %fa3_50 = [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] /\
    %fa4_50 = [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] /\
    %fa5_50 = [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] /\
    %fa6_50 = [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] /\
    %fa7_50 = [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] /\
    %fa8_50 = [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_50 = %v0 /\ %fa1_50 = %v1 /\ %fa2_50 = %v2 /\ %fa3_50 = %v3 /\
    %fa4_50 = %v4 /\ %fa5_50 = %v5 /\ %fa6_50 = %v6 /\ %fa7_50 = %v7 /\
    %fa8_50 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_50 = [arr5000, arr5001, arr5002, arr5003, arr5004, arr5005, arr5006, arr5007] /\
    %fa1_50 = [arr5080, arr5081, arr5082, arr5083, arr5084, arr5085, arr5086, arr5087] /\
    %fa2_50 = [arr5070, arr5071, arr5072, arr5073, arr5074, arr5075, arr5076, arr5077] /\
    %fa3_50 = [arr5060, arr5061, arr5062, arr5063, arr5064, arr5065, arr5066, arr5067] /\
    %fa4_50 = [arr5050, arr5051, arr5052, arr5053, arr5054, arr5055, arr5056, arr5057] /\
    %fa5_50 = [arr5040, arr5041, arr5042, arr5043, arr5044, arr5045, arr5046, arr5047] /\
    %fa6_50 = [arr5030, arr5031, arr5032, arr5033, arr5034, arr5035, arr5036, arr5037] /\
    %fa7_50 = [arr5020, arr5021, arr5022, arr5023, arr5024, arr5025, arr5026, arr5027] /\
    %fa8_50 = [arr5010, arr5011, arr5012, arr5013, arr5014, arr5015, arr5016, arr5017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_50 + %fa3_50 + %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_50 + %fa3_50 + %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_50 + %fa3_50 + %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_50 + %fa3_50 + %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_50 - %fa3_50 - %fa6_50
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_50 - %fa6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_50 - %fa4_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_50 - %fa4_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_50 - %fa4_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_50 - %fa4_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_50 - %fa5_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_50 - %fa5_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_50 - %fa5_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_50 - %fa5_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 42 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        %fa3_50 +
        %fa6_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_50 +
        %fa7_50 +
        %fa4_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_50 +
        %fa2_50 +
        %fa5_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_50 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_50 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 41], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 41]];


ghost %fb0_50@sint16[8], %fb1_50@sint16[8], %fb2_50@sint16[8], %fb3_50@sint16[8], %fb4_50@sint16[8], %fb5_50@sint16[8], %fb6_50@sint16[8], %fb7_50@sint16[8], %fb8_50@sint16[8] :
    %fb0_50 = %v17 /\ %fb1_50 = %v16 /\ %fb2_50 = %v0  /\ %fb3_50 = %v6  /\
    %fb4_50 = %v3  /\ %fb5_50 = %v7  /\ %fb6_50 = %v19 /\ %fb7_50 = %v1  /\ %fb8_50 = %v2
  &&
    %fb0_50 = %v17 /\ %fb1_50 = %v16 /\ %fb2_50 = %v0  /\ %fb3_50 = %v6  /\
    %fb4_50 = %v3  /\ %fb5_50 = %v7  /\ %fb6_50 = %v19 /\ %fb7_50 = %v1  /\ %fb8_50 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_50 + %fb6_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_50 + %fb6_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_50 + %fb6_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_50 + %fb6_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_50 - %fb6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_50 - %fb6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_50 - %fb6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_50 - %fb6_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_50@sint16[8] : %b1i1_50 = %v1 && %b1i1_50 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_50@sint16[8] : %b1i2_50 = %v5 && %b1i2_50 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_50 + %b1i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_50 + %b1i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_50 + %b1i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_50 + %b1i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_50 - %b1i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_50 - %b1i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_50 - %b1i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_50 - %b1i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_50@sint16[8] : %b2i1_50 = %v3 && %b2i1_50 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_50@sint16[8] : %b2i2_50 = %v1 && %b2i2_50 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_50 + %b2i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_50 + %b2i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_50 + %b2i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_50 + %b2i2_50
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_50 - %b2i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_50 - %b2i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_50 - %b2i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_50 - %b2i2_50)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 43 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        %fa1_50 +
        %fa2_50 +
        %fa3_50 +
        %fa4_50 +
        %fa5_50 +
        %fa6_50 +
        %fa7_50 +
        %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_50 +
        %fa3_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_50 +
        %fa6_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_50 +
        %fa3_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_50 +
        %fa6_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_50 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_50 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_50 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_50 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_50 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_50 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_50 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_50 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_50
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 41]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 41], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 41]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffc8f0; PC = 0x55555514f8 *)
mov [L0x7fffffc8f0,L0x7fffffc8f2,L0x7fffffc8f4,L0x7fffffc8f6,L0x7fffffc8f8,L0x7fffffc8fa,L0x7fffffc8fc,L0x7fffffc8fe] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffc900; PC = 0x5555551500 *)
mov [L0x7fffffc900,L0x7fffffc902,L0x7fffffc904,L0x7fffffc906,L0x7fffffc908,L0x7fffffc90a,L0x7fffffc90c,L0x7fffffc90e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffc910; PC = 0x5555551508 *)
mov [L0x7fffffc910,L0x7fffffc912,L0x7fffffc914,L0x7fffffc916,L0x7fffffc918,L0x7fffffc91a,L0x7fffffc91c,L0x7fffffc91e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffc920; PC = 0x5555551510 *)
mov [L0x7fffffc920,L0x7fffffc922,L0x7fffffc924,L0x7fffffc926,L0x7fffffc928,L0x7fffffc92a,L0x7fffffc92c,L0x7fffffc92e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffc930; PC = 0x5555551518 *)
mov [L0x7fffffc930,L0x7fffffc932,L0x7fffffc934,L0x7fffffc936,L0x7fffffc938,L0x7fffffc93a,L0x7fffffc93c,L0x7fffffc93e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffc940; PC = 0x5555551520 *)
mov [L0x7fffffc940,L0x7fffffc942,L0x7fffffc944,L0x7fffffc946,L0x7fffffc948,L0x7fffffc94a,L0x7fffffc94c,L0x7fffffc94e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffc950; PC = 0x5555551528 *)
mov [L0x7fffffc950,L0x7fffffc952,L0x7fffffc954,L0x7fffffc956,L0x7fffffc958,L0x7fffffc95a,L0x7fffffc95c,L0x7fffffc95e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffc960; PC = 0x5555551530 *)
mov [L0x7fffffc960,L0x7fffffc962,L0x7fffffc964,L0x7fffffc966,L0x7fffffc968,L0x7fffffc96a,L0x7fffffc96c,L0x7fffffc96e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffc970; PC = 0x5555551538 *)
mov [L0x7fffffc970,L0x7fffffc972,L0x7fffffc974,L0x7fffffc976,L0x7fffffc978,L0x7fffffc97a,L0x7fffffc97c,L0x7fffffc97e] %v0;

cut (* 44 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffc9a0; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffc9a0,L0x7fffffc9a2,L0x7fffffc9a4,L0x7fffffc9a6,L0x7fffffc9a8,L0x7fffffc9aa,L0x7fffffc9ac,L0x7fffffc9ae];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffc9b0; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffc9b0,L0x7fffffc9b2,L0x7fffffc9b4,L0x7fffffc9b6,L0x7fffffc9b8,L0x7fffffc9ba,L0x7fffffc9bc,L0x7fffffc9be];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffc9c0; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffc9c0,L0x7fffffc9c2,L0x7fffffc9c4,L0x7fffffc9c6,L0x7fffffc9c8,L0x7fffffc9ca,L0x7fffffc9cc,L0x7fffffc9ce];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffc9d0; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffc9d0,L0x7fffffc9d2,L0x7fffffc9d4,L0x7fffffc9d6,L0x7fffffc9d8,L0x7fffffc9da,L0x7fffffc9dc,L0x7fffffc9de];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffc9e0; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffc9e0,L0x7fffffc9e2,L0x7fffffc9e4,L0x7fffffc9e6,L0x7fffffc9e8,L0x7fffffc9ea,L0x7fffffc9ec,L0x7fffffc9ee];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffc9f0; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffc9f0,L0x7fffffc9f2,L0x7fffffc9f4,L0x7fffffc9f6,L0x7fffffc9f8,L0x7fffffc9fa,L0x7fffffc9fc,L0x7fffffc9fe];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffca00; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffca00,L0x7fffffca02,L0x7fffffca04,L0x7fffffca06,L0x7fffffca08,L0x7fffffca0a,L0x7fffffca0c,L0x7fffffca0e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffc980; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffc980,L0x7fffffc982,L0x7fffffc984,L0x7fffffc986,L0x7fffffc988,L0x7fffffc98a,L0x7fffffc98c,L0x7fffffc98e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffc990; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffc990,L0x7fffffc992,L0x7fffffc994,L0x7fffffc996,L0x7fffffc998,L0x7fffffc99a,L0x7fffffc99c,L0x7fffffc99e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_51@sint16[8], %fa1_51@sint16[8], %fa2_51@sint16[8], %fa3_51@sint16[8], %fa4_51@sint16[8], %fa5_51@sint16[8], %fa6_51@sint16[8], %fa7_51@sint16[8], %fa8_51@sint16[8] :
    %fa0_51 = %v0 /\ %fa1_51 = %v1 /\ %fa2_51 = %v2 /\ %fa3_51 = %v3 /\
    %fa4_51 = %v4 /\ %fa5_51 = %v5 /\ %fa6_51 = %v6 /\ %fa7_51 = %v7 /\
    %fa8_51 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_51 = %v0 /\ %fa1_51 = %v1 /\ %fa2_51 = %v2 /\ %fa3_51 = %v3 /\
    %fa4_51 = %v4 /\ %fa5_51 = %v5 /\ %fa6_51 = %v6 /\ %fa7_51 = %v7 /\
    %fa8_51 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 45 *)
    %fa0_51 = %v0 /\ %fa1_51 = %v1 /\ %fa2_51 = %v2 /\ %fa3_51 = %v3 /\
    %fa4_51 = %v4 /\ %fa5_51 = %v5 /\ %fa6_51 = %v6 /\ %fa7_51 = %v7 /\
    %fa8_51 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_51 = [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] /\
    %fa1_51 = [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] /\
    %fa2_51 = [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] /\
    %fa3_51 = [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] /\
    %fa4_51 = [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] /\
    %fa5_51 = [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] /\
    %fa6_51 = [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] /\
    %fa7_51 = [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] /\
    %fa8_51 = [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_51 = %v0 /\ %fa1_51 = %v1 /\ %fa2_51 = %v2 /\ %fa3_51 = %v3 /\
    %fa4_51 = %v4 /\ %fa5_51 = %v5 /\ %fa6_51 = %v6 /\ %fa7_51 = %v7 /\
    %fa8_51 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_51 = [arr5100, arr5101, arr5102, arr5103, arr5104, arr5105, arr5106, arr5107] /\
    %fa1_51 = [arr5180, arr5181, arr5182, arr5183, arr5184, arr5185, arr5186, arr5187] /\
    %fa2_51 = [arr5170, arr5171, arr5172, arr5173, arr5174, arr5175, arr5176, arr5177] /\
    %fa3_51 = [arr5160, arr5161, arr5162, arr5163, arr5164, arr5165, arr5166, arr5167] /\
    %fa4_51 = [arr5150, arr5151, arr5152, arr5153, arr5154, arr5155, arr5156, arr5157] /\
    %fa5_51 = [arr5140, arr5141, arr5142, arr5143, arr5144, arr5145, arr5146, arr5147] /\
    %fa6_51 = [arr5130, arr5131, arr5132, arr5133, arr5134, arr5135, arr5136, arr5137] /\
    %fa7_51 = [arr5120, arr5121, arr5122, arr5123, arr5124, arr5125, arr5126, arr5127] /\
    %fa8_51 = [arr5110, arr5111, arr5112, arr5113, arr5114, arr5115, arr5116, arr5117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_51 + %fa3_51 + %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_51 + %fa3_51 + %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_51 + %fa3_51 + %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_51 + %fa3_51 + %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_51 - %fa3_51 - %fa6_51
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_51 - %fa6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_51 - %fa4_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_51 - %fa4_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_51 - %fa4_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_51 - %fa4_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_51 - %fa5_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_51 - %fa5_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_51 - %fa5_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_51 - %fa5_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 46 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        %fa3_51 +
        %fa6_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_51 +
        %fa7_51 +
        %fa4_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_51 +
        %fa2_51 +
        %fa5_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_51 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_51 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 45], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 45]];


ghost %fb0_51@sint16[8], %fb1_51@sint16[8], %fb2_51@sint16[8], %fb3_51@sint16[8], %fb4_51@sint16[8], %fb5_51@sint16[8], %fb6_51@sint16[8], %fb7_51@sint16[8], %fb8_51@sint16[8] :
    %fb0_51 = %v17 /\ %fb1_51 = %v16 /\ %fb2_51 = %v0  /\ %fb3_51 = %v6  /\
    %fb4_51 = %v3  /\ %fb5_51 = %v7  /\ %fb6_51 = %v19 /\ %fb7_51 = %v1  /\ %fb8_51 = %v2
  &&
    %fb0_51 = %v17 /\ %fb1_51 = %v16 /\ %fb2_51 = %v0  /\ %fb3_51 = %v6  /\
    %fb4_51 = %v3  /\ %fb5_51 = %v7  /\ %fb6_51 = %v19 /\ %fb7_51 = %v1  /\ %fb8_51 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_51 + %fb6_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_51 + %fb6_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_51 + %fb6_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_51 + %fb6_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_51 - %fb6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_51 - %fb6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_51 - %fb6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_51 - %fb6_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_51@sint16[8] : %b1i1_51 = %v1 && %b1i1_51 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_51@sint16[8] : %b1i2_51 = %v5 && %b1i2_51 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_51 + %b1i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_51 + %b1i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_51 + %b1i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_51 + %b1i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_51 - %b1i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_51 - %b1i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_51 - %b1i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_51 - %b1i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_51@sint16[8] : %b2i1_51 = %v3 && %b2i1_51 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_51@sint16[8] : %b2i2_51 = %v1 && %b2i2_51 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_51 + %b2i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_51 + %b2i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_51 + %b2i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_51 + %b2i2_51
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_51 - %b2i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_51 - %b2i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_51 - %b2i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_51 - %b2i2_51)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 47 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        %fa1_51 +
        %fa2_51 +
        %fa3_51 +
        %fa4_51 +
        %fa5_51 +
        %fa6_51 +
        %fa7_51 +
        %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_51 +
        %fa3_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_51 +
        %fa6_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_51 +
        %fa3_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_51 +
        %fa6_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_51 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_51 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_51 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_51 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_51 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_51 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_51 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_51 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_51
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 45]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 45], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 45]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffc980; PC = 0x5555551590 *)
mov [L0x7fffffc980,L0x7fffffc982,L0x7fffffc984,L0x7fffffc986,L0x7fffffc988,L0x7fffffc98a,L0x7fffffc98c,L0x7fffffc98e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffc990; PC = 0x5555551598 *)
mov [L0x7fffffc990,L0x7fffffc992,L0x7fffffc994,L0x7fffffc996,L0x7fffffc998,L0x7fffffc99a,L0x7fffffc99c,L0x7fffffc99e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffc9a0; PC = 0x55555515a0 *)
mov [L0x7fffffc9a0,L0x7fffffc9a2,L0x7fffffc9a4,L0x7fffffc9a6,L0x7fffffc9a8,L0x7fffffc9aa,L0x7fffffc9ac,L0x7fffffc9ae] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffc9b0; PC = 0x55555515a8 *)
mov [L0x7fffffc9b0,L0x7fffffc9b2,L0x7fffffc9b4,L0x7fffffc9b6,L0x7fffffc9b8,L0x7fffffc9ba,L0x7fffffc9bc,L0x7fffffc9be] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffc9c0; PC = 0x55555515b0 *)
mov [L0x7fffffc9c0,L0x7fffffc9c2,L0x7fffffc9c4,L0x7fffffc9c6,L0x7fffffc9c8,L0x7fffffc9ca,L0x7fffffc9cc,L0x7fffffc9ce] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffc9d0; PC = 0x55555515b8 *)
mov [L0x7fffffc9d0,L0x7fffffc9d2,L0x7fffffc9d4,L0x7fffffc9d6,L0x7fffffc9d8,L0x7fffffc9da,L0x7fffffc9dc,L0x7fffffc9de] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffc9e0; PC = 0x55555515c0 *)
mov [L0x7fffffc9e0,L0x7fffffc9e2,L0x7fffffc9e4,L0x7fffffc9e6,L0x7fffffc9e8,L0x7fffffc9ea,L0x7fffffc9ec,L0x7fffffc9ee] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffc9f0; PC = 0x55555515c8 *)
mov [L0x7fffffc9f0,L0x7fffffc9f2,L0x7fffffc9f4,L0x7fffffc9f6,L0x7fffffc9f8,L0x7fffffc9fa,L0x7fffffc9fc,L0x7fffffc9fe] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffca00; PC = 0x55555515d0 *)
mov [L0x7fffffca00,L0x7fffffca02,L0x7fffffca04,L0x7fffffca06,L0x7fffffca08,L0x7fffffca0a,L0x7fffffca0c,L0x7fffffca0e] %v0;

cut (* 48 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffca30; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffca30,L0x7fffffca32,L0x7fffffca34,L0x7fffffca36,L0x7fffffca38,L0x7fffffca3a,L0x7fffffca3c,L0x7fffffca3e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffca40; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffca40,L0x7fffffca42,L0x7fffffca44,L0x7fffffca46,L0x7fffffca48,L0x7fffffca4a,L0x7fffffca4c,L0x7fffffca4e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffca50; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffca50,L0x7fffffca52,L0x7fffffca54,L0x7fffffca56,L0x7fffffca58,L0x7fffffca5a,L0x7fffffca5c,L0x7fffffca5e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffca60; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffca60,L0x7fffffca62,L0x7fffffca64,L0x7fffffca66,L0x7fffffca68,L0x7fffffca6a,L0x7fffffca6c,L0x7fffffca6e];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffca70; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffca70,L0x7fffffca72,L0x7fffffca74,L0x7fffffca76,L0x7fffffca78,L0x7fffffca7a,L0x7fffffca7c,L0x7fffffca7e];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffca80; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffca80,L0x7fffffca82,L0x7fffffca84,L0x7fffffca86,L0x7fffffca88,L0x7fffffca8a,L0x7fffffca8c,L0x7fffffca8e];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffca90; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffca90,L0x7fffffca92,L0x7fffffca94,L0x7fffffca96,L0x7fffffca98,L0x7fffffca9a,L0x7fffffca9c,L0x7fffffca9e];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffca20; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffca20,L0x7fffffca22,L0x7fffffca24,L0x7fffffca26,L0x7fffffca28,L0x7fffffca2a,L0x7fffffca2c,L0x7fffffca2e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffca10; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffca10,L0x7fffffca12,L0x7fffffca14,L0x7fffffca16,L0x7fffffca18,L0x7fffffca1a,L0x7fffffca1c,L0x7fffffca1e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_60@sint16[8], %fa1_60@sint16[8], %fa2_60@sint16[8], %fa3_60@sint16[8], %fa4_60@sint16[8], %fa5_60@sint16[8], %fa6_60@sint16[8], %fa7_60@sint16[8], %fa8_60@sint16[8] :
    %fa0_60 = %v0 /\ %fa1_60 = %v1 /\ %fa2_60 = %v2 /\ %fa3_60 = %v3 /\
    %fa4_60 = %v4 /\ %fa5_60 = %v5 /\ %fa6_60 = %v6 /\ %fa7_60 = %v7 /\
    %fa8_60 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_60 = %v0 /\ %fa1_60 = %v1 /\ %fa2_60 = %v2 /\ %fa3_60 = %v3 /\
    %fa4_60 = %v4 /\ %fa5_60 = %v5 /\ %fa6_60 = %v6 /\ %fa7_60 = %v7 /\
    %fa8_60 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 49 *)
    %fa0_60 = %v0 /\ %fa1_60 = %v1 /\ %fa2_60 = %v2 /\ %fa3_60 = %v3 /\
    %fa4_60 = %v4 /\ %fa5_60 = %v5 /\ %fa6_60 = %v6 /\ %fa7_60 = %v7 /\
    %fa8_60 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_60 = [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] /\
    %fa1_60 = [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] /\
    %fa2_60 = [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] /\
    %fa3_60 = [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] /\
    %fa4_60 = [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] /\
    %fa5_60 = [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] /\
    %fa6_60 = [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] /\
    %fa7_60 = [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] /\
    %fa8_60 = [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_60 = %v0 /\ %fa1_60 = %v1 /\ %fa2_60 = %v2 /\ %fa3_60 = %v3 /\
    %fa4_60 = %v4 /\ %fa5_60 = %v5 /\ %fa6_60 = %v6 /\ %fa7_60 = %v7 /\
    %fa8_60 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_60 = [arr6000, arr6001, arr6002, arr6003, arr6004, arr6005, arr6006, arr6007] /\
    %fa1_60 = [arr6080, arr6081, arr6082, arr6083, arr6084, arr6085, arr6086, arr6087] /\
    %fa2_60 = [arr6070, arr6071, arr6072, arr6073, arr6074, arr6075, arr6076, arr6077] /\
    %fa3_60 = [arr6060, arr6061, arr6062, arr6063, arr6064, arr6065, arr6066, arr6067] /\
    %fa4_60 = [arr6050, arr6051, arr6052, arr6053, arr6054, arr6055, arr6056, arr6057] /\
    %fa5_60 = [arr6040, arr6041, arr6042, arr6043, arr6044, arr6045, arr6046, arr6047] /\
    %fa6_60 = [arr6030, arr6031, arr6032, arr6033, arr6034, arr6035, arr6036, arr6037] /\
    %fa7_60 = [arr6020, arr6021, arr6022, arr6023, arr6024, arr6025, arr6026, arr6027] /\
    %fa8_60 = [arr6010, arr6011, arr6012, arr6013, arr6014, arr6015, arr6016, arr6017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_60 + %fa3_60 + %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_60 + %fa3_60 + %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_60 + %fa3_60 + %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_60 + %fa3_60 + %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_60 - %fa3_60 - %fa6_60
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_60 - %fa6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_60 - %fa4_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_60 - %fa4_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_60 - %fa4_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_60 - %fa4_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_60 - %fa5_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_60 - %fa5_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_60 - %fa5_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_60 - %fa5_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 50 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        %fa3_60 +
        %fa6_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_60 +
        %fa7_60 +
        %fa4_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_60 +
        %fa2_60 +
        %fa5_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_60 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_60 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 49], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 49]];


ghost %fb0_60@sint16[8], %fb1_60@sint16[8], %fb2_60@sint16[8], %fb3_60@sint16[8], %fb4_60@sint16[8], %fb5_60@sint16[8], %fb6_60@sint16[8], %fb7_60@sint16[8], %fb8_60@sint16[8] :
    %fb0_60 = %v17 /\ %fb1_60 = %v16 /\ %fb2_60 = %v0  /\ %fb3_60 = %v6  /\
    %fb4_60 = %v3  /\ %fb5_60 = %v7  /\ %fb6_60 = %v19 /\ %fb7_60 = %v1  /\ %fb8_60 = %v2
  &&
    %fb0_60 = %v17 /\ %fb1_60 = %v16 /\ %fb2_60 = %v0  /\ %fb3_60 = %v6  /\
    %fb4_60 = %v3  /\ %fb5_60 = %v7  /\ %fb6_60 = %v19 /\ %fb7_60 = %v1  /\ %fb8_60 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_60 + %fb6_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_60 + %fb6_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_60 + %fb6_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_60 + %fb6_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_60 - %fb6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_60 - %fb6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_60 - %fb6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_60 - %fb6_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_60@sint16[8] : %b1i1_60 = %v1 && %b1i1_60 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_60@sint16[8] : %b1i2_60 = %v5 && %b1i2_60 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_60 + %b1i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_60 + %b1i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_60 + %b1i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_60 + %b1i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_60 - %b1i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_60 - %b1i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_60 - %b1i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_60 - %b1i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_60@sint16[8] : %b2i1_60 = %v3 && %b2i1_60 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_60@sint16[8] : %b2i2_60 = %v1 && %b2i2_60 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_60 + %b2i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_60 + %b2i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_60 + %b2i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_60 + %b2i2_60
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_60 - %b2i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_60 - %b2i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_60 - %b2i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_60 - %b2i2_60)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 51 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        %fa1_60 +
        %fa2_60 +
        %fa3_60 +
        %fa4_60 +
        %fa5_60 +
        %fa6_60 +
        %fa7_60 +
        %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_60 +
        %fa3_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_60 +
        %fa6_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_60 +
        %fa3_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_60 +
        %fa6_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_60 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_60 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_60 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_60 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_60 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_60 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_60 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_60 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_60
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 49]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 49], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 49]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffca10; PC = 0x55555514f8 *)
mov [L0x7fffffca10,L0x7fffffca12,L0x7fffffca14,L0x7fffffca16,L0x7fffffca18,L0x7fffffca1a,L0x7fffffca1c,L0x7fffffca1e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffca20; PC = 0x5555551500 *)
mov [L0x7fffffca20,L0x7fffffca22,L0x7fffffca24,L0x7fffffca26,L0x7fffffca28,L0x7fffffca2a,L0x7fffffca2c,L0x7fffffca2e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffca30; PC = 0x5555551508 *)
mov [L0x7fffffca30,L0x7fffffca32,L0x7fffffca34,L0x7fffffca36,L0x7fffffca38,L0x7fffffca3a,L0x7fffffca3c,L0x7fffffca3e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffca40; PC = 0x5555551510 *)
mov [L0x7fffffca40,L0x7fffffca42,L0x7fffffca44,L0x7fffffca46,L0x7fffffca48,L0x7fffffca4a,L0x7fffffca4c,L0x7fffffca4e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffca50; PC = 0x5555551518 *)
mov [L0x7fffffca50,L0x7fffffca52,L0x7fffffca54,L0x7fffffca56,L0x7fffffca58,L0x7fffffca5a,L0x7fffffca5c,L0x7fffffca5e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffca60; PC = 0x5555551520 *)
mov [L0x7fffffca60,L0x7fffffca62,L0x7fffffca64,L0x7fffffca66,L0x7fffffca68,L0x7fffffca6a,L0x7fffffca6c,L0x7fffffca6e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffca70; PC = 0x5555551528 *)
mov [L0x7fffffca70,L0x7fffffca72,L0x7fffffca74,L0x7fffffca76,L0x7fffffca78,L0x7fffffca7a,L0x7fffffca7c,L0x7fffffca7e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffca80; PC = 0x5555551530 *)
mov [L0x7fffffca80,L0x7fffffca82,L0x7fffffca84,L0x7fffffca86,L0x7fffffca88,L0x7fffffca8a,L0x7fffffca8c,L0x7fffffca8e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffca90; PC = 0x5555551538 *)
mov [L0x7fffffca90,L0x7fffffca92,L0x7fffffca94,L0x7fffffca96,L0x7fffffca98,L0x7fffffca9a,L0x7fffffca9c,L0x7fffffca9e] %v0;

cut (* 52 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffcac0; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffcac0,L0x7fffffcac2,L0x7fffffcac4,L0x7fffffcac6,L0x7fffffcac8,L0x7fffffcaca,L0x7fffffcacc,L0x7fffffcace];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffcad0; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffcad0,L0x7fffffcad2,L0x7fffffcad4,L0x7fffffcad6,L0x7fffffcad8,L0x7fffffcada,L0x7fffffcadc,L0x7fffffcade];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffcae0; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffcae0,L0x7fffffcae2,L0x7fffffcae4,L0x7fffffcae6,L0x7fffffcae8,L0x7fffffcaea,L0x7fffffcaec,L0x7fffffcaee];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffcaf0; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffcaf0,L0x7fffffcaf2,L0x7fffffcaf4,L0x7fffffcaf6,L0x7fffffcaf8,L0x7fffffcafa,L0x7fffffcafc,L0x7fffffcafe];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffcb00; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffcb00,L0x7fffffcb02,L0x7fffffcb04,L0x7fffffcb06,L0x7fffffcb08,L0x7fffffcb0a,L0x7fffffcb0c,L0x7fffffcb0e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffcb10; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffcb10,L0x7fffffcb12,L0x7fffffcb14,L0x7fffffcb16,L0x7fffffcb18,L0x7fffffcb1a,L0x7fffffcb1c,L0x7fffffcb1e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffcb20; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffcb20,L0x7fffffcb22,L0x7fffffcb24,L0x7fffffcb26,L0x7fffffcb28,L0x7fffffcb2a,L0x7fffffcb2c,L0x7fffffcb2e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffcaa0; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffcaa0,L0x7fffffcaa2,L0x7fffffcaa4,L0x7fffffcaa6,L0x7fffffcaa8,L0x7fffffcaaa,L0x7fffffcaac,L0x7fffffcaae];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffcab0; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffcab0,L0x7fffffcab2,L0x7fffffcab4,L0x7fffffcab6,L0x7fffffcab8,L0x7fffffcaba,L0x7fffffcabc,L0x7fffffcabe];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_61@sint16[8], %fa1_61@sint16[8], %fa2_61@sint16[8], %fa3_61@sint16[8], %fa4_61@sint16[8], %fa5_61@sint16[8], %fa6_61@sint16[8], %fa7_61@sint16[8], %fa8_61@sint16[8] :
    %fa0_61 = %v0 /\ %fa1_61 = %v1 /\ %fa2_61 = %v2 /\ %fa3_61 = %v3 /\
    %fa4_61 = %v4 /\ %fa5_61 = %v5 /\ %fa6_61 = %v6 /\ %fa7_61 = %v7 /\
    %fa8_61 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_61 = %v0 /\ %fa1_61 = %v1 /\ %fa2_61 = %v2 /\ %fa3_61 = %v3 /\
    %fa4_61 = %v4 /\ %fa5_61 = %v5 /\ %fa6_61 = %v6 /\ %fa7_61 = %v7 /\
    %fa8_61 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 53 *)
    %fa0_61 = %v0 /\ %fa1_61 = %v1 /\ %fa2_61 = %v2 /\ %fa3_61 = %v3 /\
    %fa4_61 = %v4 /\ %fa5_61 = %v5 /\ %fa6_61 = %v6 /\ %fa7_61 = %v7 /\
    %fa8_61 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_61 = [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] /\
    %fa1_61 = [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] /\
    %fa2_61 = [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] /\
    %fa3_61 = [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] /\
    %fa4_61 = [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] /\
    %fa5_61 = [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] /\
    %fa6_61 = [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] /\
    %fa7_61 = [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] /\
    %fa8_61 = [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_61 = %v0 /\ %fa1_61 = %v1 /\ %fa2_61 = %v2 /\ %fa3_61 = %v3 /\
    %fa4_61 = %v4 /\ %fa5_61 = %v5 /\ %fa6_61 = %v6 /\ %fa7_61 = %v7 /\
    %fa8_61 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_61 = [arr6100, arr6101, arr6102, arr6103, arr6104, arr6105, arr6106, arr6107] /\
    %fa1_61 = [arr6180, arr6181, arr6182, arr6183, arr6184, arr6185, arr6186, arr6187] /\
    %fa2_61 = [arr6170, arr6171, arr6172, arr6173, arr6174, arr6175, arr6176, arr6177] /\
    %fa3_61 = [arr6160, arr6161, arr6162, arr6163, arr6164, arr6165, arr6166, arr6167] /\
    %fa4_61 = [arr6150, arr6151, arr6152, arr6153, arr6154, arr6155, arr6156, arr6157] /\
    %fa5_61 = [arr6140, arr6141, arr6142, arr6143, arr6144, arr6145, arr6146, arr6147] /\
    %fa6_61 = [arr6130, arr6131, arr6132, arr6133, arr6134, arr6135, arr6136, arr6137] /\
    %fa7_61 = [arr6120, arr6121, arr6122, arr6123, arr6124, arr6125, arr6126, arr6127] /\
    %fa8_61 = [arr6110, arr6111, arr6112, arr6113, arr6114, arr6115, arr6116, arr6117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_61 + %fa3_61 + %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_61 + %fa3_61 + %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_61 + %fa3_61 + %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_61 + %fa3_61 + %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_61 - %fa3_61 - %fa6_61
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_61 - %fa6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_61 - %fa4_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_61 - %fa4_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_61 - %fa4_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_61 - %fa4_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_61 - %fa5_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_61 - %fa5_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_61 - %fa5_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_61 - %fa5_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 54 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        %fa3_61 +
        %fa6_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_61 +
        %fa7_61 +
        %fa4_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_61 +
        %fa2_61 +
        %fa5_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_61 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_61 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 53], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 53]];


ghost %fb0_61@sint16[8], %fb1_61@sint16[8], %fb2_61@sint16[8], %fb3_61@sint16[8], %fb4_61@sint16[8], %fb5_61@sint16[8], %fb6_61@sint16[8], %fb7_61@sint16[8], %fb8_61@sint16[8] :
    %fb0_61 = %v17 /\ %fb1_61 = %v16 /\ %fb2_61 = %v0  /\ %fb3_61 = %v6  /\
    %fb4_61 = %v3  /\ %fb5_61 = %v7  /\ %fb6_61 = %v19 /\ %fb7_61 = %v1  /\ %fb8_61 = %v2
  &&
    %fb0_61 = %v17 /\ %fb1_61 = %v16 /\ %fb2_61 = %v0  /\ %fb3_61 = %v6  /\
    %fb4_61 = %v3  /\ %fb5_61 = %v7  /\ %fb6_61 = %v19 /\ %fb7_61 = %v1  /\ %fb8_61 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_61 + %fb6_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_61 + %fb6_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_61 + %fb6_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_61 + %fb6_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_61 - %fb6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_61 - %fb6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_61 - %fb6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_61 - %fb6_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_61@sint16[8] : %b1i1_61 = %v1 && %b1i1_61 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_61@sint16[8] : %b1i2_61 = %v5 && %b1i2_61 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_61 + %b1i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_61 + %b1i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_61 + %b1i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_61 + %b1i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_61 - %b1i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_61 - %b1i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_61 - %b1i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_61 - %b1i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_61@sint16[8] : %b2i1_61 = %v3 && %b2i1_61 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_61@sint16[8] : %b2i2_61 = %v1 && %b2i2_61 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_61 + %b2i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_61 + %b2i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_61 + %b2i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_61 + %b2i2_61
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_61 - %b2i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_61 - %b2i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_61 - %b2i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_61 - %b2i2_61)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 55 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        %fa1_61 +
        %fa2_61 +
        %fa3_61 +
        %fa4_61 +
        %fa5_61 +
        %fa6_61 +
        %fa7_61 +
        %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_61 +
        %fa3_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_61 +
        %fa6_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_61 +
        %fa3_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_61 +
        %fa6_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_61 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_61 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_61 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_61 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_61 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_61 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_61 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_61 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_61
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 53]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 53], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 53]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffcaa0; PC = 0x5555551590 *)
mov [L0x7fffffcaa0,L0x7fffffcaa2,L0x7fffffcaa4,L0x7fffffcaa6,L0x7fffffcaa8,L0x7fffffcaaa,L0x7fffffcaac,L0x7fffffcaae] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffcab0; PC = 0x5555551598 *)
mov [L0x7fffffcab0,L0x7fffffcab2,L0x7fffffcab4,L0x7fffffcab6,L0x7fffffcab8,L0x7fffffcaba,L0x7fffffcabc,L0x7fffffcabe] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffcac0; PC = 0x55555515a0 *)
mov [L0x7fffffcac0,L0x7fffffcac2,L0x7fffffcac4,L0x7fffffcac6,L0x7fffffcac8,L0x7fffffcaca,L0x7fffffcacc,L0x7fffffcace] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffcad0; PC = 0x55555515a8 *)
mov [L0x7fffffcad0,L0x7fffffcad2,L0x7fffffcad4,L0x7fffffcad6,L0x7fffffcad8,L0x7fffffcada,L0x7fffffcadc,L0x7fffffcade] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffcae0; PC = 0x55555515b0 *)
mov [L0x7fffffcae0,L0x7fffffcae2,L0x7fffffcae4,L0x7fffffcae6,L0x7fffffcae8,L0x7fffffcaea,L0x7fffffcaec,L0x7fffffcaee] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffcaf0; PC = 0x55555515b8 *)
mov [L0x7fffffcaf0,L0x7fffffcaf2,L0x7fffffcaf4,L0x7fffffcaf6,L0x7fffffcaf8,L0x7fffffcafa,L0x7fffffcafc,L0x7fffffcafe] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffcb00; PC = 0x55555515c0 *)
mov [L0x7fffffcb00,L0x7fffffcb02,L0x7fffffcb04,L0x7fffffcb06,L0x7fffffcb08,L0x7fffffcb0a,L0x7fffffcb0c,L0x7fffffcb0e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffcb10; PC = 0x55555515c8 *)
mov [L0x7fffffcb10,L0x7fffffcb12,L0x7fffffcb14,L0x7fffffcb16,L0x7fffffcb18,L0x7fffffcb1a,L0x7fffffcb1c,L0x7fffffcb1e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffcb20; PC = 0x55555515d0 *)
mov [L0x7fffffcb20,L0x7fffffcb22,L0x7fffffcb24,L0x7fffffcb26,L0x7fffffcb28,L0x7fffffcb2a,L0x7fffffcb2c,L0x7fffffcb2e] %v0;

cut (* 56 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffcb50; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffcb50,L0x7fffffcb52,L0x7fffffcb54,L0x7fffffcb56,L0x7fffffcb58,L0x7fffffcb5a,L0x7fffffcb5c,L0x7fffffcb5e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffcb60; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffcb60,L0x7fffffcb62,L0x7fffffcb64,L0x7fffffcb66,L0x7fffffcb68,L0x7fffffcb6a,L0x7fffffcb6c,L0x7fffffcb6e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffcb70; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffcb70,L0x7fffffcb72,L0x7fffffcb74,L0x7fffffcb76,L0x7fffffcb78,L0x7fffffcb7a,L0x7fffffcb7c,L0x7fffffcb7e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffcb80; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffcb80,L0x7fffffcb82,L0x7fffffcb84,L0x7fffffcb86,L0x7fffffcb88,L0x7fffffcb8a,L0x7fffffcb8c,L0x7fffffcb8e];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffcb90; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffcb90,L0x7fffffcb92,L0x7fffffcb94,L0x7fffffcb96,L0x7fffffcb98,L0x7fffffcb9a,L0x7fffffcb9c,L0x7fffffcb9e];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffcba0; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffcba0,L0x7fffffcba2,L0x7fffffcba4,L0x7fffffcba6,L0x7fffffcba8,L0x7fffffcbaa,L0x7fffffcbac,L0x7fffffcbae];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffcbb0; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffcbb0,L0x7fffffcbb2,L0x7fffffcbb4,L0x7fffffcbb6,L0x7fffffcbb8,L0x7fffffcbba,L0x7fffffcbbc,L0x7fffffcbbe];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffcb40; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffcb40,L0x7fffffcb42,L0x7fffffcb44,L0x7fffffcb46,L0x7fffffcb48,L0x7fffffcb4a,L0x7fffffcb4c,L0x7fffffcb4e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffcb30; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffcb30,L0x7fffffcb32,L0x7fffffcb34,L0x7fffffcb36,L0x7fffffcb38,L0x7fffffcb3a,L0x7fffffcb3c,L0x7fffffcb3e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_70@sint16[8], %fa1_70@sint16[8], %fa2_70@sint16[8], %fa3_70@sint16[8], %fa4_70@sint16[8], %fa5_70@sint16[8], %fa6_70@sint16[8], %fa7_70@sint16[8], %fa8_70@sint16[8] :
    %fa0_70 = %v0 /\ %fa1_70 = %v1 /\ %fa2_70 = %v2 /\ %fa3_70 = %v3 /\
    %fa4_70 = %v4 /\ %fa5_70 = %v5 /\ %fa6_70 = %v6 /\ %fa7_70 = %v7 /\
    %fa8_70 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_70 = %v0 /\ %fa1_70 = %v1 /\ %fa2_70 = %v2 /\ %fa3_70 = %v3 /\
    %fa4_70 = %v4 /\ %fa5_70 = %v5 /\ %fa6_70 = %v6 /\ %fa7_70 = %v7 /\
    %fa8_70 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 57 *)
    %fa0_70 = %v0 /\ %fa1_70 = %v1 /\ %fa2_70 = %v2 /\ %fa3_70 = %v3 /\
    %fa4_70 = %v4 /\ %fa5_70 = %v5 /\ %fa6_70 = %v6 /\ %fa7_70 = %v7 /\
    %fa8_70 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_70 = [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] /\
    %fa1_70 = [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] /\
    %fa2_70 = [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] /\
    %fa3_70 = [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] /\
    %fa4_70 = [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] /\
    %fa5_70 = [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] /\
    %fa6_70 = [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] /\
    %fa7_70 = [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] /\
    %fa8_70 = [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_70 = %v0 /\ %fa1_70 = %v1 /\ %fa2_70 = %v2 /\ %fa3_70 = %v3 /\
    %fa4_70 = %v4 /\ %fa5_70 = %v5 /\ %fa6_70 = %v6 /\ %fa7_70 = %v7 /\
    %fa8_70 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_70 = [arr7000, arr7001, arr7002, arr7003, arr7004, arr7005, arr7006, arr7007] /\
    %fa1_70 = [arr7080, arr7081, arr7082, arr7083, arr7084, arr7085, arr7086, arr7087] /\
    %fa2_70 = [arr7070, arr7071, arr7072, arr7073, arr7074, arr7075, arr7076, arr7077] /\
    %fa3_70 = [arr7060, arr7061, arr7062, arr7063, arr7064, arr7065, arr7066, arr7067] /\
    %fa4_70 = [arr7050, arr7051, arr7052, arr7053, arr7054, arr7055, arr7056, arr7057] /\
    %fa5_70 = [arr7040, arr7041, arr7042, arr7043, arr7044, arr7045, arr7046, arr7047] /\
    %fa6_70 = [arr7030, arr7031, arr7032, arr7033, arr7034, arr7035, arr7036, arr7037] /\
    %fa7_70 = [arr7020, arr7021, arr7022, arr7023, arr7024, arr7025, arr7026, arr7027] /\
    %fa8_70 = [arr7010, arr7011, arr7012, arr7013, arr7014, arr7015, arr7016, arr7017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_70 + %fa3_70 + %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_70 + %fa3_70 + %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_70 + %fa3_70 + %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_70 + %fa3_70 + %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_70 - %fa3_70 - %fa6_70
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_70 - %fa6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_70 - %fa4_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_70 - %fa4_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_70 - %fa4_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_70 - %fa4_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_70 - %fa5_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_70 - %fa5_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_70 - %fa5_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_70 - %fa5_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 58 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        %fa3_70 +
        %fa6_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_70 +
        %fa7_70 +
        %fa4_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_70 +
        %fa2_70 +
        %fa5_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_70 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_70 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 57], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 57]];


ghost %fb0_70@sint16[8], %fb1_70@sint16[8], %fb2_70@sint16[8], %fb3_70@sint16[8], %fb4_70@sint16[8], %fb5_70@sint16[8], %fb6_70@sint16[8], %fb7_70@sint16[8], %fb8_70@sint16[8] :
    %fb0_70 = %v17 /\ %fb1_70 = %v16 /\ %fb2_70 = %v0  /\ %fb3_70 = %v6  /\
    %fb4_70 = %v3  /\ %fb5_70 = %v7  /\ %fb6_70 = %v19 /\ %fb7_70 = %v1  /\ %fb8_70 = %v2
  &&
    %fb0_70 = %v17 /\ %fb1_70 = %v16 /\ %fb2_70 = %v0  /\ %fb3_70 = %v6  /\
    %fb4_70 = %v3  /\ %fb5_70 = %v7  /\ %fb6_70 = %v19 /\ %fb7_70 = %v1  /\ %fb8_70 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_70 + %fb6_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_70 + %fb6_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_70 + %fb6_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_70 + %fb6_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_70 - %fb6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_70 - %fb6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_70 - %fb6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_70 - %fb6_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_70@sint16[8] : %b1i1_70 = %v1 && %b1i1_70 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_70@sint16[8] : %b1i2_70 = %v5 && %b1i2_70 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_70 + %b1i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_70 + %b1i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_70 + %b1i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_70 + %b1i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_70 - %b1i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_70 - %b1i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_70 - %b1i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_70 - %b1i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_70@sint16[8] : %b2i1_70 = %v3 && %b2i1_70 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_70@sint16[8] : %b2i2_70 = %v1 && %b2i2_70 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_70 + %b2i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_70 + %b2i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_70 + %b2i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_70 + %b2i2_70
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_70 - %b2i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_70 - %b2i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_70 - %b2i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_70 - %b2i2_70)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 59 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        %fa1_70 +
        %fa2_70 +
        %fa3_70 +
        %fa4_70 +
        %fa5_70 +
        %fa6_70 +
        %fa7_70 +
        %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_70 +
        %fa3_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_70 +
        %fa6_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_70 +
        %fa3_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_70 +
        %fa6_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_70 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_70 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_70 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_70 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_70 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_70 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_70 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_70 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_70
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 57]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 57], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 57]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffcb30; PC = 0x55555514f8 *)
mov [L0x7fffffcb30,L0x7fffffcb32,L0x7fffffcb34,L0x7fffffcb36,L0x7fffffcb38,L0x7fffffcb3a,L0x7fffffcb3c,L0x7fffffcb3e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffcb40; PC = 0x5555551500 *)
mov [L0x7fffffcb40,L0x7fffffcb42,L0x7fffffcb44,L0x7fffffcb46,L0x7fffffcb48,L0x7fffffcb4a,L0x7fffffcb4c,L0x7fffffcb4e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffcb50; PC = 0x5555551508 *)
mov [L0x7fffffcb50,L0x7fffffcb52,L0x7fffffcb54,L0x7fffffcb56,L0x7fffffcb58,L0x7fffffcb5a,L0x7fffffcb5c,L0x7fffffcb5e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffcb60; PC = 0x5555551510 *)
mov [L0x7fffffcb60,L0x7fffffcb62,L0x7fffffcb64,L0x7fffffcb66,L0x7fffffcb68,L0x7fffffcb6a,L0x7fffffcb6c,L0x7fffffcb6e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffcb70; PC = 0x5555551518 *)
mov [L0x7fffffcb70,L0x7fffffcb72,L0x7fffffcb74,L0x7fffffcb76,L0x7fffffcb78,L0x7fffffcb7a,L0x7fffffcb7c,L0x7fffffcb7e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffcb80; PC = 0x5555551520 *)
mov [L0x7fffffcb80,L0x7fffffcb82,L0x7fffffcb84,L0x7fffffcb86,L0x7fffffcb88,L0x7fffffcb8a,L0x7fffffcb8c,L0x7fffffcb8e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffcb90; PC = 0x5555551528 *)
mov [L0x7fffffcb90,L0x7fffffcb92,L0x7fffffcb94,L0x7fffffcb96,L0x7fffffcb98,L0x7fffffcb9a,L0x7fffffcb9c,L0x7fffffcb9e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffcba0; PC = 0x5555551530 *)
mov [L0x7fffffcba0,L0x7fffffcba2,L0x7fffffcba4,L0x7fffffcba6,L0x7fffffcba8,L0x7fffffcbaa,L0x7fffffcbac,L0x7fffffcbae] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffcbb0; PC = 0x5555551538 *)
mov [L0x7fffffcbb0,L0x7fffffcbb2,L0x7fffffcbb4,L0x7fffffcbb6,L0x7fffffcbb8,L0x7fffffcbba,L0x7fffffcbbc,L0x7fffffcbbe] %v0;

cut (* 60 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffcbe0; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffcbe0,L0x7fffffcbe2,L0x7fffffcbe4,L0x7fffffcbe6,L0x7fffffcbe8,L0x7fffffcbea,L0x7fffffcbec,L0x7fffffcbee];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffcbf0; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffcbf0,L0x7fffffcbf2,L0x7fffffcbf4,L0x7fffffcbf6,L0x7fffffcbf8,L0x7fffffcbfa,L0x7fffffcbfc,L0x7fffffcbfe];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffcc00; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffcc00,L0x7fffffcc02,L0x7fffffcc04,L0x7fffffcc06,L0x7fffffcc08,L0x7fffffcc0a,L0x7fffffcc0c,L0x7fffffcc0e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffcc10; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffcc10,L0x7fffffcc12,L0x7fffffcc14,L0x7fffffcc16,L0x7fffffcc18,L0x7fffffcc1a,L0x7fffffcc1c,L0x7fffffcc1e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffcc20; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffcc20,L0x7fffffcc22,L0x7fffffcc24,L0x7fffffcc26,L0x7fffffcc28,L0x7fffffcc2a,L0x7fffffcc2c,L0x7fffffcc2e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffcc30; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffcc30,L0x7fffffcc32,L0x7fffffcc34,L0x7fffffcc36,L0x7fffffcc38,L0x7fffffcc3a,L0x7fffffcc3c,L0x7fffffcc3e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffcc40; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffcc40,L0x7fffffcc42,L0x7fffffcc44,L0x7fffffcc46,L0x7fffffcc48,L0x7fffffcc4a,L0x7fffffcc4c,L0x7fffffcc4e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffcbc0; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffcbc0,L0x7fffffcbc2,L0x7fffffcbc4,L0x7fffffcbc6,L0x7fffffcbc8,L0x7fffffcbca,L0x7fffffcbcc,L0x7fffffcbce];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffcbd0; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffcbd0,L0x7fffffcbd2,L0x7fffffcbd4,L0x7fffffcbd6,L0x7fffffcbd8,L0x7fffffcbda,L0x7fffffcbdc,L0x7fffffcbde];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_71@sint16[8], %fa1_71@sint16[8], %fa2_71@sint16[8], %fa3_71@sint16[8], %fa4_71@sint16[8], %fa5_71@sint16[8], %fa6_71@sint16[8], %fa7_71@sint16[8], %fa8_71@sint16[8] :
    %fa0_71 = %v0 /\ %fa1_71 = %v1 /\ %fa2_71 = %v2 /\ %fa3_71 = %v3 /\
    %fa4_71 = %v4 /\ %fa5_71 = %v5 /\ %fa6_71 = %v6 /\ %fa7_71 = %v7 /\
    %fa8_71 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_71 = %v0 /\ %fa1_71 = %v1 /\ %fa2_71 = %v2 /\ %fa3_71 = %v3 /\
    %fa4_71 = %v4 /\ %fa5_71 = %v5 /\ %fa6_71 = %v6 /\ %fa7_71 = %v7 /\
    %fa8_71 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 61 *)
    %fa0_71 = %v0 /\ %fa1_71 = %v1 /\ %fa2_71 = %v2 /\ %fa3_71 = %v3 /\
    %fa4_71 = %v4 /\ %fa5_71 = %v5 /\ %fa6_71 = %v6 /\ %fa7_71 = %v7 /\
    %fa8_71 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_71 = [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] /\
    %fa1_71 = [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] /\
    %fa2_71 = [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] /\
    %fa3_71 = [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] /\
    %fa4_71 = [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] /\
    %fa5_71 = [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] /\
    %fa6_71 = [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] /\
    %fa7_71 = [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] /\
    %fa8_71 = [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_71 = %v0 /\ %fa1_71 = %v1 /\ %fa2_71 = %v2 /\ %fa3_71 = %v3 /\
    %fa4_71 = %v4 /\ %fa5_71 = %v5 /\ %fa6_71 = %v6 /\ %fa7_71 = %v7 /\
    %fa8_71 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_71 = [arr7100, arr7101, arr7102, arr7103, arr7104, arr7105, arr7106, arr7107] /\
    %fa1_71 = [arr7180, arr7181, arr7182, arr7183, arr7184, arr7185, arr7186, arr7187] /\
    %fa2_71 = [arr7170, arr7171, arr7172, arr7173, arr7174, arr7175, arr7176, arr7177] /\
    %fa3_71 = [arr7160, arr7161, arr7162, arr7163, arr7164, arr7165, arr7166, arr7167] /\
    %fa4_71 = [arr7150, arr7151, arr7152, arr7153, arr7154, arr7155, arr7156, arr7157] /\
    %fa5_71 = [arr7140, arr7141, arr7142, arr7143, arr7144, arr7145, arr7146, arr7147] /\
    %fa6_71 = [arr7130, arr7131, arr7132, arr7133, arr7134, arr7135, arr7136, arr7137] /\
    %fa7_71 = [arr7120, arr7121, arr7122, arr7123, arr7124, arr7125, arr7126, arr7127] /\
    %fa8_71 = [arr7110, arr7111, arr7112, arr7113, arr7114, arr7115, arr7116, arr7117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_71 + %fa3_71 + %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_71 + %fa3_71 + %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_71 + %fa3_71 + %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_71 + %fa3_71 + %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_71 - %fa3_71 - %fa6_71
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_71 - %fa6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_71 - %fa4_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_71 - %fa4_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_71 - %fa4_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_71 - %fa4_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_71 - %fa5_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_71 - %fa5_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_71 - %fa5_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_71 - %fa5_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 62 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        %fa3_71 +
        %fa6_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_71 +
        %fa7_71 +
        %fa4_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_71 +
        %fa2_71 +
        %fa5_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_71 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_71 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 61], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 61]];


ghost %fb0_71@sint16[8], %fb1_71@sint16[8], %fb2_71@sint16[8], %fb3_71@sint16[8], %fb4_71@sint16[8], %fb5_71@sint16[8], %fb6_71@sint16[8], %fb7_71@sint16[8], %fb8_71@sint16[8] :
    %fb0_71 = %v17 /\ %fb1_71 = %v16 /\ %fb2_71 = %v0  /\ %fb3_71 = %v6  /\
    %fb4_71 = %v3  /\ %fb5_71 = %v7  /\ %fb6_71 = %v19 /\ %fb7_71 = %v1  /\ %fb8_71 = %v2
  &&
    %fb0_71 = %v17 /\ %fb1_71 = %v16 /\ %fb2_71 = %v0  /\ %fb3_71 = %v6  /\
    %fb4_71 = %v3  /\ %fb5_71 = %v7  /\ %fb6_71 = %v19 /\ %fb7_71 = %v1  /\ %fb8_71 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_71 + %fb6_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_71 + %fb6_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_71 + %fb6_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_71 + %fb6_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_71 - %fb6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_71 - %fb6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_71 - %fb6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_71 - %fb6_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_71@sint16[8] : %b1i1_71 = %v1 && %b1i1_71 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_71@sint16[8] : %b1i2_71 = %v5 && %b1i2_71 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_71 + %b1i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_71 + %b1i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_71 + %b1i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_71 + %b1i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_71 - %b1i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_71 - %b1i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_71 - %b1i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_71 - %b1i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_71@sint16[8] : %b2i1_71 = %v3 && %b2i1_71 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_71@sint16[8] : %b2i2_71 = %v1 && %b2i2_71 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_71 + %b2i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_71 + %b2i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_71 + %b2i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_71 + %b2i2_71
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_71 - %b2i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_71 - %b2i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_71 - %b2i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_71 - %b2i2_71)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 63 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        %fa1_71 +
        %fa2_71 +
        %fa3_71 +
        %fa4_71 +
        %fa5_71 +
        %fa6_71 +
        %fa7_71 +
        %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_71 +
        %fa3_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_71 +
        %fa6_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_71 +
        %fa3_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_71 +
        %fa6_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_71 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_71 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_71 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_71 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_71 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_71 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_71 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_71 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_71
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 61]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 61], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 61]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffcbc0; PC = 0x5555551590 *)
mov [L0x7fffffcbc0,L0x7fffffcbc2,L0x7fffffcbc4,L0x7fffffcbc6,L0x7fffffcbc8,L0x7fffffcbca,L0x7fffffcbcc,L0x7fffffcbce] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffcbd0; PC = 0x5555551598 *)
mov [L0x7fffffcbd0,L0x7fffffcbd2,L0x7fffffcbd4,L0x7fffffcbd6,L0x7fffffcbd8,L0x7fffffcbda,L0x7fffffcbdc,L0x7fffffcbde] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffcbe0; PC = 0x55555515a0 *)
mov [L0x7fffffcbe0,L0x7fffffcbe2,L0x7fffffcbe4,L0x7fffffcbe6,L0x7fffffcbe8,L0x7fffffcbea,L0x7fffffcbec,L0x7fffffcbee] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffcbf0; PC = 0x55555515a8 *)
mov [L0x7fffffcbf0,L0x7fffffcbf2,L0x7fffffcbf4,L0x7fffffcbf6,L0x7fffffcbf8,L0x7fffffcbfa,L0x7fffffcbfc,L0x7fffffcbfe] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffcc00; PC = 0x55555515b0 *)
mov [L0x7fffffcc00,L0x7fffffcc02,L0x7fffffcc04,L0x7fffffcc06,L0x7fffffcc08,L0x7fffffcc0a,L0x7fffffcc0c,L0x7fffffcc0e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffcc10; PC = 0x55555515b8 *)
mov [L0x7fffffcc10,L0x7fffffcc12,L0x7fffffcc14,L0x7fffffcc16,L0x7fffffcc18,L0x7fffffcc1a,L0x7fffffcc1c,L0x7fffffcc1e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffcc20; PC = 0x55555515c0 *)
mov [L0x7fffffcc20,L0x7fffffcc22,L0x7fffffcc24,L0x7fffffcc26,L0x7fffffcc28,L0x7fffffcc2a,L0x7fffffcc2c,L0x7fffffcc2e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffcc30; PC = 0x55555515c8 *)
mov [L0x7fffffcc30,L0x7fffffcc32,L0x7fffffcc34,L0x7fffffcc36,L0x7fffffcc38,L0x7fffffcc3a,L0x7fffffcc3c,L0x7fffffcc3e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffcc40; PC = 0x55555515d0 *)
mov [L0x7fffffcc40,L0x7fffffcc42,L0x7fffffcc44,L0x7fffffcc46,L0x7fffffcc48,L0x7fffffcc4a,L0x7fffffcc4c,L0x7fffffcc4e] %v0;

cut (* 64 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffcc70; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffcc70,L0x7fffffcc72,L0x7fffffcc74,L0x7fffffcc76,L0x7fffffcc78,L0x7fffffcc7a,L0x7fffffcc7c,L0x7fffffcc7e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffcc80; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffcc80,L0x7fffffcc82,L0x7fffffcc84,L0x7fffffcc86,L0x7fffffcc88,L0x7fffffcc8a,L0x7fffffcc8c,L0x7fffffcc8e];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffcc90; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffcc90,L0x7fffffcc92,L0x7fffffcc94,L0x7fffffcc96,L0x7fffffcc98,L0x7fffffcc9a,L0x7fffffcc9c,L0x7fffffcc9e];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffcca0; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffcca0,L0x7fffffcca2,L0x7fffffcca4,L0x7fffffcca6,L0x7fffffcca8,L0x7fffffccaa,L0x7fffffccac,L0x7fffffccae];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffccb0; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffccb0,L0x7fffffccb2,L0x7fffffccb4,L0x7fffffccb6,L0x7fffffccb8,L0x7fffffccba,L0x7fffffccbc,L0x7fffffccbe];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffccc0; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffccc0,L0x7fffffccc2,L0x7fffffccc4,L0x7fffffccc6,L0x7fffffccc8,L0x7fffffccca,L0x7fffffcccc,L0x7fffffccce];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffccd0; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffccd0,L0x7fffffccd2,L0x7fffffccd4,L0x7fffffccd6,L0x7fffffccd8,L0x7fffffccda,L0x7fffffccdc,L0x7fffffccde];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffcc60; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffcc60,L0x7fffffcc62,L0x7fffffcc64,L0x7fffffcc66,L0x7fffffcc68,L0x7fffffcc6a,L0x7fffffcc6c,L0x7fffffcc6e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffcc50; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffcc50,L0x7fffffcc52,L0x7fffffcc54,L0x7fffffcc56,L0x7fffffcc58,L0x7fffffcc5a,L0x7fffffcc5c,L0x7fffffcc5e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_80@sint16[8], %fa1_80@sint16[8], %fa2_80@sint16[8], %fa3_80@sint16[8], %fa4_80@sint16[8], %fa5_80@sint16[8], %fa6_80@sint16[8], %fa7_80@sint16[8], %fa8_80@sint16[8] :
    %fa0_80 = %v0 /\ %fa1_80 = %v1 /\ %fa2_80 = %v2 /\ %fa3_80 = %v3 /\
    %fa4_80 = %v4 /\ %fa5_80 = %v5 /\ %fa6_80 = %v6 /\ %fa7_80 = %v7 /\
    %fa8_80 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_80 = %v0 /\ %fa1_80 = %v1 /\ %fa2_80 = %v2 /\ %fa3_80 = %v3 /\
    %fa4_80 = %v4 /\ %fa5_80 = %v5 /\ %fa6_80 = %v6 /\ %fa7_80 = %v7 /\
    %fa8_80 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 65 *)
    %fa0_80 = %v0 /\ %fa1_80 = %v1 /\ %fa2_80 = %v2 /\ %fa3_80 = %v3 /\
    %fa4_80 = %v4 /\ %fa5_80 = %v5 /\ %fa6_80 = %v6 /\ %fa7_80 = %v7 /\
    %fa8_80 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_80 = [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] /\
    %fa1_80 = [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] /\
    %fa2_80 = [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] /\
    %fa3_80 = [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] /\
    %fa4_80 = [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] /\
    %fa5_80 = [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] /\
    %fa6_80 = [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] /\
    %fa7_80 = [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] /\
    %fa8_80 = [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_80 = %v0 /\ %fa1_80 = %v1 /\ %fa2_80 = %v2 /\ %fa3_80 = %v3 /\
    %fa4_80 = %v4 /\ %fa5_80 = %v5 /\ %fa6_80 = %v6 /\ %fa7_80 = %v7 /\
    %fa8_80 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_80 = [arr8000, arr8001, arr8002, arr8003, arr8004, arr8005, arr8006, arr8007] /\
    %fa1_80 = [arr8080, arr8081, arr8082, arr8083, arr8084, arr8085, arr8086, arr8087] /\
    %fa2_80 = [arr8070, arr8071, arr8072, arr8073, arr8074, arr8075, arr8076, arr8077] /\
    %fa3_80 = [arr8060, arr8061, arr8062, arr8063, arr8064, arr8065, arr8066, arr8067] /\
    %fa4_80 = [arr8050, arr8051, arr8052, arr8053, arr8054, arr8055, arr8056, arr8057] /\
    %fa5_80 = [arr8040, arr8041, arr8042, arr8043, arr8044, arr8045, arr8046, arr8047] /\
    %fa6_80 = [arr8030, arr8031, arr8032, arr8033, arr8034, arr8035, arr8036, arr8037] /\
    %fa7_80 = [arr8020, arr8021, arr8022, arr8023, arr8024, arr8025, arr8026, arr8027] /\
    %fa8_80 = [arr8010, arr8011, arr8012, arr8013, arr8014, arr8015, arr8016, arr8017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_80 + %fa3_80 + %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_80 + %fa3_80 + %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_80 + %fa3_80 + %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_80 + %fa3_80 + %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_80 - %fa3_80 - %fa6_80
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_80 - %fa6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_80 - %fa4_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_80 - %fa4_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_80 - %fa4_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_80 - %fa4_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_80 - %fa5_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_80 - %fa5_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_80 - %fa5_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_80 - %fa5_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 66 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        %fa3_80 +
        %fa6_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_80 +
        %fa7_80 +
        %fa4_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_80 +
        %fa2_80 +
        %fa5_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_80 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_80 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 65], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 65]];


ghost %fb0_80@sint16[8], %fb1_80@sint16[8], %fb2_80@sint16[8], %fb3_80@sint16[8], %fb4_80@sint16[8], %fb5_80@sint16[8], %fb6_80@sint16[8], %fb7_80@sint16[8], %fb8_80@sint16[8] :
    %fb0_80 = %v17 /\ %fb1_80 = %v16 /\ %fb2_80 = %v0  /\ %fb3_80 = %v6  /\
    %fb4_80 = %v3  /\ %fb5_80 = %v7  /\ %fb6_80 = %v19 /\ %fb7_80 = %v1  /\ %fb8_80 = %v2
  &&
    %fb0_80 = %v17 /\ %fb1_80 = %v16 /\ %fb2_80 = %v0  /\ %fb3_80 = %v6  /\
    %fb4_80 = %v3  /\ %fb5_80 = %v7  /\ %fb6_80 = %v19 /\ %fb7_80 = %v1  /\ %fb8_80 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_80 + %fb6_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_80 + %fb6_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_80 + %fb6_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_80 + %fb6_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_80 - %fb6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_80 - %fb6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_80 - %fb6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_80 - %fb6_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_80@sint16[8] : %b1i1_80 = %v1 && %b1i1_80 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_80@sint16[8] : %b1i2_80 = %v5 && %b1i2_80 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_80 + %b1i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_80 + %b1i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_80 + %b1i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_80 + %b1i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_80 - %b1i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_80 - %b1i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_80 - %b1i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_80 - %b1i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_80@sint16[8] : %b2i1_80 = %v3 && %b2i1_80 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_80@sint16[8] : %b2i2_80 = %v1 && %b2i2_80 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_80 + %b2i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_80 + %b2i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_80 + %b2i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_80 + %b2i2_80
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_80 - %b2i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_80 - %b2i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_80 - %b2i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_80 - %b2i2_80)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 67 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        %fa1_80 +
        %fa2_80 +
        %fa3_80 +
        %fa4_80 +
        %fa5_80 +
        %fa6_80 +
        %fa7_80 +
        %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_80 +
        %fa3_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_80 +
        %fa6_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_80 +
        %fa3_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_80 +
        %fa6_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_80 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_80 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_80 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_80 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_80 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_80 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_80 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_80 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_80
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 65]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 65], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 65]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffcc50; PC = 0x55555514f8 *)
mov [L0x7fffffcc50,L0x7fffffcc52,L0x7fffffcc54,L0x7fffffcc56,L0x7fffffcc58,L0x7fffffcc5a,L0x7fffffcc5c,L0x7fffffcc5e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffcc60; PC = 0x5555551500 *)
mov [L0x7fffffcc60,L0x7fffffcc62,L0x7fffffcc64,L0x7fffffcc66,L0x7fffffcc68,L0x7fffffcc6a,L0x7fffffcc6c,L0x7fffffcc6e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffcc70; PC = 0x5555551508 *)
mov [L0x7fffffcc70,L0x7fffffcc72,L0x7fffffcc74,L0x7fffffcc76,L0x7fffffcc78,L0x7fffffcc7a,L0x7fffffcc7c,L0x7fffffcc7e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffcc80; PC = 0x5555551510 *)
mov [L0x7fffffcc80,L0x7fffffcc82,L0x7fffffcc84,L0x7fffffcc86,L0x7fffffcc88,L0x7fffffcc8a,L0x7fffffcc8c,L0x7fffffcc8e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffcc90; PC = 0x5555551518 *)
mov [L0x7fffffcc90,L0x7fffffcc92,L0x7fffffcc94,L0x7fffffcc96,L0x7fffffcc98,L0x7fffffcc9a,L0x7fffffcc9c,L0x7fffffcc9e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffcca0; PC = 0x5555551520 *)
mov [L0x7fffffcca0,L0x7fffffcca2,L0x7fffffcca4,L0x7fffffcca6,L0x7fffffcca8,L0x7fffffccaa,L0x7fffffccac,L0x7fffffccae] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffccb0; PC = 0x5555551528 *)
mov [L0x7fffffccb0,L0x7fffffccb2,L0x7fffffccb4,L0x7fffffccb6,L0x7fffffccb8,L0x7fffffccba,L0x7fffffccbc,L0x7fffffccbe] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffccc0; PC = 0x5555551530 *)
mov [L0x7fffffccc0,L0x7fffffccc2,L0x7fffffccc4,L0x7fffffccc6,L0x7fffffccc8,L0x7fffffccca,L0x7fffffcccc,L0x7fffffccce] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffccd0; PC = 0x5555551538 *)
mov [L0x7fffffccd0,L0x7fffffccd2,L0x7fffffccd4,L0x7fffffccd6,L0x7fffffccd8,L0x7fffffccda,L0x7fffffccdc,L0x7fffffccde] %v0;

cut (* 68 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffcd00; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffcd00,L0x7fffffcd02,L0x7fffffcd04,L0x7fffffcd06,L0x7fffffcd08,L0x7fffffcd0a,L0x7fffffcd0c,L0x7fffffcd0e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffcd10; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffcd10,L0x7fffffcd12,L0x7fffffcd14,L0x7fffffcd16,L0x7fffffcd18,L0x7fffffcd1a,L0x7fffffcd1c,L0x7fffffcd1e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffcd20; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffcd20,L0x7fffffcd22,L0x7fffffcd24,L0x7fffffcd26,L0x7fffffcd28,L0x7fffffcd2a,L0x7fffffcd2c,L0x7fffffcd2e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffcd30; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffcd30,L0x7fffffcd32,L0x7fffffcd34,L0x7fffffcd36,L0x7fffffcd38,L0x7fffffcd3a,L0x7fffffcd3c,L0x7fffffcd3e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffcd40; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffcd40,L0x7fffffcd42,L0x7fffffcd44,L0x7fffffcd46,L0x7fffffcd48,L0x7fffffcd4a,L0x7fffffcd4c,L0x7fffffcd4e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffcd50; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffcd50,L0x7fffffcd52,L0x7fffffcd54,L0x7fffffcd56,L0x7fffffcd58,L0x7fffffcd5a,L0x7fffffcd5c,L0x7fffffcd5e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffcd60; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffcd60,L0x7fffffcd62,L0x7fffffcd64,L0x7fffffcd66,L0x7fffffcd68,L0x7fffffcd6a,L0x7fffffcd6c,L0x7fffffcd6e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffcce0; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffcce0,L0x7fffffcce2,L0x7fffffcce4,L0x7fffffcce6,L0x7fffffcce8,L0x7fffffccea,L0x7fffffccec,L0x7fffffccee];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffccf0; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffccf0,L0x7fffffccf2,L0x7fffffccf4,L0x7fffffccf6,L0x7fffffccf8,L0x7fffffccfa,L0x7fffffccfc,L0x7fffffccfe];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_81@sint16[8], %fa1_81@sint16[8], %fa2_81@sint16[8], %fa3_81@sint16[8], %fa4_81@sint16[8], %fa5_81@sint16[8], %fa6_81@sint16[8], %fa7_81@sint16[8], %fa8_81@sint16[8] :
    %fa0_81 = %v0 /\ %fa1_81 = %v1 /\ %fa2_81 = %v2 /\ %fa3_81 = %v3 /\
    %fa4_81 = %v4 /\ %fa5_81 = %v5 /\ %fa6_81 = %v6 /\ %fa7_81 = %v7 /\
    %fa8_81 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_81 = %v0 /\ %fa1_81 = %v1 /\ %fa2_81 = %v2 /\ %fa3_81 = %v3 /\
    %fa4_81 = %v4 /\ %fa5_81 = %v5 /\ %fa6_81 = %v6 /\ %fa7_81 = %v7 /\
    %fa8_81 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 69 *)
    %fa0_81 = %v0 /\ %fa1_81 = %v1 /\ %fa2_81 = %v2 /\ %fa3_81 = %v3 /\
    %fa4_81 = %v4 /\ %fa5_81 = %v5 /\ %fa6_81 = %v6 /\ %fa7_81 = %v7 /\
    %fa8_81 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_81 = [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] /\
    %fa1_81 = [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] /\
    %fa2_81 = [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] /\
    %fa3_81 = [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] /\
    %fa4_81 = [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] /\
    %fa5_81 = [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] /\
    %fa6_81 = [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] /\
    %fa7_81 = [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] /\
    %fa8_81 = [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_81 = %v0 /\ %fa1_81 = %v1 /\ %fa2_81 = %v2 /\ %fa3_81 = %v3 /\
    %fa4_81 = %v4 /\ %fa5_81 = %v5 /\ %fa6_81 = %v6 /\ %fa7_81 = %v7 /\
    %fa8_81 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_81 = [arr8100, arr8101, arr8102, arr8103, arr8104, arr8105, arr8106, arr8107] /\
    %fa1_81 = [arr8180, arr8181, arr8182, arr8183, arr8184, arr8185, arr8186, arr8187] /\
    %fa2_81 = [arr8170, arr8171, arr8172, arr8173, arr8174, arr8175, arr8176, arr8177] /\
    %fa3_81 = [arr8160, arr8161, arr8162, arr8163, arr8164, arr8165, arr8166, arr8167] /\
    %fa4_81 = [arr8150, arr8151, arr8152, arr8153, arr8154, arr8155, arr8156, arr8157] /\
    %fa5_81 = [arr8140, arr8141, arr8142, arr8143, arr8144, arr8145, arr8146, arr8147] /\
    %fa6_81 = [arr8130, arr8131, arr8132, arr8133, arr8134, arr8135, arr8136, arr8137] /\
    %fa7_81 = [arr8120, arr8121, arr8122, arr8123, arr8124, arr8125, arr8126, arr8127] /\
    %fa8_81 = [arr8110, arr8111, arr8112, arr8113, arr8114, arr8115, arr8116, arr8117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_81 + %fa3_81 + %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_81 + %fa3_81 + %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_81 + %fa3_81 + %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_81 + %fa3_81 + %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_81 - %fa3_81 - %fa6_81
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_81 - %fa6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_81 - %fa4_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_81 - %fa4_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_81 - %fa4_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_81 - %fa4_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_81 - %fa5_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_81 - %fa5_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_81 - %fa5_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_81 - %fa5_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 70 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        %fa3_81 +
        %fa6_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_81 +
        %fa7_81 +
        %fa4_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_81 +
        %fa2_81 +
        %fa5_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_81 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_81 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 69], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 69]];


ghost %fb0_81@sint16[8], %fb1_81@sint16[8], %fb2_81@sint16[8], %fb3_81@sint16[8], %fb4_81@sint16[8], %fb5_81@sint16[8], %fb6_81@sint16[8], %fb7_81@sint16[8], %fb8_81@sint16[8] :
    %fb0_81 = %v17 /\ %fb1_81 = %v16 /\ %fb2_81 = %v0  /\ %fb3_81 = %v6  /\
    %fb4_81 = %v3  /\ %fb5_81 = %v7  /\ %fb6_81 = %v19 /\ %fb7_81 = %v1  /\ %fb8_81 = %v2
  &&
    %fb0_81 = %v17 /\ %fb1_81 = %v16 /\ %fb2_81 = %v0  /\ %fb3_81 = %v6  /\
    %fb4_81 = %v3  /\ %fb5_81 = %v7  /\ %fb6_81 = %v19 /\ %fb7_81 = %v1  /\ %fb8_81 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_81 + %fb6_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_81 + %fb6_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_81 + %fb6_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_81 + %fb6_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_81 - %fb6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_81 - %fb6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_81 - %fb6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_81 - %fb6_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_81@sint16[8] : %b1i1_81 = %v1 && %b1i1_81 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_81@sint16[8] : %b1i2_81 = %v5 && %b1i2_81 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_81 + %b1i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_81 + %b1i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_81 + %b1i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_81 + %b1i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_81 - %b1i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_81 - %b1i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_81 - %b1i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_81 - %b1i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_81@sint16[8] : %b2i1_81 = %v3 && %b2i1_81 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_81@sint16[8] : %b2i2_81 = %v1 && %b2i2_81 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_81 + %b2i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_81 + %b2i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_81 + %b2i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_81 + %b2i2_81
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_81 - %b2i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_81 - %b2i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_81 - %b2i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_81 - %b2i2_81)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 71 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        %fa1_81 +
        %fa2_81 +
        %fa3_81 +
        %fa4_81 +
        %fa5_81 +
        %fa6_81 +
        %fa7_81 +
        %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_81 +
        %fa3_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_81 +
        %fa6_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_81 +
        %fa3_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_81 +
        %fa6_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_81 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_81 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_81 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_81 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_81 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_81 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_81 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_81 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_81
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 69]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 69], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 69]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffcce0; PC = 0x5555551590 *)
mov [L0x7fffffcce0,L0x7fffffcce2,L0x7fffffcce4,L0x7fffffcce6,L0x7fffffcce8,L0x7fffffccea,L0x7fffffccec,L0x7fffffccee] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffccf0; PC = 0x5555551598 *)
mov [L0x7fffffccf0,L0x7fffffccf2,L0x7fffffccf4,L0x7fffffccf6,L0x7fffffccf8,L0x7fffffccfa,L0x7fffffccfc,L0x7fffffccfe] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffcd00; PC = 0x55555515a0 *)
mov [L0x7fffffcd00,L0x7fffffcd02,L0x7fffffcd04,L0x7fffffcd06,L0x7fffffcd08,L0x7fffffcd0a,L0x7fffffcd0c,L0x7fffffcd0e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffcd10; PC = 0x55555515a8 *)
mov [L0x7fffffcd10,L0x7fffffcd12,L0x7fffffcd14,L0x7fffffcd16,L0x7fffffcd18,L0x7fffffcd1a,L0x7fffffcd1c,L0x7fffffcd1e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffcd20; PC = 0x55555515b0 *)
mov [L0x7fffffcd20,L0x7fffffcd22,L0x7fffffcd24,L0x7fffffcd26,L0x7fffffcd28,L0x7fffffcd2a,L0x7fffffcd2c,L0x7fffffcd2e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffcd30; PC = 0x55555515b8 *)
mov [L0x7fffffcd30,L0x7fffffcd32,L0x7fffffcd34,L0x7fffffcd36,L0x7fffffcd38,L0x7fffffcd3a,L0x7fffffcd3c,L0x7fffffcd3e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffcd40; PC = 0x55555515c0 *)
mov [L0x7fffffcd40,L0x7fffffcd42,L0x7fffffcd44,L0x7fffffcd46,L0x7fffffcd48,L0x7fffffcd4a,L0x7fffffcd4c,L0x7fffffcd4e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffcd50; PC = 0x55555515c8 *)
mov [L0x7fffffcd50,L0x7fffffcd52,L0x7fffffcd54,L0x7fffffcd56,L0x7fffffcd58,L0x7fffffcd5a,L0x7fffffcd5c,L0x7fffffcd5e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffcd60; PC = 0x55555515d0 *)
mov [L0x7fffffcd60,L0x7fffffcd62,L0x7fffffcd64,L0x7fffffcd66,L0x7fffffcd68,L0x7fffffcd6a,L0x7fffffcd6c,L0x7fffffcd6e] %v0;

cut (* 72 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


## i_iter


### k0_iter_0


#### load

(* ldr	q7, [x19, #32]                              #! EA = L0x7fffffcd90; Value = 0x0000000000000000; PC = 0x55555514a0 *)
mov %v7 [L0x7fffffcd90,L0x7fffffcd92,L0x7fffffcd94,L0x7fffffcd96,L0x7fffffcd98,L0x7fffffcd9a,L0x7fffffcd9c,L0x7fffffcd9e];
(* ldr	q6, [x19, #48]                              #! EA = L0x7fffffcda0; Value = 0x0000000000000000; PC = 0x55555514a4 *)
mov %v6 [L0x7fffffcda0,L0x7fffffcda2,L0x7fffffcda4,L0x7fffffcda6,L0x7fffffcda8,L0x7fffffcdaa,L0x7fffffcdac,L0x7fffffcdae];
(* ldr	q5, [x19, #64]                              #! EA = L0x7fffffcdb0; Value = 0x0000000000000000; PC = 0x55555514a8 *)
mov %v5 [L0x7fffffcdb0,L0x7fffffcdb2,L0x7fffffcdb4,L0x7fffffcdb6,L0x7fffffcdb8,L0x7fffffcdba,L0x7fffffcdbc,L0x7fffffcdbe];
(* ldr	q4, [x19, #80]                              #! EA = L0x7fffffcdc0; Value = 0x0000000000000000; PC = 0x55555514ac *)
mov %v4 [L0x7fffffcdc0,L0x7fffffcdc2,L0x7fffffcdc4,L0x7fffffcdc6,L0x7fffffcdc8,L0x7fffffcdca,L0x7fffffcdcc,L0x7fffffcdce];
(* ldr	q3, [x19, #96]                              #! EA = L0x7fffffcdd0; Value = 0x0000000000000000; PC = 0x55555514b0 *)
mov %v3 [L0x7fffffcdd0,L0x7fffffcdd2,L0x7fffffcdd4,L0x7fffffcdd6,L0x7fffffcdd8,L0x7fffffcdda,L0x7fffffcddc,L0x7fffffcdde];
(* ldr	q2, [x19, #112]                             #! EA = L0x7fffffcde0; Value = 0x0000000000000000; PC = 0x55555514b4 *)
mov %v2 [L0x7fffffcde0,L0x7fffffcde2,L0x7fffffcde4,L0x7fffffcde6,L0x7fffffcde8,L0x7fffffcdea,L0x7fffffcdec,L0x7fffffcdee];
(* ldr	q1, [x19, #128]                             #! EA = L0x7fffffcdf0; Value = 0x0000000000000000; PC = 0x55555514b8 *)
mov %v1 [L0x7fffffcdf0,L0x7fffffcdf2,L0x7fffffcdf4,L0x7fffffcdf6,L0x7fffffcdf8,L0x7fffffcdfa,L0x7fffffcdfc,L0x7fffffcdfe];
(* ldr	q0, [x19, #16]                              #! EA = L0x7fffffcd80; Value = 0x0000000000000000; PC = 0x55555514c4 *)
mov %v0 [L0x7fffffcd80,L0x7fffffcd82,L0x7fffffcd84,L0x7fffffcd86,L0x7fffffcd88,L0x7fffffcd8a,L0x7fffffcd8c,L0x7fffffcd8e];
(* str	q0, [sp]                                    #! EA = L0x7fffffc200; PC = 0x55555514c8 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v0;
(* ldr	q0, [x19]                                   #! EA = L0x7fffffcd70; Value = 0x0000000000000000; PC = 0x55555514ec *)
mov %v0 [L0x7fffffcd70,L0x7fffffcd72,L0x7fffffcd74,L0x7fffffcd76,L0x7fffffcd78,L0x7fffffcd7a,L0x7fffffcd7c,L0x7fffffcd7e];
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x55555514f0 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x55555514f0 = 0x55555514f0;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_90@sint16[8], %fa1_90@sint16[8], %fa2_90@sint16[8], %fa3_90@sint16[8], %fa4_90@sint16[8], %fa5_90@sint16[8], %fa6_90@sint16[8], %fa7_90@sint16[8], %fa8_90@sint16[8] :
    %fa0_90 = %v0 /\ %fa1_90 = %v1 /\ %fa2_90 = %v2 /\ %fa3_90 = %v3 /\
    %fa4_90 = %v4 /\ %fa5_90 = %v5 /\ %fa6_90 = %v6 /\ %fa7_90 = %v7 /\
    %fa8_90 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_90 = %v0 /\ %fa1_90 = %v1 /\ %fa2_90 = %v2 /\ %fa3_90 = %v3 /\
    %fa4_90 = %v4 /\ %fa5_90 = %v5 /\ %fa6_90 = %v6 /\ %fa7_90 = %v7 /\
    %fa8_90 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 73 *)
    %fa0_90 = %v0 /\ %fa1_90 = %v1 /\ %fa2_90 = %v2 /\ %fa3_90 = %v3 /\
    %fa4_90 = %v4 /\ %fa5_90 = %v5 /\ %fa6_90 = %v6 /\ %fa7_90 = %v7 /\
    %fa8_90 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_90 = [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] /\
    %fa1_90 = [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] /\
    %fa2_90 = [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] /\
    %fa3_90 = [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] /\
    %fa4_90 = [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] /\
    %fa5_90 = [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] /\
    %fa6_90 = [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] /\
    %fa7_90 = [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] /\
    %fa8_90 = [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_90 = %v0 /\ %fa1_90 = %v1 /\ %fa2_90 = %v2 /\ %fa3_90 = %v3 /\
    %fa4_90 = %v4 /\ %fa5_90 = %v5 /\ %fa6_90 = %v6 /\ %fa7_90 = %v7 /\
    %fa8_90 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_90 = [arr9000, arr9001, arr9002, arr9003, arr9004, arr9005, arr9006, arr9007] /\
    %fa1_90 = [arr9080, arr9081, arr9082, arr9083, arr9084, arr9085, arr9086, arr9087] /\
    %fa2_90 = [arr9070, arr9071, arr9072, arr9073, arr9074, arr9075, arr9076, arr9077] /\
    %fa3_90 = [arr9060, arr9061, arr9062, arr9063, arr9064, arr9065, arr9066, arr9067] /\
    %fa4_90 = [arr9050, arr9051, arr9052, arr9053, arr9054, arr9055, arr9056, arr9057] /\
    %fa5_90 = [arr9040, arr9041, arr9042, arr9043, arr9044, arr9045, arr9046, arr9047] /\
    %fa6_90 = [arr9030, arr9031, arr9032, arr9033, arr9034, arr9035, arr9036, arr9037] /\
    %fa7_90 = [arr9020, arr9021, arr9022, arr9023, arr9024, arr9025, arr9026, arr9027] /\
    %fa8_90 = [arr9010, arr9011, arr9012, arr9013, arr9014, arr9015, arr9016, arr9017] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_90 + %fa3_90 + %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_90 + %fa3_90 + %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_90 + %fa3_90 + %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_90 + %fa3_90 + %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_90 - %fa3_90 - %fa6_90
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_90 - %fa6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_90 - %fa4_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_90 - %fa4_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_90 - %fa4_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_90 - %fa4_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_90 - %fa5_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_90 - %fa5_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_90 - %fa5_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_90 - %fa5_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 74 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        %fa3_90 +
        %fa6_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_90 +
        %fa7_90 +
        %fa4_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_90 +
        %fa2_90 +
        %fa5_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_90 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_90 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 73], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 73]];


ghost %fb0_90@sint16[8], %fb1_90@sint16[8], %fb2_90@sint16[8], %fb3_90@sint16[8], %fb4_90@sint16[8], %fb5_90@sint16[8], %fb6_90@sint16[8], %fb7_90@sint16[8], %fb8_90@sint16[8] :
    %fb0_90 = %v17 /\ %fb1_90 = %v16 /\ %fb2_90 = %v0  /\ %fb3_90 = %v6  /\
    %fb4_90 = %v3  /\ %fb5_90 = %v7  /\ %fb6_90 = %v19 /\ %fb7_90 = %v1  /\ %fb8_90 = %v2
  &&
    %fb0_90 = %v17 /\ %fb1_90 = %v16 /\ %fb2_90 = %v0  /\ %fb3_90 = %v6  /\
    %fb4_90 = %v3  /\ %fb5_90 = %v7  /\ %fb6_90 = %v19 /\ %fb7_90 = %v1  /\ %fb8_90 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_90 + %fb6_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_90 + %fb6_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_90 + %fb6_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_90 + %fb6_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_90 - %fb6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_90 - %fb6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_90 - %fb6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_90 - %fb6_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_90@sint16[8] : %b1i1_90 = %v1 && %b1i1_90 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_90@sint16[8] : %b1i2_90 = %v5 && %b1i2_90 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_90 + %b1i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_90 + %b1i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_90 + %b1i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_90 + %b1i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_90 - %b1i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_90 - %b1i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_90 - %b1i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_90 - %b1i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_90@sint16[8] : %b2i1_90 = %v3 && %b2i1_90 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_90@sint16[8] : %b2i2_90 = %v1 && %b2i2_90 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_90 + %b2i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_90 + %b2i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_90 + %b2i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_90 + %b2i2_90
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_90 - %b2i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_90 - %b2i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_90 - %b2i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_90 - %b2i2_90)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 75 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        %fa1_90 +
        %fa2_90 +
        %fa3_90 +
        %fa4_90 +
        %fa5_90 +
        %fa6_90 +
        %fa7_90 +
        %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_90 +
        %fa3_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_90 +
        %fa6_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_90 +
        %fa3_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_90 +
        %fa6_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_90 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_90 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_90 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_90 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_90 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_90 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_90 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_90 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_90
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 73]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 73], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 73]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x55555514f4 *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19]                                   #! EA = L0x7fffffcd70; PC = 0x55555514f8 *)
mov [L0x7fffffcd70,L0x7fffffcd72,L0x7fffffcd74,L0x7fffffcd76,L0x7fffffcd78,L0x7fffffcd7a,L0x7fffffcd7c,L0x7fffffcd7e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x55555514fc *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #16]                              #! EA = L0x7fffffcd80; PC = 0x5555551500 *)
mov [L0x7fffffcd80,L0x7fffffcd82,L0x7fffffcd84,L0x7fffffcd86,L0x7fffffcd88,L0x7fffffcd8a,L0x7fffffcd8c,L0x7fffffcd8e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x5555551504 *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #32]                              #! EA = L0x7fffffcd90; PC = 0x5555551508 *)
mov [L0x7fffffcd90,L0x7fffffcd92,L0x7fffffcd94,L0x7fffffcd96,L0x7fffffcd98,L0x7fffffcd9a,L0x7fffffcd9c,L0x7fffffcd9e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x555555150c *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #48]                              #! EA = L0x7fffffcda0; PC = 0x5555551510 *)
mov [L0x7fffffcda0,L0x7fffffcda2,L0x7fffffcda4,L0x7fffffcda6,L0x7fffffcda8,L0x7fffffcdaa,L0x7fffffcdac,L0x7fffffcdae] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x5555551514 *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #64]                              #! EA = L0x7fffffcdb0; PC = 0x5555551518 *)
mov [L0x7fffffcdb0,L0x7fffffcdb2,L0x7fffffcdb4,L0x7fffffcdb6,L0x7fffffcdb8,L0x7fffffcdba,L0x7fffffcdbc,L0x7fffffcdbe] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x555555151c *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #80]                              #! EA = L0x7fffffcdc0; PC = 0x5555551520 *)
mov [L0x7fffffcdc0,L0x7fffffcdc2,L0x7fffffcdc4,L0x7fffffcdc6,L0x7fffffcdc8,L0x7fffffcdca,L0x7fffffcdcc,L0x7fffffcdce] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x5555551524 *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #96]                              #! EA = L0x7fffffcdd0; PC = 0x5555551528 *)
mov [L0x7fffffcdd0,L0x7fffffcdd2,L0x7fffffcdd4,L0x7fffffcdd6,L0x7fffffcdd8,L0x7fffffcdda,L0x7fffffcddc,L0x7fffffcdde] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x555555152c *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #112]                             #! EA = L0x7fffffcde0; PC = 0x5555551530 *)
mov [L0x7fffffcde0,L0x7fffffcde2,L0x7fffffcde4,L0x7fffffcde6,L0x7fffffcde8,L0x7fffffcdea,L0x7fffffcdec,L0x7fffffcdee] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x5555551534 *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #128]                             #! EA = L0x7fffffcdf0; PC = 0x5555551538 *)
mov [L0x7fffffcdf0,L0x7fffffcdf2,L0x7fffffcdf4,L0x7fffffcdf6,L0x7fffffcdf8,L0x7fffffcdfa,L0x7fffffcdfc,L0x7fffffcdfe] %v0;

cut (* 76 *)
    true && true;


### k0_iter_1


#### load

(* ldr	q7, [x19, #176]                             #! EA = L0x7fffffce20; Value = 0x0000000000000000; PC = 0x555555153c *)
mov %v7 [L0x7fffffce20,L0x7fffffce22,L0x7fffffce24,L0x7fffffce26,L0x7fffffce28,L0x7fffffce2a,L0x7fffffce2c,L0x7fffffce2e];
(* ldr	q6, [x19, #192]                             #! EA = L0x7fffffce30; Value = 0x0000000000000000; PC = 0x5555551540 *)
mov %v6 [L0x7fffffce30,L0x7fffffce32,L0x7fffffce34,L0x7fffffce36,L0x7fffffce38,L0x7fffffce3a,L0x7fffffce3c,L0x7fffffce3e];
(* ldr	q5, [x19, #208]                             #! EA = L0x7fffffce40; Value = 0x0000000000000000; PC = 0x5555551544 *)
mov %v5 [L0x7fffffce40,L0x7fffffce42,L0x7fffffce44,L0x7fffffce46,L0x7fffffce48,L0x7fffffce4a,L0x7fffffce4c,L0x7fffffce4e];
(* ldr	q4, [x19, #224]                             #! EA = L0x7fffffce50; Value = 0x0000000000000000; PC = 0x5555551548 *)
mov %v4 [L0x7fffffce50,L0x7fffffce52,L0x7fffffce54,L0x7fffffce56,L0x7fffffce58,L0x7fffffce5a,L0x7fffffce5c,L0x7fffffce5e];
(* ldr	q3, [x19, #240]                             #! EA = L0x7fffffce60; Value = 0x0000000000000000; PC = 0x555555154c *)
mov %v3 [L0x7fffffce60,L0x7fffffce62,L0x7fffffce64,L0x7fffffce66,L0x7fffffce68,L0x7fffffce6a,L0x7fffffce6c,L0x7fffffce6e];
(* ldr	q2, [x19, #256]                             #! EA = L0x7fffffce70; Value = 0x0000000000000000; PC = 0x5555551550 *)
mov %v2 [L0x7fffffce70,L0x7fffffce72,L0x7fffffce74,L0x7fffffce76,L0x7fffffce78,L0x7fffffce7a,L0x7fffffce7c,L0x7fffffce7e];
(* ldr	q1, [x19, #272]                             #! EA = L0x7fffffce80; Value = 0x0000000000000000; PC = 0x5555551554 *)
mov %v1 [L0x7fffffce80,L0x7fffffce82,L0x7fffffce84,L0x7fffffce86,L0x7fffffce88,L0x7fffffce8a,L0x7fffffce8c,L0x7fffffce8e];
(* ldr	q0, [x19, #144]                             #! EA = L0x7fffffce00; Value = 0x0000000000000000; PC = 0x5555551558 *)
mov %v0 [L0x7fffffce00,L0x7fffffce02,L0x7fffffce04,L0x7fffffce06,L0x7fffffce08,L0x7fffffce0a,L0x7fffffce0c,L0x7fffffce0e];
(* ldr	q16, [x19, #160]                            #! EA = L0x7fffffce10; Value = 0x0000000000000000; PC = 0x5555551560 *)
mov %v16 [L0x7fffffce10,L0x7fffffce12,L0x7fffffce14,L0x7fffffce16,L0x7fffffce18,L0x7fffffce1a,L0x7fffffce1c,L0x7fffffce1e];
(* str	q16, [sp]                                   #! EA = L0x7fffffc200; PC = 0x5555551564 *)
mov [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e] %v16;
(* #bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! PC = 0x5555551588 *)
#bl	0x5555551604 <_ZN6xpower9main_lay27ntt9_2xE11__Int16x8_tS1_S1_S1_S1_S1_S1_S1_S1_RS1_S2_S2_S2_S2_S2_S2_S2_S2_>#! 0x5555551588 = 0x5555551588;
(* #! -> SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;

ghost %fa0_91@sint16[8], %fa1_91@sint16[8], %fa2_91@sint16[8], %fa3_91@sint16[8], %fa4_91@sint16[8], %fa5_91@sint16[8], %fa6_91@sint16[8], %fa7_91@sint16[8], %fa8_91@sint16[8] :
    %fa0_91 = %v0 /\ %fa1_91 = %v1 /\ %fa2_91 = %v2 /\ %fa3_91 = %v3 /\
    %fa4_91 = %v4 /\ %fa5_91 = %v5 /\ %fa6_91 = %v6 /\ %fa7_91 = %v7 /\
    %fa8_91 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e]
  &&
    %fa0_91 = %v0 /\ %fa1_91 = %v1 /\ %fa2_91 = %v2 /\ %fa3_91 = %v3 /\
    %fa4_91 = %v4 /\ %fa5_91 = %v5 /\ %fa6_91 = %v6 /\ %fa7_91 = %v7 /\
    %fa8_91 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e];

cut (* 77 *)
    %fa0_91 = %v0 /\ %fa1_91 = %v1 /\ %fa2_91 = %v2 /\ %fa3_91 = %v3 /\
    %fa4_91 = %v4 /\ %fa5_91 = %v5 /\ %fa6_91 = %v6 /\ %fa7_91 = %v7 /\
    %fa8_91 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_91 = [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] /\
    %fa1_91 = [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] /\
    %fa2_91 = [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] /\
    %fa3_91 = [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] /\
    %fa4_91 = [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] /\
    %fa5_91 = [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] /\
    %fa6_91 = [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] /\
    %fa7_91 = [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] /\
    %fa8_91 = [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] /\
    true
    prove with [cuts[0]]
  &&
    %fa0_91 = %v0 /\ %fa1_91 = %v1 /\ %fa2_91 = %v2 /\ %fa3_91 = %v3 /\
    %fa4_91 = %v4 /\ %fa5_91 = %v5 /\ %fa6_91 = %v6 /\ %fa7_91 = %v7 /\
    %fa8_91 = [L0x7fffffc200, L0x7fffffc202, L0x7fffffc204, L0x7fffffc206, L0x7fffffc208, L0x7fffffc20a, L0x7fffffc20c, L0x7fffffc20e] /\

    %fa0_91 = [arr9100, arr9101, arr9102, arr9103, arr9104, arr9105, arr9106, arr9107] /\
    %fa1_91 = [arr9180, arr9181, arr9182, arr9183, arr9184, arr9185, arr9186, arr9187] /\
    %fa2_91 = [arr9170, arr9171, arr9172, arr9173, arr9174, arr9175, arr9176, arr9177] /\
    %fa3_91 = [arr9160, arr9161, arr9162, arr9163, arr9164, arr9165, arr9166, arr9167] /\
    %fa4_91 = [arr9150, arr9151, arr9152, arr9153, arr9154, arr9155, arr9156, arr9157] /\
    %fa5_91 = [arr9140, arr9141, arr9142, arr9143, arr9144, arr9145, arr9146, arr9147] /\
    %fa6_91 = [arr9130, arr9131, arr9132, arr9133, arr9134, arr9135, arr9136, arr9137] /\
    %fa7_91 = [arr9120, arr9121, arr9122, arr9123, arr9124, arr9125, arr9126, arr9127] /\
    %fa8_91 = [arr9110, arr9111, arr9112, arr9113, arr9114, arr9115, arr9116, arr9117] /\
    true
    prove with [cuts[0]];

#### ntt9_2x

(* stp	d8, d9, [sp, #-16]!                         #! EA = L0x7fffffc1f0; PC = 0x5555551604 *)
mov [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6] %v8[0:4];
mov [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe] %v9[0:4];
(* mov	v20.16b, v4.16b                             #! PC = 0x5555551608 *)
mov %v20 %v4;
(* mov	v19.16b, v5.16b                             #! PC = 0x555555160c *)
mov %v19 %v5;
(* ldr	q18, [sp, #16]                              #! EA = L0x7fffffc200; Value = 0x0000000000000000; PC = 0x5555551610 *)
mov %v18 [L0x7fffffc200,L0x7fffffc202,L0x7fffffc204,L0x7fffffc206,L0x7fffffc208,L0x7fffffc20a,L0x7fffffc20c,L0x7fffffc20e];
(* add	v9.8h, v3.8h, v6.8h                         #! PC = 0x5555551614 *)
add %v9 %v3 %v6;
(* sub	v3.8h, v3.8h, v6.8h                         #! PC = 0x5555551618 *)
sub %v3 %v3 %v6;
(* add	v5.8h, v0.8h, v9.8h                         #! PC = 0x555555161c *)
add %v5 %v0 %v9;
(* ldr	q8, [x8, #2064]                             #! EA = L0x5555552810; Value = 0x00000000723311ef; PC = 0x5555551624 *)
mov %v8 [L0x5555552810,L0x5555552812,L0x5555552814,L0x5555552816,L0x5555552818,L0x555555281a,L0x555555281c,L0x555555281e];
(* sqdmulh	v4.8h, v5.8h, v8.h[1]                   #! PC = 0x5555551628 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v5 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #11                         #! PC = 0x555555162c *)
split %HI %LO %v4 11; split %LO1 %dc %LO (11-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* shl	v5.8h, v5.8h, #1                            #! PC = 0x5555551630 *)
shl %v5 %v5 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* mov	v17.16b, v5.16b                             #! PC = 0x5555551634 *)
mov %v17 %v5;
(* mls	v17.8h, v4.8h, v8.h[0]                      #! PC = 0x5555551638 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v17 %v17 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_91 + %fa3_91 + %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_91 + %fa3_91 + %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_91 + %fa3_91 + %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * (%fa0_91 + %fa3_91 + %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v17
    && true;

(* shl	v0.8h, v0.8h, #1                            #! PC = 0x555555163c *)
shl %v0 %v0 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v0.8h, v0.8h, v9.8h                         #! PC = 0x5555551640 *)
sub %v0 %v0 %v9;
(* sqdmulh	v6.8h, v0.8h, v8.h[1]                   #! PC = 0x5555551644 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v0 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v6@sint16[8] %LO0;
(* srshr	v6.8h, v6.8h, #10                         #! PC = 0x5555551648 *)
split %HI %LO %v6 10; split %LO1 %dc %LO (10-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* ldr	q9, [x8, #2080]                             #! EA = L0x5555552820; Value = 0x0000fcdd0763fd93; PC = 0x5555551650 *)
mov %v9 [L0x5555552820,L0x5555552822,L0x5555552824,L0x5555552826,L0x5555552828,L0x555555282a,L0x555555282c,L0x555555282e];
(* sqrdmulh	v4.8h, v3.8h, v9.h[7]                  #! PC = 0x5555551654 *)
broadcast %sqrdmulh 8 [%v9[7]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* add	v5.8h, v6.8h, v4.8h                         #! PC = 0x5555551658 *)
add %v5 %v6 %v4;
(* srshr	v5.8h, v5.8h, #2                          #! PC = 0x555555165c *)
split %HI %LO %v5 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* sub	v6.8h, v6.8h, v4.8h                         #! PC = 0x5555551660 *)
sub %v6 %v6 %v4;
(* srshr	v6.8h, v6.8h, #2                          #! PC = 0x5555551664 *)
split %HI %LO %v6 2; split %LO1 %dc %LO (2-1);
cast %LO0@sint16[8] %LO1; add %v6 %HI %LO0;
(* mul	v3.8h, v3.8h, v9.h[0]                       #! PC = 0x5555551668 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* add	v4.8h, v0.8h, v3.8h                         #! PC = 0x555555166c *)
adds %add_dc %v4 %v0 %v3;
(* mov	v16.16b, v4.16b                             #! PC = 0x5555551670 *)
mov %v16 %v4;
(* mls	v16.8h, v5.8h, v8.h[0]                      #! PC = 0x5555551674 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v16 %v16 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16 ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    + [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v16
    && true;

(* sub	v0.8h, v0.8h, v3.8h                         #! PC = 0x5555551678 *)
subc %sub_dc %v0 %v0 %v3;
(* mls	v0.8h, v6.8h, v8.h[0]                       #! PC = 0x555555167c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v6 %mls;
cast %mls@int16[8] %mls; subc %mdc %v0 %v0 %mls;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [2, 2, 2, 2, 2, 2, 2, 2] * %fa0_91 - %fa3_91 - %fa6_91
    - [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa3_91 - %fa6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v6
    = %v0
    && true;

(* add	v3.8h, v7.8h, v20.8h                        #! PC = 0x5555551680 *)
add %v3 %v7 %v20;
(* sub	v4.8h, v7.8h, v20.8h                        #! PC = 0x5555551684 *)
sub %v4 %v7 %v20;
(* add	v6.8h, v1.8h, v3.8h                         #! PC = 0x5555551688 *)
add %v6 %v1 %v3;
(* shl	v7.8h, v1.8h, #1                            #! PC = 0x555555168c *)
shl %v7 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v7.8h, v7.8h, v3.8h                         #! PC = 0x5555551690 *)
sub %v7 %v7 %v3;
(* sqrdmulh	v1.8h, v4.8h, v9.h[4]                  #! PC = 0x5555551694 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v4 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v4.8h, v4.8h, v9.h[0]                       #! PC = 0x5555551698 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v4 %v4 %mul; cast %v4@int16[8] %v4;
(* mls	v4.8h, v1.8h, v8.h[0]                       #! PC = 0x555555169c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_91 - %fa4_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_91 - %fa4_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_91 - %fa4_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa7_91 - %fa4_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v4
    && true;

(* add	v3.8h, v4.8h, v7.8h                         #! PC = 0x55555516a0 *)
add %v3 %v4 %v7;
(* sub	v7.8h, v7.8h, v4.8h                         #! PC = 0x55555516a4 *)
sub %v7 %v7 %v4;
(* add	v1.8h, v2.8h, v19.8h                        #! PC = 0x55555516a8 *)
add %v1 %v2 %v19;
(* sub	v5.8h, v2.8h, v19.8h                        #! PC = 0x55555516ac *)
sub %v5 %v2 %v19;
(* add	v19.8h, v18.8h, v1.8h                       #! PC = 0x55555516b0 *)
add %v19 %v18 %v1;
(* shl	v2.8h, v18.8h, #1                           #! PC = 0x55555516b4 *)
shl %v2 %v18 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* sub	v4.8h, v2.8h, v1.8h                         #! PC = 0x55555516b8 *)
sub %v4 %v2 %v1;
(* sqrdmulh	v1.8h, v5.8h, v9.h[4]                  #! PC = 0x55555516bc *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v5 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v2.8h, v5.8h, v9.h[0]                       #! PC = 0x55555516c0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v2 %v5 %mul; cast %v2@int16[8] %v2;
(* mls	v2.8h, v1.8h, v8.h[0]                       #! PC = 0x55555516c4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v2 %v2 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_91 - %fa5_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_91 - %fa5_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_91 - %fa5_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fa2_91 - %fa5_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v2
    && true;

(* add	v1.8h, v2.8h, v4.8h                         #! PC = 0x55555516c8 *)
add %v1 %v2 %v4;
(* sub	v2.8h, v4.8h, v2.8h                         #! PC = 0x55555516cc *)
sub %v2 %v4 %v2;

cut (* 78 *)
    %v8 = [4591, 29235, 0, 0, 0, 0, 0, 0] /\
    %v9 = [-621, 1891, -803, 0, -4432, 13497, -5731, -17729] /\

    %v17 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        %fa3_91 +
        %fa6_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v16 = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa3_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa6_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v0  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa3_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa6_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v6  = (
        %fa1_91 +
        %fa7_91 +
        %fa4_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v3  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa7_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa4_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v7  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa1_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa7_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa4_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v19 = (
        %fa8_91 +
        %fa2_91 +
        %fa5_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v1  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa2_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa5_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    %v2  = [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa8_91 +
        [ 310,  310,  310,  310,  310,  310,  310,  310] * %fa2_91 +
        [-311, -311, -311, -311, -311, -311, -311, -311] * %fa5_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ),

    %v17 <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v17 >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295] /\

    %v16 <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v16 >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v0  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v0  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750] /\

    %v6  <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v6  >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v3  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v3  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v7  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v7  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v19 <= [13800, 13800, 13800, 13800, 13800, 13800, 13800, 13800] /\
    %v19 >= [-13800, -13800, -13800, -13800, -13800, -13800, -13800, -13800] /\

    %v1  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v1  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    %v2  <= [20600, 20600, 20600, 20600, 20600, 20600, 20600, 20600] /\
    %v2  >= [-20600, -20600, -20600, -20600, -20600, -20600, -20600, -20600] /\

    true
    prove with [precondition, cuts[0, 77], algebra solver isl]
  &&
    %v8 = [4591@16, 29235@16, 0@16, 0@16, 0@16, 0@16, 0@16, 0@16] /\
    %v9 = [(-621)@16, 1891@16, (-803)@16, 0@16, (-4432)@16, 13497@16, (-5731)@16, (-17729)@16] /\

    %v17 <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v17 >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16] /\

    %v16 <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v16 >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v0  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v0  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16] /\

    %v6  <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v6  >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v3  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v3  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v7  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v7  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v19 <=s [13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16, 13800@16] /\
    %v19 >=s [(-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16, (-13800)@16] /\

    %v1  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v1  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    %v2  <=s [20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16, 20600@16] /\
    %v2  >=s [(-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16, (-20600)@16] /\

    true
    prove with [precondition, cuts[0, 77]];


ghost %fb0_91@sint16[8], %fb1_91@sint16[8], %fb2_91@sint16[8], %fb3_91@sint16[8], %fb4_91@sint16[8], %fb5_91@sint16[8], %fb6_91@sint16[8], %fb7_91@sint16[8], %fb8_91@sint16[8] :
    %fb0_91 = %v17 /\ %fb1_91 = %v16 /\ %fb2_91 = %v0  /\ %fb3_91 = %v6  /\
    %fb4_91 = %v3  /\ %fb5_91 = %v7  /\ %fb6_91 = %v19 /\ %fb7_91 = %v1  /\ %fb8_91 = %v2
  &&
    %fb0_91 = %v17 /\ %fb1_91 = %v16 /\ %fb2_91 = %v0  /\ %fb3_91 = %v6  /\
    %fb4_91 = %v3  /\ %fb5_91 = %v7  /\ %fb6_91 = %v19 /\ %fb7_91 = %v1  /\ %fb8_91 = %v2;

(* add	v4.8h, v6.8h, v19.8h                        #! PC = 0x55555516d0 *)
add %v4 %v6 %v19;
(* sub	v6.8h, v6.8h, v19.8h                        #! PC = 0x55555516d4 *)
sub %v6 %v6 %v19;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x55555516d8 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x55555516dc *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x55555516e0 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %fb3_91 + %fb6_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %fb3_91 + %fb6_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %fb3_91 + %fb6_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %fb3_91 + %fb6_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x55555516e4 *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v17.8h                        #! PC = 0x55555516e8 *)
add %v5 %v5 %v17;
(* str	q5, [x0]                                    #! EA = L0x7fffffc290; PC = 0x55555516ec *)
mov [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e] %v5;
(* sub	v5.8h, v17.8h, v4.8h                        #! PC = 0x55555516f0 *)
sub %v5 %v17 %v4;
(* sqrdmulh	v4.8h, v6.8h, v9.h[4]                  #! PC = 0x55555516f4 *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v6 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v6.8h, v6.8h, v9.h[0]                       #! PC = 0x55555516f8 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v6 %v6 %mul; cast %v6@int16[8] %v6;
(* mls	v6.8h, v4.8h, v8.h[0]                       #! PC = 0x55555516fc *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v6 %v6 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_91 - %fb6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_91 - %fb6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_91 - %fb6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%fb3_91 - %fb6_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v6
    && true;

assert
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v6  <= [3750, 3750, 3750, 3750, 3750, 3750, 3750, 3750] /\
    %v6  >= [-3750, -3750, -3750, -3750, -3750, -3750, -3750, -3750]
  &&
    %v6  <=s [3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16, 3750@16] /\
    %v6  >=s [(-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16, (-3750)@16];

(* add	v4.8h, v5.8h, v6.8h                         #! PC = 0x5555551700 *)
add %v4 %v5 %v6;
(* str	q4, [x3]                                    #! EA = L0x7fffffc2c0; PC = 0x5555551704 *)
mov [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce] %v4;
(* sub	v5.8h, v5.8h, v6.8h                         #! PC = 0x5555551708 *)
sub %v5 %v5 %v6;
(* str	q5, [x6]                                    #! EA = L0x7fffffc2f0; PC = 0x555555170c *)
mov [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe] %v5;
(* sqrdmulh	v4.8h, v1.8h, v9.h[5]                  #! PC = 0x5555551710 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[1]                       #! PC = 0x5555551714 *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551718 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb7_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b1i1_91@sint16[8] : %b1i1_91 = %v1 && %b1i1_91 = %v1;

(* sqrdmulh	v4.8h, v7.8h, v9.h[6]                  #! PC = 0x555555171c *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v7 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v5.8h, v7.8h, v9.h[2]                       #! PC = 0x5555551720 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v5 %v7 %mul; cast %v5@int16[8] %v5;
(* mls	v5.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551724 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v5 %v5 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb5_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v5
    && true;

ghost %b1i2_91@sint16[8] : %b1i2_91 = %v5 && %b1i2_91 = %v5;

(* add	v4.8h, v5.8h, v1.8h                         #! PC = 0x5555551728 *)
add %v4 %v5 %v1;
(* sub	v1.8h, v1.8h, v5.8h                         #! PC = 0x555555172c *)
sub %v1 %v1 %v5;
(* sqdmulh	v5.8h, v4.8h, v8.h[1]                   #! PC = 0x5555551730 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v4 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v5@sint16[8] %LO0;
(* srshr	v5.8h, v5.8h, #12                         #! PC = 0x5555551734 *)
split %HI %LO %v5 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v5 %HI %LO0;
(* mls	v4.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551738 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v4 %v4 %mls;

assert
    %b1i1_91 + %b1i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b1i1_91 + %b1i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b1i1_91 + %b1i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b1i1_91 + %b1i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v4
    && true;

assert
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v4  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v4  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v4  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v4  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v5.8h, v4.8h, #1                            #! PC = 0x555555173c *)
shl %v5 %v4 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v5.8h, v5.8h, v16.8h                        #! PC = 0x5555551740 *)
add %v5 %v5 %v16;
(* str	q5, [x1]                                    #! EA = L0x7fffffc2a0; PC = 0x5555551744 *)
mov [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae] %v5;
(* sub	v4.8h, v16.8h, v4.8h                        #! PC = 0x5555551748 *)
sub %v4 %v16 %v4;
(* sqrdmulh	v5.8h, v1.8h, v9.h[4]                  #! PC = 0x555555174c *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v1 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v5 %LO11 %LO01;
(* mul	v1.8h, v1.8h, v9.h[0]                       #! PC = 0x5555551750 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v1 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v5.8h, v8.h[0]                       #! PC = 0x5555551754 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v5 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_91 - %b1i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_91 - %b1i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_91 - %b1i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b1i1_91 - %b1i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v5
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v5.8h, v4.8h, v1.8h                         #! PC = 0x5555551758 *)
add %v5 %v4 %v1;
(* str	q5, [x7]                                    #! EA = L0x7fffffc300; PC = 0x555555175c *)
mov [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e] %v5;
(* sub	v4.8h, v4.8h, v1.8h                         #! PC = 0x5555551760 *)
sub %v4 %v4 %v1;
(* str	q4, [x4]                                    #! EA = L0x7fffffc2d0; PC = 0x5555551764 *)
mov [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de] %v4;
(* sqrdmulh	v1.8h, v3.8h, v9.h[5]                  #! PC = 0x5555551768 *)
broadcast %sqrdmulh 8 [%v9[5]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v1 %LO11 %LO01;
(* mul	v3.8h, v3.8h, v9.h[1]                       #! PC = 0x555555176c *)
broadcast %mul 8 [%v9[1]]; mull %mdc %v3 %v3 %mul; cast %v3@int16[8] %v3;
(* mls	v3.8h, v1.8h, v8.h[0]                       #! PC = 0x5555551770 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v1 %mls;
cast %mls@int16[8] %mls; subc %mdc %v3 %v3 %mls;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891] * %fb4_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v1
    = %v3
    && true;

ghost %b2i1_91@sint16[8] : %b2i1_91 = %v3 && %b2i1_91 = %v3;

(* sqrdmulh	v4.8h, v2.8h, v9.h[6]                  #! PC = 0x5555551774 *)
broadcast %sqrdmulh 8 [%v9[6]]; smulj %LO %v2 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v4 %LO11 %LO01;
(* mul	v1.8h, v2.8h, v9.h[2]                       #! PC = 0x5555551778 *)
broadcast %mul 8 [%v9[2]]; mull %mdc %v1 %v2 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x555555177c *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-803, -803, -803, -803, -803, -803, -803, -803] * %fb8_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

ghost %b2i2_91@sint16[8] : %b2i2_91 = %v1 && %b2i2_91 = %v1;

(* add	v2.8h, v1.8h, v3.8h                         #! PC = 0x5555551780 *)
add %v2 %v1 %v3;
(* sub	v3.8h, v3.8h, v1.8h                         #! PC = 0x5555551784 *)
sub %v3 %v3 %v1;
(* sqdmulh	v4.8h, v2.8h, v8.h[1]                   #! PC = 0x5555551788 *)
broadcast %sqdmulh 8 [%v8[1]]; smulj %LO %v2 %sqdmulh;
ssplit %LO0 %dc %LO 15; vpc %v4@sint16[8] %LO0;
(* srshr	v4.8h, v4.8h, #12                         #! PC = 0x555555178c *)
split %HI %LO %v4 12; split %LO1 %dc %LO (12-1);
cast %LO0@sint16[8] %LO1; add %v4 %HI %LO0;
(* mov	v1.16b, v2.16b                              #! PC = 0x5555551790 *)
mov %v1 %v2;
(* mls	v1.8h, v4.8h, v8.h[0]                       #! PC = 0x5555551794 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v4 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    %b2i1_91 + %b2i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    %b2i1_91 + %b2i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    %b2i1_91 + %b2i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    %b2i1_91 + %b2i2_91
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v4
    = %v1
    && true;

assert
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2295, 2295, 2295, 2295, 2295, 2295, 2295, 2295] /\
    %v1  >= [-2295, -2295, -2295, -2295, -2295, -2295, -2295, -2295]
  &&
    %v1  <=s [2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16, 2295@16] /\
    %v1  >=s [(-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16, (-2295)@16];

(* shl	v2.8h, v1.8h, #1                            #! PC = 0x5555551798 *)
shl %v2 %v1 [(1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16, (1)@int16];
(* add	v2.8h, v2.8h, v0.8h                         #! PC = 0x555555179c *)
add %v2 %v2 %v0;
(* str	q2, [x0]                                    #! EA = L0x7fffffc310; PC = 0x55555517a4 *)
mov [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517a8 *)
sub %v0 %v0 %v1;
(* sqrdmulh	v2.8h, v3.8h, v9.h[4]                  #! PC = 0x55555517ac *)
broadcast %sqrdmulh 8 [%v9[4]]; smulj %LO %v3 %sqrdmulh;
ssplit %LO1 %LO0 %LO 15; usplit %LO00 %dc %LO0 14;
vpc %LO01@sint16[8] %LO00; vpc %LO11@sint16[8] %LO1;
add %v2 %LO11 %LO01;
(* mul	v1.8h, v3.8h, v9.h[0]                       #! PC = 0x55555517b0 *)
broadcast %mul 8 [%v9[0]]; mull %mdc %v1 %v3 %mul; cast %v1@int16[8] %v1;
(* mls	v1.8h, v2.8h, v8.h[0]                       #! PC = 0x55555517b4 *)
broadcast %mls 8 [%v8[0]]; mull %dc %mls %v2 %mls;
cast %mls@int16[8] %mls; subc %mdc %v1 %v1 %mls;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_91 - %b2i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    <= [32767, 32767, 32767, 32767, 32767, 32767, 32767, 32767] /\

    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_91 - %b2i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    >= [-32768, -32768, -32768, -32768, -32768, -32768, -32768, -32768]

    prove with [algebra solver isl] && true;

assert
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_91 - %b2i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1  ( mod [65536, 65536, 65536, 65536, 65536, 65536, 65536, 65536] )
    && true;

assume
    [-621, -621, -621, -621, -621, -621, -621, -621] * (%b2i1_91 - %b2i2_91)
    - [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] * %v2
    = %v1
    && true;

assert
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
    prove with [precondition, algebra solver isl] && true;

assume
    %v1  <= [2750, 2750, 2750, 2750, 2750, 2750, 2750, 2750] /\
    %v1  >= [-2750, -2750, -2750, -2750, -2750, -2750, -2750, -2750]
  &&
    %v1  <=s [2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16, 2750@16] /\
    %v1  >=s [(-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16, (-2750)@16];

(* add	v2.8h, v0.8h, v1.8h                         #! PC = 0x55555517b8 *)
add %v2 %v0 %v1;
(* str	q2, [x2]                                    #! EA = L0x7fffffc2b0; PC = 0x55555517bc *)
mov [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be] %v2;
(* sub	v0.8h, v0.8h, v1.8h                         #! PC = 0x55555517c0 *)
sub %v0 %v0 %v1;
(* str	q0, [x5]                                    #! EA = L0x7fffffc2e0; PC = 0x55555517c4 *)
mov [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee] %v0;
(* ldp	d8, d9, [sp], #16                           #! EA = L0x7fffffc1f0; Value = 0x0000000000000000; PC = 0x55555517c8 *)
mov %v8 [L0x7fffffc1f0,L0x7fffffc1f2,L0x7fffffc1f4,L0x7fffffc1f6,0@sint16,0@sint16,0@sint16,0@sint16];
mov %v9 [L0x7fffffc1f8,L0x7fffffc1fa,L0x7fffffc1fc,L0x7fffffc1fe,0@sint16,0@sint16,0@sint16,0@sint16];
(* #! <- SP = 0x7fffffc200 *)
#! 0x7fffffc200 = 0x7fffffc200;
(* #ret                                            #! PC = 0x55555517cc *)
#ret                                            #! 0x55555517cc = 0x55555517cc;

cut (* 79 *)
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        %fa1_91 +
        %fa2_91 +
        %fa3_91 +
        %fa4_91 +
        %fa5_91 +
        %fa6_91 +
        %fa7_91 +
        %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa1_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa2_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa4_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa5_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa7_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa1_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa2_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa4_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa5_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa7_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa1_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa2_91 +
        %fa3_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa4_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa5_91 +
        %fa6_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa7_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa1_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa2_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa4_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa5_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa7_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa1_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa2_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa4_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa5_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa7_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa1_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa2_91 +
        %fa3_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa4_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa5_91 +
        %fa6_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa7_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa1_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa2_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa3_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa4_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa5_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa6_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa7_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] =
    [2, 2, 2, 2, 2, 2, 2, 2] * (
        %fa0_91 +
        [W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8, W9**8] * %fa1_91 +
        [W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7, W9**7] * %fa2_91 +
        [W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6, W9**6] * %fa3_91 +
        [W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5, W9**5] * %fa4_91 +
        [W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4, W9**4] * %fa5_91 +
        [W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3, W9**3] * %fa6_91 +
        [W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2, W9**2] * %fa7_91 +
        [W9   , W9   , W9   , W9   , W9   , W9   , W9   , W9   ] * %fa8_91
    )
    ( mod [4591, 4591, 4591, 4591, 4591, 4591, 4591, 4591] ) /\


    true
    prove with [precondition, cuts[0, 77]],

    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] <= [6885, 6885, 6885, 6885, 6885, 6885, 6885, 6885] /\
    [L0x7fffffc290, L0x7fffffc292, L0x7fffffc294, L0x7fffffc296, L0x7fffffc298, L0x7fffffc29a, L0x7fffffc29c, L0x7fffffc29e] >= [-6885, -6885, -6885, -6885, -6885, -6885, -6885, -6885] /\

    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2a0, L0x7fffffc2a2, L0x7fffffc2a4, L0x7fffffc2a6, L0x7fffffc2a8, L0x7fffffc2aa, L0x7fffffc2ac, L0x7fffffc2ae] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2b0, L0x7fffffc2b2, L0x7fffffc2b4, L0x7fffffc2b6, L0x7fffffc2b8, L0x7fffffc2ba, L0x7fffffc2bc, L0x7fffffc2be] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2c0, L0x7fffffc2c2, L0x7fffffc2c4, L0x7fffffc2c6, L0x7fffffc2c8, L0x7fffffc2ca, L0x7fffffc2cc, L0x7fffffc2ce] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2d0, L0x7fffffc2d2, L0x7fffffc2d4, L0x7fffffc2d6, L0x7fffffc2d8, L0x7fffffc2da, L0x7fffffc2dc, L0x7fffffc2de] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc2e0, L0x7fffffc2e2, L0x7fffffc2e4, L0x7fffffc2e6, L0x7fffffc2e8, L0x7fffffc2ea, L0x7fffffc2ec, L0x7fffffc2ee] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc2f0, L0x7fffffc2f2, L0x7fffffc2f4, L0x7fffffc2f6, L0x7fffffc2f8, L0x7fffffc2fa, L0x7fffffc2fc, L0x7fffffc2fe] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] <= [8795, 8795, 8795, 8795, 8795, 8795, 8795, 8795] /\
    [L0x7fffffc300, L0x7fffffc302, L0x7fffffc304, L0x7fffffc306, L0x7fffffc308, L0x7fffffc30a, L0x7fffffc30c, L0x7fffffc30e] >= [-8795, -8795, -8795, -8795, -8795, -8795, -8795, -8795] /\

    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] <= [8340, 8340, 8340, 8340, 8340, 8340, 8340, 8340] /\
    [L0x7fffffc310, L0x7fffffc312, L0x7fffffc314, L0x7fffffc316, L0x7fffffc318, L0x7fffffc31a, L0x7fffffc31c, L0x7fffffc31e] >= [-8340, -8340, -8340, -8340, -8340, -8340, -8340, -8340] /\

    true
    prove with [precondition, cuts[0, 77], algebra solver isl]
  &&
    true
    prove with [precondition, cuts[0, 77]];


#### store

(* ldr	q0, [sp, #144]                              #! EA = L0x7fffffc290; Value = 0x0000000000000000; PC = 0x555555158c *)
mov %v0 [L0x7fffffc290,L0x7fffffc292,L0x7fffffc294,L0x7fffffc296,L0x7fffffc298,L0x7fffffc29a,L0x7fffffc29c,L0x7fffffc29e];
(* str	q0, [x19, #144]                             #! EA = L0x7fffffce00; PC = 0x5555551590 *)
mov [L0x7fffffce00,L0x7fffffce02,L0x7fffffce04,L0x7fffffce06,L0x7fffffce08,L0x7fffffce0a,L0x7fffffce0c,L0x7fffffce0e] %v0;
(* ldr	q0, [sp, #160]                              #! EA = L0x7fffffc2a0; Value = 0x0000000000000000; PC = 0x5555551594 *)
mov %v0 [L0x7fffffc2a0,L0x7fffffc2a2,L0x7fffffc2a4,L0x7fffffc2a6,L0x7fffffc2a8,L0x7fffffc2aa,L0x7fffffc2ac,L0x7fffffc2ae];
(* str	q0, [x19, #160]                             #! EA = L0x7fffffce10; PC = 0x5555551598 *)
mov [L0x7fffffce10,L0x7fffffce12,L0x7fffffce14,L0x7fffffce16,L0x7fffffce18,L0x7fffffce1a,L0x7fffffce1c,L0x7fffffce1e] %v0;
(* ldr	q0, [sp, #176]                              #! EA = L0x7fffffc2b0; Value = 0x0000000000000000; PC = 0x555555159c *)
mov %v0 [L0x7fffffc2b0,L0x7fffffc2b2,L0x7fffffc2b4,L0x7fffffc2b6,L0x7fffffc2b8,L0x7fffffc2ba,L0x7fffffc2bc,L0x7fffffc2be];
(* str	q0, [x19, #176]                             #! EA = L0x7fffffce20; PC = 0x55555515a0 *)
mov [L0x7fffffce20,L0x7fffffce22,L0x7fffffce24,L0x7fffffce26,L0x7fffffce28,L0x7fffffce2a,L0x7fffffce2c,L0x7fffffce2e] %v0;
(* ldr	q0, [sp, #192]                              #! EA = L0x7fffffc2c0; Value = 0x0000000000000000; PC = 0x55555515a4 *)
mov %v0 [L0x7fffffc2c0,L0x7fffffc2c2,L0x7fffffc2c4,L0x7fffffc2c6,L0x7fffffc2c8,L0x7fffffc2ca,L0x7fffffc2cc,L0x7fffffc2ce];
(* str	q0, [x19, #192]                             #! EA = L0x7fffffce30; PC = 0x55555515a8 *)
mov [L0x7fffffce30,L0x7fffffce32,L0x7fffffce34,L0x7fffffce36,L0x7fffffce38,L0x7fffffce3a,L0x7fffffce3c,L0x7fffffce3e] %v0;
(* ldr	q0, [sp, #208]                              #! EA = L0x7fffffc2d0; Value = 0x0000000000000000; PC = 0x55555515ac *)
mov %v0 [L0x7fffffc2d0,L0x7fffffc2d2,L0x7fffffc2d4,L0x7fffffc2d6,L0x7fffffc2d8,L0x7fffffc2da,L0x7fffffc2dc,L0x7fffffc2de];
(* str	q0, [x19, #208]                             #! EA = L0x7fffffce40; PC = 0x55555515b0 *)
mov [L0x7fffffce40,L0x7fffffce42,L0x7fffffce44,L0x7fffffce46,L0x7fffffce48,L0x7fffffce4a,L0x7fffffce4c,L0x7fffffce4e] %v0;
(* ldr	q0, [sp, #224]                              #! EA = L0x7fffffc2e0; Value = 0x0000000000000000; PC = 0x55555515b4 *)
mov %v0 [L0x7fffffc2e0,L0x7fffffc2e2,L0x7fffffc2e4,L0x7fffffc2e6,L0x7fffffc2e8,L0x7fffffc2ea,L0x7fffffc2ec,L0x7fffffc2ee];
(* str	q0, [x19, #224]                             #! EA = L0x7fffffce50; PC = 0x55555515b8 *)
mov [L0x7fffffce50,L0x7fffffce52,L0x7fffffce54,L0x7fffffce56,L0x7fffffce58,L0x7fffffce5a,L0x7fffffce5c,L0x7fffffce5e] %v0;
(* ldr	q0, [sp, #240]                              #! EA = L0x7fffffc2f0; Value = 0x0000000000000000; PC = 0x55555515bc *)
mov %v0 [L0x7fffffc2f0,L0x7fffffc2f2,L0x7fffffc2f4,L0x7fffffc2f6,L0x7fffffc2f8,L0x7fffffc2fa,L0x7fffffc2fc,L0x7fffffc2fe];
(* str	q0, [x19, #240]                             #! EA = L0x7fffffce60; PC = 0x55555515c0 *)
mov [L0x7fffffce60,L0x7fffffce62,L0x7fffffce64,L0x7fffffce66,L0x7fffffce68,L0x7fffffce6a,L0x7fffffce6c,L0x7fffffce6e] %v0;
(* ldr	q0, [sp, #256]                              #! EA = L0x7fffffc300; Value = 0x0000000000000000; PC = 0x55555515c4 *)
mov %v0 [L0x7fffffc300,L0x7fffffc302,L0x7fffffc304,L0x7fffffc306,L0x7fffffc308,L0x7fffffc30a,L0x7fffffc30c,L0x7fffffc30e];
(* str	q0, [x19, #256]                             #! EA = L0x7fffffce70; PC = 0x55555515c8 *)
mov [L0x7fffffce70,L0x7fffffce72,L0x7fffffce74,L0x7fffffce76,L0x7fffffce78,L0x7fffffce7a,L0x7fffffce7c,L0x7fffffce7e] %v0;
(* ldr	q0, [sp, #272]                              #! EA = L0x7fffffc310; Value = 0x0000000000000000; PC = 0x55555515cc *)
mov %v0 [L0x7fffffc310,L0x7fffffc312,L0x7fffffc314,L0x7fffffc316,L0x7fffffc318,L0x7fffffc31a,L0x7fffffc31c,L0x7fffffc31e];
(* str	q0, [x19, #272]                             #! EA = L0x7fffffce80; PC = 0x55555515d0 *)
mov [L0x7fffffce80,L0x7fffffce82,L0x7fffffce84,L0x7fffffce86,L0x7fffffce88,L0x7fffffce8a,L0x7fffffce8c,L0x7fffffce8e] %v0;

cut (* 80 *)
    true && true;


### epilogue

(* #b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! PC = 0x55555515e0 *)
#b.ne	0x5555551494 <_ZN6xpower9main_lay211bwd_inplaceEPA2_A9_A8_s+64>  // b.any#! 0x55555515e0 = 0x55555515e0;


# epilogue

(* #! <- SP = 0x7fffffc320 *)
#! 0x7fffffc320 = 0x7fffffc320;
(* #ret                                            #! PC = 0x5555551600 *)
#ret                                            #! 0x5555551600 = 0x5555551600;


{ true && true }
